{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Digital Hub is an Open-Source platform for building and managing Data and AI applications and services. Bringing the principles of DataOps, MLOps, DevOps and GitOps, integrating the state-of-art open source technologies and modern standards, DigitalHub extends your development process and operations to the whole application life-cycle, from exploration to deployment, monitoring, and evolution.</p> <p></p> <ul> <li>Explore. Use on-demand interactive scalable workspaces of your choice to code, explore, experiment and analyze the data and ML. Workspace provide a secure and isolated execution environments natively connected to the platform, such as Jupyter Notebooks or VS Code, Dremio distributed query engine, etc. Use extensibility mechanisms provided by the underlying workspace manager to bring your own workspace templates into the platform.</li> <li>Process. Use persistent storages (Datalake and Relational DBs) to manage structured and non-structured data on top of the data abstraction layer. Elaborate  data, perform data analysis activites and train AI models using frameworks and libraries of your choice (e.g., from python-based to DBT, to arbitrary containers). Manage the supporting computational and storage resources in a declarative and transparent manner.</li> <li>Execute. Delegate the code execution, image preparation, run-time operations and services to the underlying Kubernetes-based execution infrastructure, Serverless platform, and pipeline execution automation environment.</li> <li>Integrate. Build new AI services and expose your data in a standard and interoperable manner, to facilitate the integration within different applications, systems, business intelligence tools and visualizations.</li> </ul> <p>To support this functionality, the platform relies on scalable Kubernetes platform and its extensions (operators) as well as on the modular architecture and functionality model that allows for dealing with arbitrary jobs, functions, frameworks and solutions without affecting your development workflow. The underlying methodology and management approach aim at facilitating the re-use, reproducibility, and portability of the solution among different contexts and settings.</p>"},{"location":"#interested","title":"Interested?","text":"<ul> <li>Quick Start. Bring the platform up and explore it in few minutes!</li> <li>Installation. Learn how to install, configure, and manage the platform in different settings.</li> <li>Overview. Deep dive into the platform functionality, architecture, components, and functionality.</li> </ul>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<ul> <li>Native support for the integration of new functionality</li> <li>Standard infrastructures and frameworks</li> <li>Common workflow and abstractions</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform relies on a built-in serverless platform, and supports Python functions as well as a set of ML model formats for serving.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with ML a range of frameworks, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (under development), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-on-minikube","title":"Installation on minikube","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>Minikube</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>1) Start minikube (change 192.168.49.0 if your network setup is different): <pre><code>    minikube start --insecure-registry \"192.168.49.0/24\" --memory 12288 --cpus 4\n</code></pre></p> <p>2) Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></p> <p>3) Get minikube external IP: <pre><code>    minikube ip\n</code></pre></p> <p>4) Install DigitalHub with Helm.</p> <p>The platform requires a Values file with the necessary configuration. We provide a configured <code>values-demo.yaml</code> file that you can find on our GitHub repository, feel free to use it for starting the platform in a test environment or for reference.</p> <p>Replace the two placeholders called <code>MINIKUBE_IP_ADDRESS</code> in the command below with the output of the previous command, <code>minikube ip</code> and <code>PATH_TO_YOUR_VALUES_FILE</code> with the path of your configuration file. <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --set global.registry.url=\"MINIKUBE_IP_ADDRESS\" --set global.externalHostAddress=\"MINIKUBE_IP_ADDRESS\" --values PATH_TO_YOUR_VALUES_FILE --timeout 45m0s\n</code></pre></p> <p>5) Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></p> <p>Once installed, you should see the references (URLs) for the different tools of the platform, similar to the example below: <pre><code>##########################################################\n#   _____   _       _           _ _     _       _        #\n#  (____ \\ (_)     (_)_        | | |   | |     | |       #\n#   _   \\ \\ _  ____ _| |_  ____| | |__ | |_   _| | _     #\n#  | |   | | |/ _  | |  _)/ _  | |  __)| | | | | || \\    #\n#  | |__/ /| ( ( | | | |_( ( | | | |   | | |_| | |_) )   #\n#  |_____/ |_|\\_|| |_|\\___)_||_|_|_|   |_|\\____|____/    #\n#            (_____|                                     #\n#                                                        #\n##########################################################\n\nDigitalhub has been installed. Check its status by running:\n  kubectl --namespace digitalhub get pods\n\nDigitalhub componet URLs:\n  - Dashboard: http://192.168.76.2:30110\n  - Jupyter: http://192.168.76.2:30040 (Create jupyter workspace from template in the coder dashboard before use)\n  - Dremio: http://192.168.76.2:30120 (Create dremio workspace from template in the coder dashboard before use)\n  - Sqlpad: http://192.168.76.2:30140 (Create sqlpad workspace from template in the coder dashboard before use)\n  - Grafana: http://192.168.76.2:30130 (Create grafana workspace from template in the coder dashboard before use)\n  - Vscode: http://192.168.76.2:30190 (Create vscode workspace from template in the coder dashboard before use)\n  - Docker Registry: http://192.168.76.2:30150\n  - Minio API: http://192.168.76.2:30080 (Username: minio Password: minio123)\n  - Minio UI: http://192.168.76.2:30090 (Username: minio Password: minio123)\n  - KubeFlow: http://192.168.76.2:30100\n  - Coder: http://192.168.76.2:30170 (Username: test@digitalhub.test Password: Test12456@!)\n  - Core: http://192.168.76.2:30180\n  - Kubernetes Resource Manager: http://192.168.76.2:30160\n</code></pre></p> <p>A note for Windows, Darwin and WSL users</p> <p>As of now, due to the limitations of Minikube it is not possible to access your applications directly while using one of the OS mentioned above.</p> <p>You can still access your apps from browser, but you will have to use the <code>kubectl port-forward</code> command.</p> <p>For example, if you wish to expose the core service, you can use: <pre><code>kubectl -n digitalhub port-forward service/digitalhub-core 30180:8080\n</code></pre> This will allow you to access core by typing <code>localhost:30180</code> in your browser.</p> <p>The full list of services can be checked using this command: <pre><code>kubectl -n digitalhub get services\n</code></pre></p> <p>Please consult the official Kubernetes documentation for more details.</p>"},{"location":"installation/#installation-on-cluster","title":"Installation on cluster","text":"<p>To install DigitalHub on a production environment, please consult the admin section of the documentation, where you will find informations about the configuration options and the installation as well.</p>"},{"location":"installation/#installing-the-cli","title":"Installing the CLI","text":"<p>To install and use the command-line interface, please refer to this section</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>To start with DigitalHub, the first step is to install the platform and all its components. For its functionality, DigitalHub relies on Kubernetes, a state-of-art Open-Source containerized application deployment, orchestration and execution platform. While it is possible to run DigitalHub on any Kubernetes installation, the quickest way is to deploy it on Minikube, a local Kubernetes environment with minimal settings. See here instruction on how to set up DigitalHub on Minikube.</p> <p>Once installed, you can access different platform components and perform different operations, ranging from explorative data science with Jupyter Notebooks, creating projects for data processing or ML tasks, managing necessary resources (e.g., databases or datalake buckets), creating and running different functions, etc.</p>"},{"location":"quickstart/#platform-components-and-functionality","title":"Platform Components and Functionality","text":"<p>To access the different components of the platform start from the landing page, where the components are linked:</p> <ul> <li>Use Coder to create interactive workspaces, such as Jupyter Notebooks, to perform explorative tasks, access and manage the data. See how to use Workspaces for these type of activities.</li> <li>Use DH Core UI to manage your data science and ML project and start with management activities, such as creating data items, defining and executing different functions and operations. Please note that these tasks may be done directly with the DH Core Python SDK from your interactive environment. See how to use DH Console for the management operations.</li> <li>To see and manage the relevant Kubernetes resources (e.g., services, jobs, secrets), as well as custom resources of the platform (e.g., databases, S3 buckets, data services), use Kubernetes Resource Manager. The operations and the functionality of the tool are described in the Resource Management with KRM section of the documentation.</li> <li>Use Minio browser to navigate your datalake, upload and manage the files. The datalake is based on S3 protocol and can be used also programmatically. See the Data and Transformations section on how the data abstraction layer is defined and implemented.</li> <li>If you perform ML task with the Python runtime, you can prepare data, create and log ML Models using DH Core (see, e.g., Python Runtime if you want to use python operations through DHCore). </li> <li>Use Core Serverless platform to deploy and expose Python functions or ML Models in different formats as services within the platform.</li> <li>It is possible to organize the data and ML operations in complex pipelines. Currently the platform relies on Kubeflow Pipelines specification for this purpose, orchestrating the activities as single Kubernetes Jobs. See more on this in the corresponding Pipelines section.</li> </ul> <p>You can also use the CLI to perform operations on the platform externally. See how to install and use it in this section.</p>"},{"location":"quickstart/#tutorials","title":"Tutorials","text":"<p>Start exploring the platform through a series of tutorials aiming at explaining the key usage scenarios for DigitalHub platform. Specifically</p> <ul> <li>Create your first data management pipeline, from data exploration to automated data ETL procedure running on the platform.</li> <li>Perform DBT data transformation and store the data in a database.</li> <li>Train a scikit-learn ML Model and deploy it as an inference server.</li> <li>Train a MLFLow-compatible Model and deploy it as an inference server.</li> <li>Train a custom ML Model and deploy it as a service with the serverless platform.</li> <li>Work with LLM Model and deploy it as a service with the serverless platform.</li> <li>Use Dremio distributed query engine to organize data and visualize with Grafana.</li> <li>Store data in DB to perform efficient and complex queries and expose the data as REST API.</li> </ul>"},{"location":"cli/commands/","title":"CLI commands","text":"<p>Available CLI commands and their parameters are listed here. In these examples, the executable is named <code>dhcli</code>.</p> <p>If you need to install the CLI, refer to this section.</p> <p>Run commands</p> <p>Depending on the shell you are using, you may have to run the CLI with <code>./dhcli</code>.</p>"},{"location":"cli/commands/#register","title":"<code>register</code>","text":"<p><code>register</code> takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional. Name of the environment to register.</li> <li><code>core_endpoint</code> Mandatory</li> </ul> <p><pre><code>dhcli register -e example http://localhost:8080\n</code></pre> It will create a <code>.dhcore.ini</code> file (if it doesn't already exist) in the user's home directory, or, if not possible, in the current one. A section will be appended, using the provided environment name (or, if missing, the one returned by the endpoint), containing the environment's configuration. This environment will be set as default, unless one is already set.</p>"},{"location":"cli/commands/#list-env","title":"<code>list-env</code>","text":"<p><code>list-env</code> lists available environments. It takes no parameters.</p> <pre><code>dhcli list-env\n</code></pre>"},{"location":"cli/commands/#use","title":"<code>use</code>","text":"<p><code>use</code> takes the following parameters:</p> <ul> <li><code>environment</code> Mandatory</li> </ul> <p><pre><code>dhcli use example\n</code></pre> This will set the default environment.</p>"},{"location":"cli/commands/#login","title":"<code>login</code>","text":"<p><code>login</code> is to be used after registering an environment with the <code>register</code> command. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional.</li> </ul> <p><pre><code>dhcli login -e example\n</code></pre> It will read the corresponding section from the configuration file and start the log in procedure. It will update this section with the access token obtained. If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#refresh","title":"<code>refresh</code>","text":"<p><code>refresh</code> is to be used after the <code>login</code> command, to update <code>access_token</code> and <code>refresh_token</code>. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> </ul> <p><pre><code>dhcli refresh example\n</code></pre> If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#remove","title":"<code>remove</code>","text":"<p><code>remove</code> takes the following parameters:</p> <ul> <li><code>environment</code> Mandatory</li> </ul> <p><pre><code>dhcli remove example\n</code></pre> It will remove the section from the configuration file.</p>"},{"location":"cli/commands/#init","title":"<code>init</code>","text":"<p><code>init</code> is used to install the platform's Python packages; therefore, Python must be installed. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> </ul> <p><pre><code>dhcli init example\n</code></pre> It will match core's minor version as indicated in the specified environment. If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#create","title":"<code>create</code>","text":"<p><code>create</code> will create an instance of the indicated resource on the platform. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) when creating projects, mandatory otherwise.</li> <li><code>-f yaml_file_path</code> Mandatory when creating resources other than projects, alternative to <code>name</code> for projects.</li> <li><code>-n name</code> Optional (ignored) when creating resources other than projects, alternative to <code>yaml_file_path</code> for projects.</li> <li><code>-reset-id</code> Optional. Boolean. If set, the <code>id</code> specified in the file is ignored.</li> <li><code>resource</code> Mandatory</li> </ul> <p>The type of resource to create is mandatory. The project flag <code>-p</code> is only mandatory when creating resources other than projects (artifacts, models, etc.). For projects, you may omit the file path and just use the <code>-n</code> flag to specify the name. The <code>-reset-id</code> flag, when set, ensures the created object has a randomly-generated ID, ignoring the <code>id</code> field if present in the input file (this is not relevant to projects).</p> <p>Create a project: <pre><code>dhcli create -f samples/project.yaml projects\n</code></pre></p> <p>Create an artifact, while resetting its ID: <pre><code>dhcli create -p my-project -f samples/artifact.yaml -reset-id artifacts\n</code></pre></p>"},{"location":"cli/commands/#list","title":"<code>list</code>","text":"<p><code>list</code> returns a list of resources of the specified type. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-o output_format</code> Optional. Accepts <code>short</code>, <code>json</code>, <code>yaml</code>. Defaults to <code>short</code>.</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Optional. If present, will return all versions of specified resource. If missing, will return the latest version of all matching resources.</li> <li><code>-k kind</code> Optional</li> <li><code>-s state</code> Optional</li> <li><code>resource</code> Mandatory</li> </ul> <p><code>output_format</code> determines how the output will be formatted. The default value, <code>short</code>, is meant to be used to quickly check resources in the terminal, while <code>json</code> and <code>yaml</code> will format the output accordingly, making it ideal to write to file.</p> <p>List all projects:</p> <pre><code>dhcli list projects\n</code></pre> <p>List all artifacts in a project:</p> <pre><code>dhcli list -p my-project artifacts\n</code></pre> <p>Note that you can easily write the results to file by redirecting standard output: <pre><code>dhcli list -o yaml -p my-project artifacts &gt; output.yaml\n</code></pre></p>"},{"location":"cli/commands/#get","title":"<code>get</code>","text":"<p><code>get</code> returns the details of a single resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-o output_format</code> Optional. Accepts <code>short</code>, <code>json</code>, <code>yaml</code>. Defaults to <code>short</code>.</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Ignored if <code>id</code> is present, otherwise mandatory and will return the latest version of the specified resource.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>-n name</code>.</li> </ul> <p>Similarly to the <code>list</code> command, <code>output_format</code> determines how the output will be formatted. The default value, <code>short</code>, is meant to be used to quickly check resources in the terminal, while <code>json</code> and <code>yaml</code> will format the output accordingly, making it ideal to write to file.</p> <p>Get project:</p> <pre><code>dhcli get projects my-project\n</code></pre> <p>Get artifact: <pre><code>dhcli get -p my-project artifacts my-artifact-id\n</code></pre></p> <p>Get artifact and write to file: <pre><code>dhcli get -o yaml -p my-project artifacts my-artifact-id &gt; output.yaml\n</code></pre></p>"},{"location":"cli/commands/#update","title":"<code>update</code>","text":"<p><code>update</code> will update a resource with a new definition. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-f yaml_file_path</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Update a project: <pre><code>dhcli update -f samples/project.yaml projects my-project\n</code></pre></p> <p>Update an artifact: <pre><code>dhcli update -p my-project -f samples/artifact.yaml artifacts my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#delete","title":"<code>delete</code>","text":"<p><code>delete</code> will delete a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Alternative to <code>id</code>, will delete all versions of a resource.</li> <li><code>-y</code> Optional. Boolean. If omitted, confirmation will be asked.</li> <li><code>-c</code> Optional. Boolean, only applies to projects. When set, all resource belonging to the project will also be deleted.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>name</code>, will delete a specific version. For projects, since versions do not apply, this is synonym with <code>id</code>.</li> </ul> <p>Delete a project and all of its resources: <pre><code>dhcli delete -c projects my-project\n</code></pre></p> <p>Delete an artifact, skip confirmation: <pre><code>dhcli delete -p my-project -y artifacts my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#run","title":"<code>run</code>","text":"<p>Creates a run of the specified function. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name_of_function</code> Ignored if <code>id</code> is specified, otherwise mandatory and will run the latest version of the function.</li> <li><code>-i id</code> Alternative to <code>-n name_of_function</code>.</li> <li><code>-f yaml_file_path</code> Optional, can contain additional parameters for the run.</li> <li><code>task</code> Mandatory. Must contain a valid task, such as <code>python+build</code>.</li> </ul> <p>Create a <code>python+build</code> run of latest version of <code>my-function</code>: <pre><code>dhcli run -p my-project -n my-function python+build\n</code></pre></p>"},{"location":"cli/commands/#log","title":"<code>log</code>","text":"<p>Returns the logs of the specified resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-c container</code> Optional, ID of the container to read logs from. If not specified, the main container will be picked.</li> <li><code>-f</code> Optional, will update the printed logs periodically if set.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Retrieve and follow logs from the main container of a run: <pre><code>dhcli log -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#metrics","title":"<code>metrics</code>","text":"<p>Returns metrics for the specified resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-c container</code> Optional, ID of the container to read metrics from. If not specified, the main container will be picked.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Retrieve metrics from the main container of a run: <pre><code>dhcli metrics -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#stop","title":"<code>stop</code>","text":"<p>Stops a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Stop a run: <pre><code>dhcli stop -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#resume","title":"<code>resume</code>","text":"<p>Resumes a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Resume a run: <pre><code>dhcli resume -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#download","title":"<code>download</code>","text":"<p>Downloads a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name</code> Alternative to <code>id</code>, will download latest version.</li> <li><code>-o output_filename_or_dir</code> Optional, base directory for downloaded resources, will be created if missing.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>-n name</code>.</li> </ul> <p>Download an artifact: <pre><code>dhcli download -p my-project -o downloaded_artifacts artifact my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#upload","title":"<code>upload</code>","text":"<p>Uploads a resource. takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name</code> Must be specified when creating a new artifact.</li> <li><code>-f input_filename_or_dir</code> Mandatory, path to input file or directory.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Must be omitted for new artifacts; used to update an existing artifact.</li> </ul> <p>Upload an artifact: <pre><code>dhcli upload -p my-project -f artifacts/artifact.csv artifact -n my-artifact\n</code></pre></p>"},{"location":"components/cli/","title":"CLI","text":"<p>A command-line interface (CLI) is available, allowing access to certain functionalities of the platform remotely.</p>"},{"location":"components/cli/#installation","title":"Installation","text":""},{"location":"components/cli/#linux","title":"Linux","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli</code> file, open a shell and access this path. Run <code>dhcli -h</code> for a list of available commands.</p>"},{"location":"components/cli/#macos","title":"MacOS","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli</code> file, open a terminal and access this path. Run <code>./dhcli -h</code> for a list of available commands.</p> <p>Alternatively, the CLI can be installed using Homebrew Tap:</p> <pre><code>brew tap scc-digitalhub/digitalhub-cli https://github.com/scc-digitalhub/digitalhub-cli\nbrew install dhcli\n</code></pre>"},{"location":"components/cli/#windows","title":"Windows","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli.exe</code> file, open the command prompt and access this path. Run <code>./dhcli -h</code> for a list of available commands.</p>"},{"location":"components/cli/#cli-usage","title":"CLI usage","text":"<p>Run commands</p> <p>Depending on the shell you are using, you may have to run the CLI with <code>./dhcli</code>.</p> <p>The standard use flow of the CLI is as follows:</p> <ol> <li>Register your instance's configuration. This creates a <code>.dhcore.ini</code> file in your home directory (or, if not possible, in the current one), where the configuration will be stored, to be used and updated by subsequent commands. The register command takes an optional <code>-e environment</code> and a mandatory parameter <code>core_endpoint</code> \u2014 this is the base URL of your DigitalHub core (e.g. http://localhost:8080).</li> </ol> <pre><code>dhcli register http://localhost:8080\n</code></pre> <ol> <li>Log in. This will open a tab in your Internet browser, where you will have to carry out the log in procedure.</li> </ol> <pre><code>dhcli login\n</code></pre> <ol> <li>If you wish to install the python packages, <code>init</code> will do so, matching the platform's version.</li> </ol> <pre><code>dhcli init\n</code></pre> <p>In-detail descriptions of available commands can be found in this dedicated section.</p>"},{"location":"components/code-toolbox/","title":"Code Toolbox","text":"<p>Code toolbox is a workspace designed to support the devolopment activities using a variety of tools. Its configuration allows, in particular, for choosing</p> <ul> <li>resource configuration including the number of CPUs, memory size, disk size, and whether GPU-based node should be used (if available)</li> <li>software configuration including the support for JetBrains tools integration (e.g., IDEA, PyCharm, etc), Python version.</li> </ul> <p>Each toolbox is by default shipped with the Jupyter notebook support, VS Code Desktop integration (via preconfigured agent and VS Code extension),  VS Code Web support. </p> <p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/code-toolbox/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/code-toolbox/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/dashboard/","title":"Landing Page","text":"<p>The landing page is a central access point to reach a number of tools that are automatically run when the platform is installed. It provides access to the platform components and to the monitoring subsystem of the platform.</p> <p></p> <p>Components</p> <ul> <li>Coder, Tool for managing interactive workspaces</li> <li>DH Core Console, UI for the platform management</li> <li>KRM, or Kubernetes Resource Manager, is the tool for organizing and managing standard and custom Kubernetes resources</li> <li>Kubeflow, a tool for ML pipelines on Kubernetes</li> <li>MinIO, an S3-compatible object datalake UI</li> </ul>"},{"location":"components/dh_console/","title":"Core UI","text":"<p>The Core console is a front-end application backed by the Core API. It provides a management interface  for the organization and operations over the Data Science Projects and the associated entities, such as:</p> <ul> <li>functions of various runtimes (see the Functions and Runtimes section for details), as well as their executions (runs) grouped by the corresponding operations (tasks)</li> <li>workflows - composite pipelines combining executions of different functions</li> <li>dataitems - structured Data Items managed by the project</li> <li>artifacts - unstructured files related to and managed by the project</li> <li>models - versioned ML Model artifacts with their metrics and metadata (see ML Models section for details)</li> </ul> <p>When you access the console, you land to the project management page, where you can create or delete projects.</p> <p>Note that all the functionality that is performed via UI console through the Core API can be also performed using the platform management Python SDK reflecting management of the same platform entities.</p>"},{"location":"components/dh_console/#create-a-project","title":"Create a Project","text":"<p>Start by clicking the <code>CREATE</code> button.</p> <p></p> <p>Fill the form's properties.</p> <p></p> <p>Following the selection of a project, you can get an overview of the associated objects on its dashboard and manage them on their dedicated pages.</p>"},{"location":"components/dh_console/#dashboard","title":"Dashboard","text":"<p>The console dashboard shows the resources that have been created with a series of cards and allows you to quickly access them. You can see the runs performed and their respective status, as well as workflows, data items and functions.</p> <p></p>"},{"location":"components/dh_console/#objects","title":"Objects","text":"<p>Through the console it is also possible to manage directly the entities related to the project and perform different operations over those. This amounts not only to CRUD (create, update, delete, and read) operations, but also tracking relationships, viewing detailed metadata and versions, executing functions and pipelines, etc. </p>"},{"location":"components/dh_console/#functions","title":"Functions","text":"<p>Functions define the executable procedures implemented in various ways that can be run by the platform. Via the console the user can create new functions by selecting the corresponding runtime and, based on that, providing its specification, e.g., source code. The console lists the existing versions of each function, its specification and code (if available), as well as different tasks that can be performed over the function in the corresponding runtime. For example, in the case of Python runtime, it is possible to <code>build</code> the function (i.e., generate the corresponding Docker image and cache in the registry), to run the function as <code>job</code> (to be executed on the Kubernetes), or to <code>serve</code> the function (i.e., expose it as a service). </p> <p></p> <p>Within the tab corresponding to the specific task, the user can access the list of runs executed over the function, the status of execution, the execution log. It is also possible to create new runs of the task, defining the specific parameters and configurations. </p>"},{"location":"components/dh_console/#workflows","title":"Workflows","text":"<p>Workflows represent a composition of function executions that is run over the platform, specifying their dependencies (in terms of data and order). This allows for creating complex pipelines for AI/ML and data operations. Currently, the implementation of the workflow relies on the Kubeflow Pipelines framework, that in turn relies on Kubernetes Argo Workflows so that each step of the workflow is executed as a single Kubernetes Job. </p> <p>From the console it is possible to define a new workflow by providing the code of the pipeline and running the <code>pipeline</code> task. As in case of function runs, the execution of the pipeline is being tracked, as well as the progress of single steps, and the corresponding log. </p>"},{"location":"components/dh_console/#dataitems","title":"Dataitems","text":"<p>Through Dataitems the project may define the relevant structured and semi-structured datasets. Dataset may created manually, starting from a reference to an URL of the file or DB table, or may be produced as a result of some data transformation function execution. As in case of the functions, the dataitems are equipped with the relevant metadata (e.g., creation and changes, tags, ownership, etc). Furthermore, the datasets structured as tables (i.e., <code>table</code> kind datasets) are equipped with the derived schemas and profiling and data preview.</p> <p>Through the console it is possible to manage the datasets, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#artifacts","title":"Artifacts","text":"<p>Similar to dataitems, Artifacts it is possible to explicitly capture the relevant unstructured objects and files. Artifacts may be of arbitrary type, and equipped with a generic metadata properties.  </p> <p>Through the console it is possible to manage the artifacts, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#ml-models","title":"ML Models","text":"<p>ML Models represent a specific type of artifacts, which are produced by the AI tranining activites and represent the datasets used for inference operations. While managed in the same manner as other types of entities, ML Models may have a specific set of metadata and specification attributes, such model kind, metrics, algorithm and framework specification, etc. </p> <p>ML Models are further used by the inference services.</p>"},{"location":"components/dh_console/#secrets","title":"Secrets","text":"<p>When executing operations with the platform, the execution might need access to some sensitive values, for example to access data residing on a data-store that requires credentials (such as a private S3 bucket), access a private repository, or many other similar needs.</p> <p>The platform provides the functionality to manage these values, reffered to as Secrets, both through UI and SDK, where it is possible to associate the key-value pair to the project. The data is managed as Kubernetes secrets and is embedded in the execution of a run that relies on that.</p> <p>The management of secrets allows through the console to create, delete, and read the secret values.</p>"},{"location":"components/dh_console/#versioning","title":"Versioning","text":"<p>All entities operated by Core are versioned. When you view the details of an object, all of its versions are listed and browsable. Moreover, when you view a dataitem, its schema and data preview are available.</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with your Coder username and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from [Coder, using its template]../tasks/workspaces.md). After launching it from Coder you can access Grafana on Grafana UI (http://nodeipaddress:30110).</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> </li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> </li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/kubeai/","title":"KubeAI and OpenWebUI","text":"<p>KubeAI is an AI Inference Operator for deploy and scale machine learning models on Kubernetes currently built for LLMs, embeddings, and speech-to-text.</p> <p>KubeAI does not depend on other systems like Istio &amp; Knative (for scale-from-zero), or the Prometheus metrics adapter (for autoscaling). This allows KubeAI to work out of the box in almost any Kubernetes cluster. Day-two operations is greatly simplified as well - don't worry about inter-project version and configuration mismatches.</p> <p>To accomplish this, KubeAI as a part of the platform is offered with these primary sub-components:</p> <ol> <li> <p>The model proxy: the KubeAI proxy provides an OpenAI-compatible API. Behind this API, the proxy implements a prefix-aware load balancing strategy that optimizes for KV the cache utilization of the backend serving engines (i.e. vLLM). The proxy also implements request queueing (while the system scales from zero replicas) and request retries (to seamlessly handle bad backends).</p> </li> <li> <p>The model operator: the KubeAI model operator manages backend server Pods directly. It automates common operations such as downloading models, mounting volumes, and loading dynamic LoRA adapters via the KubeAI Model CRD. The KubeAI operator abstract the concepts of specific implementations and tasks providing a common specification model for defining models under different engines (OLlama, vLLM, Infinity, FasterWhisper).</p> </li> <li> <p>Open WebUI component - an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution. Its interface is integrated with the SSO authentication adopted by the platform and provides the management tools to expose and test the AI models. </p> </li> </ol>"},{"location":"components/kubeai/#exposing-ai-models-with-kubeai","title":"Exposing AI models with KubeAI","text":"<p>Currently, KubeAI allows for serving the LLM models for text generation and text embedding and speech-to-text processing, in line with the OpenAI specification. To expose a model, it is possible to use Python SDK or directly the Core UI. More specifically,</p> <ul> <li>it is necessary to define the corresponding KubeAI function (of <code>kubeai-text</code> or <code>kubeai-speech</code> kinds), define a set of parameters, like the task (feature) to perform, reference to an engine (e.g., vLLM or OLlama for text generation), reference to a model to expose (HuggingFace id, OLlama model, or S3 resource reference) and model name.</li> <li>activate serving operation on the function passing extra parameters (like engine arguments, HW resource profile and number of GPUs, secrets and/or envirnoment variables).</li> </ul> <p>Once deployed, the serving run provides the information about the model name (which is randomized to avoid naming clashes), exposed endpoints and status. Depending on your environment, the deployment may require some time for the model to be operative.</p> <p></p> <p>It is also possible to access the exposed model API through the KubeAI model proxy. The proxy exposes the OpenAI-compatible endpoints in function of the specified task. See the KubeAI documentation on how to use and access the models, to integrate this with the client libraries and applications. </p>"},{"location":"components/kubeai/#testing-and-using-the-models-with-open-webui","title":"Testing and using the models with Open WebUI","text":"<p>To simplify the testing and usage of the model, we provide Open WebUI instance integrated with the platform and configured to access the KubeAI models. </p> <p></p> <p>The possibility to access and test the models with Open WebUI depends, however, on the configuration of the latter. By default, the installation foresees two roles - admin and user. The former can configure the tool functionality, users, groups, and the model visibility (public, private or group-scoped). The deployed model are not available to the users unless the admin changes their visibility.</p>"},{"location":"components/kubeai/#see-also","title":"See Also","text":"<ul> <li>How to manage LLM Models with KubeAI Runtime</li> <li>How to manage speech-to-text models with KubeAI Runtime</li> <li>Model Serving Runtime Reference</li> </ul>"},{"location":"components/kubeflow/","title":"KubeFlow Pipelines","text":"<p>Kubeflow Pipelines makes part of the Kubeflow platform and allows for organizing workflows out of single tasks performed as Kubernetes Jobs via Argo Workflows. Kubeflow Pipelines comes with its own DSL specification on top of Python, which is compiled into a workflow definition ready for execution in Kubernetes. In this way wach task, its resources, dependencies, etc may be configured indipendently; the management and tracking is performed by the Kubeflow Pipelines component, equipped also with the Web-based UI for monitoring.  </p> <p>The platform used Kubeflow pipelines to implement the composite pipelines through its Core orchestrator component and UI.</p> <p>Currently, version v1 of the Kubeflow Pipelines is used for the compatibility purposes. The definition of the KFP workflows is provided in the corresponding KFP Runtime section.</p> <p>How to access</p> <p>Kubeflow Pipelines UI may be accessed from the dashboard. From its interface, you will be able to monitor the deployed workflows and their executions.</p>"},{"location":"components/kubeflow/#resources","title":"Resources","text":""},{"location":"components/kubeflow/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/resourcemanager/","title":"Kubernetes Resource Manager","text":"<p>Kubernetes Resource Manager (KRM) is an application to manage several types of Kubernetes resources:</p> <ul> <li>Custom resources</li> <li>Services</li> <li>Deployments</li> <li>Volumes</li> <li>Jobs</li> </ul> <p>It consists in a back-end, written in Java, which connects to the Kubernetes API to perform actions on resources, and a front-end, written in React and based on React-admin.</p> <p>Instructions on how to install and start an instance can be found on the repository.</p>"},{"location":"components/resourcemanager/#standard-kubernetes-resources","title":"Standard Kubernetes Resources","text":"<p>With KRM you can control the main Kubernetes resources (e.g., services, deployments), manage Persistent Volume Claims, and access secrets. Click the corresponding button in the left menu, and view the details of one item by clicking its Show button.</p>"},{"location":"components/resourcemanager/#custom-resources","title":"Custom resources","text":"<p>Custom resources can be viewed, created, edited and deleted through the use of the interface. </p> <p>If you don't see a specific kind of custom resource listed to the left, it means neither Kubernetes nor KRM contain a schema for it. A schema is required so that the application may understand and describe the related resources.</p> <p>If some resources already exist, they will immediately be visible.</p>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"runtimes/asrserve/","title":"Speech to Text Serving Runtime","text":"<p>Speech to Text serving runtime (kubeai-speech) aims at supporing the possibility to expose the automated speech recognition functionality as OpenAI-compatible transcriptions API. </p> <p>For this purpose the runtime that relies on KubeAI operator to expose model using the FasterWhisper engine. The serving is performed by KubeAI as in case of KubeAI Text runtime. </p> <p>The specification of the KubeAI speech runtime amounts to defining</p> <p>model URL (from S3 storage or from HuggingFace catalog, e.g., <code>hf://Systran/faster-whisper-medium.en</code>) - name of the model to expose - optional base image for serving</p> <p>The <code>serve</code> action allows for deploying the model, and a set of extra properties may be configured, including</p> <ul> <li>inference server-specific arguments</li> <li>load balancing strategy and properties</li> <li>scaling configuration (min/max/default replicas, scale delays and request targets)</li> <li>Resource confguration (e.g., run profile), environments and secrets (e.g., reference to <code>HF_TOKEN</code> if needed for accessing Huggingface resources)</li> </ul>"},{"location":"runtimes/asrserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK runtime documentation for more information.</p>"},{"location":"runtimes/container/","title":"Container runtime","text":"<p>The Container runtime allows you to create deployments, jobs and services on Kubernetes starting from arbitrary user-defined container image. That is, container function may be run</p> <ul> <li>as a Job (<code>action=\"job\"</code>) representing a single piece of work (e.g., model training, data processing), or</li> <li>as a \"Service (<code>action=\"serve\"</code>) representing a Serverless function responding to HTTP requests, or</li> <li>as a \"Deployment (<code>action=\"deploy\"</code>) representing a deployment without exposing it as a service.</li> </ul> <p>Each container function is defined with</p> <ul> <li>image specification</li> <li>command to run within container. If not provided, the default entry point is triggered.</li> </ul> <p>To customize the image to a specific context, it is possible to perform <code>build</code> operation on the function. This operation creates a container image starting from the base image and optional list of additional instructions. Next time the Job or Service starts, this prebuilt container image will be used for execution.</p> <p>When the run is created, it is possible to specify the following information</p> <ul> <li>resource configuration (see here for details about configuring run resources)</li> <li>file system properties (e.g., <code>run_as_user</code>, <code>fs_group</code>, <code>run_as_group</code>)</li> <li>container arguments defining the specific execution parameters to pass to the container entry point or command.</li> </ul> <p>The details about the specification, parameters, execution, and semantics of the container runtime may be found in the SDK Container Runtime reference.</p>"},{"location":"runtimes/container/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK container runtime documentation for more information.</p>"},{"location":"runtimes/dbt/","title":"DBT runtime","text":"<p>The DBT runtime allows you to run DBT transformations on your data. It is a wrapper around the DBT CLI tool.</p> <p>In a nutshell, DBT transformation in the platform aims at performing a simple transformation of a tabular data (e.g., data item) to another form using SQL-based operations, such as the one represented in the following example:</p> <pre><code>with customers as (\n\n    select\n        id as customer_id,\n        first_name,\n        last_name\n\n    from {{ ref('customers') }}\n\n),\n\norders as (\n\n    select\n        id as order_id,\n        user_id as customer_id,\n        order_date,\n        status\n\n    from {{ ref('shop_orders') }}\n\n),\n\ncustomer_orders as (\n\n    select\n        customer_id,\n\n        min(order_date) as first_order_date,\n        max(order_date) as most_recent_order_date,\n        count(order_id) as number_of_orders\n\n    from orders\n\n    group by 1\n\n),\n\nfinal as (\n\n    select\n        customers.customer_id,\n        customers.first_name,\n        customers.last_name,\n        customer_orders.first_order_date,\n        customer_orders.most_recent_order_date,\n        coalesce(customer_orders.number_of_orders, 0) as number_of_orders\n\n    from customers\n\n    left join customer_orders using (customer_id)\n\n)\n\nselect * from final\n</code></pre> <p>The DBT runtime function is defined with the SQL transformation code to be executed.</p> <p>The only DBT action supported by the runtime is <code>transform</code>, which  takes references to the data items as inputs, applies the operation using the DBT executable, and creates resulting data item under the name defined in the output mapping.</p> <p>The DBT transformation may be executed both in the platform and locally (provided DBT executable is available in your environment).</p>"},{"location":"runtimes/dbt/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK DBT runtime documentation for more information.</p>"},{"location":"runtimes/fl/","title":"Federated Learning: Flower Runtime","text":"<p>Flower Runtime represents the extension of the platform to support the scenarios of federated and compositional Machine Learning going potentially beyond the scope of the platform instance and making different instances or external environments collaborate.</p> <p>This runtime relies on Flower Framework and allows for deploying and running Flower FL applications in different modes. The details about Flower architecture, concepts, and applications may be found in the corresponding specificiation. The Flower runtime integrated in the platform is agnostic to a specific application scenario and supports the following activities</p> <ul> <li>deploy the nodes of the application federation referred to as Flower <code>SuperNode</code>. The SuperNode represents a single participating element and controls computations of a single application part. In a fully distributed scenario a single node is deployed in platform instance for the given application, while the other nodes may belong to other instances. This is a typical case, e.g., in a medical domain where different tenants collaborate in order to build a single ML model without revealing proprietary data. Other scenario may represent a situation of a distributed paralllel computation, where different nodes run all in the same namespace in parallel each dealing with a dedicated data partition. </li> <li>deploy the central server node of the  application federation referred to as Flower <code>SuperLink</code>. The superlink is the coordinator of the federation; all the supernode connect to it in order to pass the intermediate weights or get the new training instructions. </li> <li>Run the specific application that consists of two parts - server coordination code and the client supernode training part. The application is passed to the superlink that distributes the client code to the supernodes of the federation and coordinates the execution iterations.</li> </ul> <p>Accordingly, the runtime consists of the following elements: </p> <ul> <li><code>flower-client</code> runtime to define the Flower SuperNode entity. Built upon standard base image, the Flower SuperNode allows for defining additional Python dependencies necessary for the application client part execution.</li> <li><code>flower-server</code> runtime to define the Flower SuperLink entity. Built upon standard base image, the Flower SuperLink allows for defining additional Python dependencies necessary for the application server part execution.</li> <li><code>flower-app</code> runtime to define the application to be executed. Requires the application code to be defined, which may be done in two different ways. First, it is possible to provide a reference to the full project (e.g., as git repository reference). The project should be defined as a standard Flower applciation with the corresponding <code>pyproject.toml</code> specification available in the project root. Second, it is possible to provide the code of client and server parts and reference to the ClientApp and ServerApp objects.</li> </ul>"},{"location":"runtimes/fl/#flower-server-runtime-execution","title":"Flower-server runtime execution","text":"<p>The SuperLink node defined with the corresponding configuration supports two actions: <code>build</code> and <code>deploy</code>.  The first one creates a container image, which is useful if there are many dependencies to be integrated or custom instructions should be provided to the image.</p> <p>The deploy tasks creates a new deployment and exposes the correponding interfaces - for the app execution and monitoring, and for the SuperNode nodes to exchange the training information and instructions. The information about the exposed endpoints are available as a part of the <code>service</code> status of the deployment run. </p> <p>The SuperLink deployment parameters include - resource and environment parameters (e.g., resource profiles, secrets, variables, volumes) - list of public keys corresponding to the SuperNode nodel that SuperLink will accept. If specified, the SuperLink will accept the only the correpsonding clients. If not specified, any SuperNode will be accepted to the federation and to the training procedure. - insecure flag to disable TLS verification (for single namespace or test purpose only)</p> <p>Once deployed, the SuperLink is ready to accept the application run execution requests.</p>"},{"location":"runtimes/fl/#flower-client-runtime-execution","title":"Flower-client runtime execution","text":"<p>The SuperNode node defined with the corresponding configuration supports two actions: <code>build</code> and <code>deploy</code>.  The first one creates a container image, which is useful if there are many dependencies to be integrated or custom instructions should be provided to the image.</p> <p>The deploy tasks creates a new deployment that connects to the specified SuperLink and ready to accept the application code.</p> <p>The SuperNode deployment parameters include</p> <ul> <li>SuperLink endpoint: the address of the server node to connect to. </li> <li>isolation mode: <code>subprocess</code> (default) meaning that the client code will be executed as a subprocess of the Node execution or <code>process</code> meaning that the client is executed elsewhere (e.g., as a separated container or application) and connects to the SuperNode through a dedicated gRPC port.</li> <li>optional private and public code of the node to communicate securely with the SuperLink. If not specified, it is assumed that any client may connect to the server node without restrictions.</li> <li>Root TLS certificate of the server node. If specified, a secure TLS communication is established between the server and client. If not specified, the communication s performed in \"insecure\" mode if supported by the server.</li> <li>resource and environment parameters (e.g., resource profiles, secrets, variables, volumes).</li> <li>dictionary of client node configuration parameters (key-value pairs) specific to the node environment.</li> </ul> <p>Once deployed, the SuperNode connects to the SuperLink and waits for the training iterations, making polling requests to the SuperLink. In case of <code>process</code> isolation mode it exposes the corresponding client gRPC port ready for client application communication.</p>"},{"location":"runtimes/fl/#flower-app-runtime-execution","title":"Flower-app runtime execution","text":"<p>The app specification define the actual application to be executed by the federation. The corresponding <code>train</code> action performs the following steps following Flower procedure:</p> <ul> <li>the <code>train</code> execution packs the application archive and calls the corresponding SuperLink API for the application deployment and execution;</li> <li>SuperLink passes the code to the clients and triggers the execution following the execution configuration;</li> <li>the <code>train</code> execution receives the confirmation from the SuperLink and the execution ID. The execution is being monitored and the status of the train execution is updated upon completion.</li> </ul> <p>The execution parameters of the action include</p> <ul> <li>name of the federation to use. Optional if the code is inline and the auto-generated federation name is used. Otherwise should correspond to the name defined in the application <code>pyproject.toml</code>.</li> <li>SuperLink endpoint to connect to with the execution API port.</li> <li>SuperLink root TLS certificate if defined by the SuperLink node to establish secure connection. </li> </ul>"},{"location":"runtimes/fl/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK flower runtime documentation for more information.</p>"},{"location":"runtimes/hera/","title":"Hera Pipelines Runtime","text":"<p>The Hera runtime allows you to run workflows within the platform. More specifically, Hera represents a workflow orchestration in Python powered by Argo Workflows in Kubernetes. </p> <p>Argo workflows are defined as Kubernetes Custom Resource and represent a composition of a Kubernetes Jobs with data and parameter passing support. </p> <p>Hera usess Python-based DSL to define the execuition tasks of a workflow that is then transformed into Argo workflow specification and may be executed through the platform. </p> <p>In addition to Here SDK and its DSL, we provide an extension of the platform SDK to run single functions of other runtimes directly as nodes of the Hera workfflow. </p> <p>Here runtime function definition is therefore composed of the Hera pipeline code in Python. The two actions supported by the runtime are</p> <ul> <li><code>build</code> action necessary to compile the Hera DSL specification into Argo Workflow definition. Note that this action should be always executed before the actual pipeline execution.</li> <li><code>pipeline</code> action that performs the actual instantiation and execution of the workflow within the platform using Argo Workflow operator. It is possible to pass the parameters to the pipelines. </li> </ul> <p>The following figure represents the execution graph of an example worklfow in the platform:</p> <p></p> <p>Hera runtime replaces the Kubeflow Pipeline runtime for the same Argo workflow-based functionality.</p>"},{"location":"runtimes/hera/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK Hera runtime documentation for more information.</p>"},{"location":"runtimes/kfp_pipelines/","title":"KFP Pipelines Runtime","text":"<p>The kfp runtime allows you to run workflows within the platform.</p>"},{"location":"runtimes/kfp_pipelines/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK KFP runtime documentation for more information.</p>"},{"location":"runtimes/llmserve/","title":"LLM Model Serving Runtime","text":"<p>LLM Model serving runtime aims at supporing the possibility to expose the LLM models as OpenAI-compatible APIs. For this purpose several different runtimes are available. Depending on the specific scenario requirements, the user may choose one or another approach.</p> <ul> <li>HuggingFace Serving (<code>huggingfaceserve</code>) runtime exposes standalone LLM models using KServe-based implementation. In a nutshell, this runtime allows for exposing LLMs using the vLLM engine. The engine supports, in particular, completions and chat completions APIs compatible with the OpenAI protocol, embedding, and a series of other functions (like embeddings, fill mask, classification) using Open Inference Protocol. See corresponding kserve documentation for the details. </li> <li>KubeAI-Text Serving (<code>kubeai-text</code>) runtime that relies on KubeAI operator to expose model. Differently to the previous one, KubeAI serving deploys model while their serving is performed by KubeAI through a single channel. Also, this runtime relies on different engine, including vLLM, OLLama, and Infinity for different tasks. KubeAI supports also serving multiple LoRA adapters, autoscaling, and many other useful options for production-ready environments. </li> </ul>"},{"location":"runtimes/llmserve/#huggingface-serve-runtime","title":"Huggingface Serve runtime","text":"<p>The specification of the HuggingfaceServe runtime functions consists of the following elements:</p> <ul> <li><code>path</code> defining the URL of the model, either from the platform storage or from HuggingFace catalog (e.g., 'huggingface://Qwen/Qwen2.5-0.5B')</li> <li><code>model</code> defining the name of the exposed model </li> <li>and optional base image to use for serving the model if different from the one used by the platform by default</li> </ul> <p>The runtime supports the <code>serve</code> action that may specify further deployment details including</p> <ul> <li>backend engine type (vLLM or custom Kserve implementation called \"huggingface\")</li> <li>inference task (e.g., <code>sequence_classification</code>, <code>fill_mask</code>, <code>text_generation</code>, <code>text_embedding</code>, etc)</li> <li>Specific parameters refering to the context length, data types, logging properties, tokenizer revision, engine args, etc.</li> <li>Resource confguration (e.g., run profile), environments and secrets (e.g., reference to <code>HF_TOKEN</code> if needed for accessing Huggingface resources)</li> </ul> <p>Once deployed, a model is exposed with the corresponding Kubernetes service. The sevice endpoint is avaialble as a part of the <code>status/service</code> data of the run.</p>"},{"location":"runtimes/llmserve/#kubeai-text-runtime","title":"KubeAI Text runtime","text":"<p>KubeAI Text runtime relies on KubeAI platform for model serving. In this case, for each serve action performed with this runtime a corresponding deployment is created, while no dedicated service is exposed - the models are service by KubeAI service directly.</p> <p>There are different advantages of using KubeAI deployment, which include</p> <ul> <li>possibility to use multiple backend engines optimized for different goals. For example OLLama is best suited for testing models without GPU, while vLLM suites better for GPU-based environments.</li> <li>possibility to serve multiple models simultaneously through LoRA adapters. In this case one single base model + a list of different fine-tuned adapters are served on the same resources, while being made available indipendently.</li> <li>possibility to configure more efficient resource management with autoscaling and scale profiles.</li> <li>use of configurable prefix caching.</li> <li>Full OpenAI compatibility (completions, chat completions, embeddings)</li> </ul> <p>The specification of the KubeAI text runtime amounts to defining</p> <ul> <li>base model URL (from S3 storage or from HuggingFace catalog)</li> <li>list of adapters (from S3 storage or from HuggingFace catalog)</li> <li>name of the model to expose</li> <li>Model task or feature: text generation (default) or embedding</li> <li>Backend engine: vLLM, OLLama, or Infinity (for embeddings only)</li> <li>optional base image for serving</li> </ul> <p>The <code>serve</code> action allows for deploying the model and adapters, and a set of extra properties may be configured, including</p> <ul> <li>inference server-specific arguments</li> <li>load balancing strategy and properties</li> <li>prefix cache length</li> <li>scaling configuration (min/max/default replicas, scale delays and request targets)</li> <li>Resource confguration (e.g., run profile), environments and secrets (e.g., reference to <code>HF_TOKEN</code> if needed for accessing Huggingface resources)</li> </ul> <p>Using GPU for model seving</p> <p>Please note that in case of large models for text generation task the usage of the corresponding GPU-based profiles may be required.</p> <p>When deployed, the corresponding <code>serve</code> run specification contains extra information for using the LLM model. This includes</p> <ul> <li>the base URL of the kube AI environment to use by the clients</li> <li>the name of the deployed model and adapters to be used in the OpenAI requests</li> <li>LLM metadata - feature information, engine, base model, etc</li> </ul>"},{"location":"runtimes/llmserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK runtime documentation for more information.</p>"},{"location":"runtimes/mlflowserve/","title":"MLFlow serve runtime","text":"<p>MLFLow serving runtime is used to expose ML model created and packaged using the MLFLow format. When packaged, the MLFlow model contains all the necessary information for its deployment, including not only the model weights, but also the specification of the dependencies and versions, as well as the serving functions for the model. All this allows for great deployment flexibility without a need to define the custom functions.</p> <p>o define the MLFLow serving function it is necessary to provide</p> <ul> <li><code>path</code> defining a reference to the model (e.g., S3 URL pointing to the model content)</li> <li><code>model</code> defining the name of the exposed model</li> <li>optional serving image if different from the one used by the platform by default.</li> </ul> <p>The <code>serve</code> action of the runtime creates a dedicated deployment and exposes the model as a Open Inference Protocol API. The standard resource- and service-related configuration may be specified.</p>"},{"location":"runtimes/mlflowserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK python runtime documentation for more information.</p>"},{"location":"runtimes/modelserve/","title":"Modelserve runtime","text":"<p>The Modelserve runtime allows you to deploy ML models on the platform.</p>"},{"location":"runtimes/modelserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK modelserve runtime documentation for more information.</p>"},{"location":"runtimes/overview/","title":"Runtime Overview","text":"<p>Runtimes define how the platform operationalizes the executable entities. The entities, or Functions are the logical description of something that the platform may execute, being a job, an ML model inference service, workflow, etc. A way a specific function is defined and executed depends on runtime. For example, a function may be defined as a piece of program written in Python, while the corresponding Python runtime defines how this code will be packaged into a container and executed in the underlying Kubernetes infrastructure. </p> <p>Runtime also defines the operations actions (also referred to as \"tasks\") that we can perform over these functions, such as job execution, deploy, container image build. A single action execution is called run, and the platform keeps track of these runs, with metadata about function version, operation parameters, and runtime parameters for a single execution.</p> <p>Runtimes, therefore, are highly specialized components which can translate the representation (spec) of a given execution, as expressed in the run, into an actual execution operation performed via libraries, code, external tools etc. Runtimes define the key point of extension of the platform: new runtimes may be added in order to implement the low-level logic of \"translating\" the high level operation definition into an executable run. </p> <p>In general, the executables may be divided into the following categories:</p> <ul> <li>Jobs or single pieces of work, representing, e.g., a data transformation, quality check, model training. Once complete, the Job run is being destroyed and the resources are cleaned.</li> <li>Services (or deployments in general) representing a continuously running executable that possibly exposes some functionality as API. Example of a service may be a Serverless Python function, an LLM model with OpenAI-compatible interface, an inference server, etc.</li> <li>Workflow or a composite pipeline that integrates a series of jobs into a single procedure. </li> </ul> <p>Different runtimes follows these separation and provides actions (e.g., job action, serve action) to create and manage these executables. THe executables therefore may be started manually by the user, autamatically by a trigger, or as a part of a workflow.</p>"},{"location":"runtimes/python/","title":"Python","text":"<p>The python runtime allows you to run generic python function within the platform in two different ways:</p> <ul> <li>as a Job (<code>action=\"job\"</code>) representing a single piece of work (e.g., model training, data processing), or</li> <li>as a \"Service (<code>action=\"serve\"</code>) representing a Serverless function responding to HTTP requests.</li> </ul> <p>Each Python runtime function is defined with</p> <ul> <li>source code, being an inline python code, a reference to the git repository, or a zip archive. The source code should provide also a reference to the <code>handler</code> - the procedure to be called (i.e., specific python function to be executed). In case of Job, this will be the operation executed by the platform. In case of Service this will be an operation called upon receiving the HTTP request with the request entity as a payload. Additionally, in case of Services, it is possible to specify the <code>init</code> operation that will be called once upon the service start.</li> <li>Python version (supported by the platform).</li> <li>optional list of Python dependencies and optionally a custom base image to be used.</li> </ul> <p>To facilitate the operation start and optimize the use of resources, it is possible to perform <code>build</code> operation on the function. This operation creates a container image starting from the source code, dependency list and optional list of additional instructions. Next time the Job or Service starts, this prebuilt container image will be used for execution.</p> <p>The function definition in the Core UI looks as follows:</p> <p></p> <p>Here the function 'test' defines a simple transformation of an external CSV file represented with <code>download_and_process</code> operation, which returns a dataframe that will be stored by the platform as <code>world-cities</code> Data Item. </p> <p>When the action run is created, it is possible to specify the following information</p> <ul> <li>resource configuration (see here for details about configuring run resources)</li> <li>parameters as a key-value dictionary (to be passed in input to the Job execution or to <code>init</code> operation of the Service execution)</li> <li>inputs as a key-value dictionary with the references to the platform entities (more precisely, their unique <code>key</code> values), such as artifacts or dataitems. They are treated explicitly by the platform and allow for reconstructing the lineage of data within the project.</li> </ul> <p>Please note that the Python runtime allows both for the execution of a Job function both in the platform (more precisely, in the underlying Kubernetes infrastructure) and locally (with <code>local_execution=True</code>). This latter modality allows for testing/debugging of the function in the development environment if there are resources available for performing such execution.  </p> <p>The details about the specification, parameters, execution, and semantics of the Python runtime may be found in the SDK Python Runtime reference.</p>"},{"location":"runtimes/python/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK python runtime documentation for more information.</p>"},{"location":"runtimes/sklearnserve/","title":"Scikit-Learn serve runtime","text":"<p>Scikit-Learn serving runtime is used to expose simple inference models created with Scikit-learn framework using Open Inference Protocol. It is expected that the model is packaged as pickle or joblib file and may be served directly, by deserializing model with the sklearn python library and creating a standard Open Inference API out of the box.</p> <p>More specifically, to define the Scikit-Learn serving function it is necessary to provide</p> <ul> <li><code>path</code> defining a reference to the packaged model</li> <li><code>model</code> defining the name of the exposed model</li> <li>optional serving image if different from the one used by the platform by default.</li> </ul> <p>The <code>serve</code> action of the runtime creates a dedicated deployment and exposes the model as a Open Inference Protocol API. The standard resource- and service-related configuration may be specified.</p>"},{"location":"runtimes/sklearnserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK python runtime documentation for more information.</p>"},{"location":"scenarios/data_validation/scenario/","title":"Data validation","text":"<p>This scenario implements a simple data validation function, which evaluates the correctness of a CSV table by leveraging an open source library, Frictionless.</p> <p>The function will read a CSV file and then produce a report, along with a LABEL marking the dataset as <code>VALID</code> or <code>INVALID</code>.</p>"},{"location":"scenarios/data_validation/scenario/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries:</p> <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre> <p>Create a project:</p> <pre><code>PROJECT = \"validation\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre>"},{"location":"scenarios/data_validation/scenario/#function-definition","title":"Function definition","text":"<p>Define the function by writing the source code and registering it via sdk.</p> <pre><code>%%writefile \"validate.py\"\n\n\nimport digitalhub as dh\nfrom digitalhub_runtime_python import handler\nfrom frictionless import Checklist, validate\nimport os\n\n@handler(outputs=[\"report\"])\ndef main(project, di):\n    # download as local file\n    path = di.download(overwrite=True)\n    # validate\n    report = validate(path)\n    # update artifact with label    \n    label = \"VALID\" if report.valid else \"INVALID\"\n    di.metadata.labels = di.metadata.labels.append(label) if di.metadata.labels else [label]\n    di.save(update=True)    \n    #cleanup\n    os.remove(path) \n\n    with open(\"report.json\", \"w\") as f:\n      f.write(report.to_json())\n\n    project.log_artifact(kind=\"artifact\", name=f\"{di.name}_validation-report.json\", source=\"report.json\")\n\n    # persist report\n    return report.to_json()\n</code></pre> <p>And then</p> <pre><code>func = project.new_function(name=\"validate-csv\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\",\n                            requirements=[\"frictionless\"],\n                            code_src=\"validate.py\",\n                            handler=\"main\")\n</code></pre> <p>The function can be tested by passing a DataItem as input. For example, we can register an URL and use it as source for a quick run.</p> <pre><code>URL = \"https://raw.githubusercontent.com/frictionlessdata/frictionless-py/refs/heads/main/data/capital-invalid.csv\"\ndi = project.new_dataitem(name=\"capital-invalid.csv\",\n                          kind=\"table\",\n                          path=URL)\n</code></pre> <p>And then execute the function:</p> <pre><code>run = func.run(\"job\",\n               inputs={'di': di.key},\n               wait=True)\n</code></pre> <p>The result will be the execution of the function as a batch job, producing a report in JSON format stored as artifact in the repository. Additionally, the function will append an <code>INVALID</code> label to the data item.</p>"},{"location":"scenarios/data_validation/scenario/#trigger","title":"Trigger","text":"<p>We set up a trigger to automatically run the validate function when a CSV file is uploaded as a data item.</p> <p>Create the trigger:</p> <pre><code>func.trigger(\"job\",\n             \"lifecycle\",\n             \"csv-trigger\",\n             states=[\"READY\"],\n             key=f\"store://{PROJECT}/dataitem/table/*\",\n             template={\"inputs\": {\"di\": \"{{input.key}}\"}})\n</code></pre> <p>Go to the console and try creating a data item by selecting <code>table</code> as kind and uploading any CSV file. When the data item is READY, the function will be run and the report artifact will be generated.</p>"},{"location":"scenarios/dremio_grafana/scenario/","title":"Data transformation and usage with Dremio and Grafana","text":"<p>In this scenario we will learn how to use Dremio to transform data and create some virtual datasets on top of it. Then, we will visualize the transformed data in a dashboard created with Grafana, by importing a template. For this template to work with minimal changes, make sure you match the naming of entities indicated throughout the tutorial.</p> <p>In order to collect the initial data and make it accessible to Dremio, we will follow the first step of the ETL scenario, in which we download some traffic data and store it in the DigitalHub datalake.</p>"},{"location":"scenarios/dremio_grafana/scenario/#collect-the-data","title":"Collect the data","text":"<p>Collect the data</p> <p>The process of collecting data is the same as described in the ETL scenario introduction and Collect the data pages.</p> <ul> <li>Access Jupyter from your Coder instance and create a new notebook using the <code>Python 3 (ipykernel)</code> kernel</li> <li> <p>Set up the environment and create a project named <code>demo-etl</code> <pre><code>import digitalhub as dh\nimport os\n</code></pre> <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p> </li> <li> <p>Create the <code>src</code> folder, define the download function and register it <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre> <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef downloader(url):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(file_format='csv',sep=\";\")\n    return df\n</code></pre> <pre><code>func = project.new_function(\n                         name=\"download-data\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_10\",\n                         code_src=\"src/download-data.py\",\n                         handler=\"downloader\")\n</code></pre></p> </li> <li> <p>Set the URL and execute the function: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi= project.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n</code></pre> <pre><code>run = func.run(action=\"job\", inputs={'url':di.key}, outputs={\"dataset\": \"dataset\"}, local_execution=True)\n</code></pre></p> </li> </ul>"},{"location":"scenarios/dremio_grafana/scenario/#access-the-data-from-dremio","title":"Access the data from Dremio","text":"<p>Access Dremio from your Coder instance or create a new Dremio workspace. You should see S3 already configured as an object storage and you should find the downloaded data in a .parquet file at the path <code>S3/datalake/demo-etl/dataitem/dataset/0eed9ced-5f04-4f12-8494-763926070835/dataset.parquet</code>. The auto-generated <code>id</code> before <code>/dataset.parquet</code> will be different for you.</p> <p>Click on the file to open its Dataset Settings, verify that the selected format is <code>Parquet</code> and click Save. It will be saved as a Dremio dataset, so that it can be queried.</p> <p>Now you can run SQL queries against the dataset. Try the following (update the <code>id</code> to match your own):</p> <pre><code>SELECT *\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\nORDER BY data, \"codice spira\"\n</code></pre> <p>Create a new Dremio space named <code>demo_etl</code>. We will create three virtual datasets and save them here.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-measurement-data","title":"Extract measurement data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic measurements to save them as a separate dataset:</p> <pre><code>SELECT \"dataset.parquet\".data, \"dataset.parquet\".\"codice spira\", \"00:00-01:00\", \"01:00-02:00\", \"02:00-03:00\", \"03:00-04:00\", \"04:00-05:00\", \"05:00-06:00\", \"06:00-07:00\", \"07:00-08:00\", \"08:00-09:00\", \"09:00-10:00\", \"10:00-11:00\", \"11:00-12:00\", \"12:00-13:00\", \"13:00-14:00\", \"14:00-15:00\", \"15:00-16:00\", \"16:00-17:00\", \"17:00-18:00\", \"18:00-19:00\", \"19:00-20:00\", \"20:00-21:00\", \"21:00-22:00\", \"22:00-23:00\", \"23:00-24:00\"\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as (top right), select Save View as..., name the new dataset <code>misurazioni</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-traffic-sensors-data","title":"Extract traffic sensors data","text":"<p>From the SQL runner, execute the following query, which will extract the traffic sensors data (e.g. their geographical position) as a separate dataset:</p> <pre><code>SELECT DISTINCT \"dataset.parquet\".\"codice spira\", \"dataset.parquet\".tipologia, \"dataset.parquet\".id_uni, \"dataset.parquet\".codice, \"dataset.parquet\".Livello, \"dataset.parquet\".\"codice arco\", \"dataset.parquet\".\"codice via\", \"dataset.parquet\".\"Nome via\", \"dataset.parquet\".\"Nodo da\", \"dataset.parquet\".\"Nodo a\", \"dataset.parquet\".stato, \"dataset.parquet\".direzione, \"dataset.parquet\".angolo, \"dataset.parquet\".longitudine, \"dataset.parquet\".latitudine, \"dataset.parquet\".geopoint\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\n</code></pre> <p>Select Save View as... again (do not overwrite the previous one), name the new dataset <code>spire</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#transform-hourly-measurements-into-daily-measurements","title":"Transform hourly measurements into daily measurements","text":"<p>From the SQL runner, execute the following query, which will sum the measurement columns, each corresponding to an hour, to obtain the daily value and save it as a new dataset:</p> <pre><code>SELECT data, \"codice spira\", \"00:00-01:00\"+\"01:00-02:00\"+\"02:00-03:00\"+\"03:00-04:00\"+\"04:00-05:00\"+\"05:00-06:00\"+\"06:00-07:00\"+\"07:00-08:00\"+\"08:00-09:00\"+\"09:00-10:00\"+\"10:00-11:00\"+\"11:00-12:00\"\n+\"12:00-13:00\"+\"13:00-14:00\"+\"14:00-15:00\"+\"15:00-16:00\"+\"16:00-17:00\"+\"17:00-18:00\"+\"18:00-19:00\"+\"19:00-20:00\"+\"20:00-21:00\"+\"21:00-22:00\"+\"22:00-23:00\"+\"23:00-24:00\" AS totale_giornaliero\nFROM (\n  SELECT * FROM \"demo_etl\".misurazioni\n) nested_0;\n</code></pre> <p>Select Save View as... again (do not overwrite the previous one), name the new dataset <code>misurazioni_giornaliere</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#connect-grafana-to-dremio","title":"Connect Grafana to Dremio","text":"<p>Access Grafana from your Coder instance or create a new Grafana workspace. Open the left menu and navigate to Connections &gt; Data Sources. Add a new <code>Dremio</code> data source configured as follows:</p> <ul> <li>Name: <code>Dremio</code></li> <li>URL: the Internal Endpoint you see on Coder for your Dremio workspace</li> <li>User: <code>admin</code></li> <li>Password: <code>&lt;dremio_password_set_on_coder&gt;</code></li> </ul> <p>Now you can create a dashboard to visualize Dremio data. An example dashboard is available as a JSON file at the <code>user/examples/dremio_grafana</code> path within the repository of this documentation.</p> <p>Navigate to Dashboards from the left menu, expand the New button on the top right and select Import. Once imported, you will need to update the <code>datasource.uid</code> field, which holds a reference to the Dremio data source in your Grafana instance, throughout the JSON model.</p> <p>To obtain your ID easily, navigate to Connections &gt; Data Sources, select the Dremio source, and copy the ID from the page's URL:</p> <pre><code>https://&lt;grafana_host&gt;/connections/datasources/edit/&lt;YOUR_DATASOURCE_ID&gt;\n</code></pre> <p>Then, go back to Dashboards, open your dashboard, open the Dashboard settings (cog icon in the top toolbar) and select JSON Model from the left. There will be a number of instances where you have to replace the ID, referenced by <code>datasource.uid</code>. When done, click Save changes and return to your dashboard.</p> <p>The dashboard includes three panels: a map of the traffic sensors, a table with the daily number of vehicles registered by each sensor and a graph of the vehicles registered monthly.</p> <p></p> <p>We can now use the dashboard to explore the data. We can either interact with the map to get the information related to each sensor, or use the dashboard filters to select different time ranges and analyze traffic evolution over time.</p>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Define a function for downloading data as-is and persisting it in the data-lake:</p> <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef downloader(url):\n    return url.as_df(file_format='csv',sep=\";\")\n</code></pre> <p>Register the function in Core:</p> <pre><code>func = project.new_function(name=\"download-data\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\",\n                            code_src=\"src/download-data.py\",\n                            handler=\"downloader\")\n</code></pre> <p>This code creates a new function definition that uses Python runtime (versione 3.10) pointing to the created file and the handler method that should be called.</p> <p>For the function to be executed, we need to pass it a reference to the data item. Let us create and register the corresponding data item:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?limit=50000&amp;lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi = project.new_dataitem(name=\"url_data_item\",\n                          kind=\"table\",\n                          path=URL)\n</code></pre> <p>It is also possible to see the data item directly in the Core UI.</p> <p>Then, execute the function (locally) as a single job. Note that it may take a few minutes.</p> <pre><code>run = func.run(\"job\",\n               inputs={'url': di.key},\n               wait=True)\n</code></pre> <p>Note that the <code>inputs</code> map should contain the references to the project entities (e.g., artifacts, dataitems, etc), while in order to pass literal values to the function execution it is necessary to use <code>parameters</code> map.</p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. The name of the artifact will be defined according to the mapping specified in <code>@handler</code> annotation. To get the value of the artifact we can refer to it by the output name:</p> <pre><code>dataset_di = project.get_dataitem('dataset')\n</code></pre> <p>Load the data item and then into a data frame:</p> <pre><code>dataset_df = dataset_di.as_df()\n</code></pre> <p>Run <code>dataset_df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API:</p> <pre><code>%%writefile 'src/api.py'\n\ndef init_context(context, dataitem):\n    di = context.project.get_dataitem(dataitem)\n    df = di.as_df()\n    setattr(context, \"df\", df)\n\ndef serve(context, event):\n    df = context.df\n\n    if df is None:\n        return \"\"\n\n    # mock REST api\n    fields = event.fields\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields[\"page\"])\n\n    if \"size\" in fields:\n        pageSize = int(fields[\"size\"])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    return {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n</code></pre> <p>Register the function:</p> <pre><code>api_func = project.new_function(name=\"api\",\n                                kind=\"python\",\n                                python_version=\"PYTHON3_10\",\n                                code_src=\"src/api.py\",\n                                handler=\"serve\",\n                                init_function=\"init_context\")\n</code></pre> <p>Please note that other than defining the handler method, it is possible to define the <code>init_function</code> to define the preparatory steps.</p> <p>Deploy the function (perform <code>serve</code> action):</p> <pre><code>run_serve_model = api_func.run(\"serve\", init_parameters={\"dataitem\": \"dataset-measures\"}, wait=True)\n</code></pre> <p>Wait till the deployment is complete and the necessary pods and services are up and running.</p> <pre><code>run_serve_model.refresh()\n</code></pre> <p>When done, the status of the run contains the <code>service</code> element with the internal service URL to be used.</p> <pre><code>svc_url = f\"http://{run_serve_model.status.service['url']}/?page=5&amp;size=10\"\n</code></pre> <p>Invoke the API and print its results:</p> <pre><code>res = run_serve_model.invoke(url=svc_url).json()\nprint(res)\n</code></pre> <p>You can also use pandas to load the result in a data frame:</p> <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a simple yet realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell. Alternatively, the final notebook for this scenario can be found in the tutorial repository.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries:</p> <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre> <p>Create a project:</p> <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?limit=50000&amp;lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> <p>Download the file and save it locally:</p> <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre> <p>Use pandas to read the file into a dataframe:</p> <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together:</p> <pre><code>%%writefile \"src/pipeline.py\"\n\nfrom digitalhub_runtime_hera.dsl import step\nfrom hera.workflows import DAG, Parameter, Workflow\n\n\ndef pipeline():\n    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"url\")) as w:\n        with DAG(name=\"dag\"):\n            A = step(\n                template={\"action\": \"job\", \"inputs\": {\"url\": \"{{workflow.parameters.url}}\"}},\n                function=\"download-data\",\n                outputs=[\"dataset\"],\n            )\n            B = step(\n                template={\"action\": \"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n                function=\"process-spire\",\n                inputs={\"di\": A.get_parameter(\"dataset\")},\n            )\n            C = step(\n                template={\"action\": \"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n                function=\"process-measures\",\n                inputs={\"di\": A.get_parameter(\"dataset\")},\n                outputs=[\"dataset-measures\"],\n            )\n            A &gt;&gt; [B, C]\n    return w\n</code></pre> <p>Here we use the Hera-based DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method maps to Hera templates and produces an Argo workflow descriptor which performs the remote execution of the corresponding job. Note that the syntax for <code>step</code> is similar to that of function execution.</p> <p>Register the workflow:</p> <pre><code>workflow = project.new_workflow(name=\"pipeline\",\n                                kind=\"hera\",\n                                code_src=\"src/pipeline.py\",\n                                handler=\"pipeline\")\n</code></pre> <p>You MUST build the workflow before running it. This is necessary to compose the Argo descriptor which will be used to execute the workflow:</p> <pre><code>run_build = workflow.run(\"build\", wait=True)\n</code></pre> <p>The Argo descriptor is saved as encoded base64 string into the workflow spec under the build attribute. Once the workflow is built, you can run it, passing the URL key as a parameter:</p> <pre><code>workflow.run(\"pipeline\", parameters={\"url\": di.key}, wait=True)\n</code></pre> <p>It is possible to monitor the execution in the Core console:</p> <p></p> <p>The next section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store:</p> <pre><code>%%writefile \"src/process-spire.py\"\n\nfrom digitalhub_runtime_python import handler\n\nKEYS=['codice spira','longitudine','latitudine',\n      'Livello','tipologia','codice','codice arco',\n      'codice via','Nome via', 'stato','direzione',\n      'angolo','geopoint']\n\n@handler(outputs=[\"dataset-spire\"])\ndef process(project, di):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n    return sdf\n</code></pre> <p>Register the function in Core:</p> <pre><code>process_func = project.new_function(name=\"process-spire\",\n                                    kind=\"python\",\n                                    python_version=\"PYTHON3_10\",\n                                    code_src=\"src/process-spire.py\",\n                                    handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_run = process_func.run(\"job\",\n                               inputs={'di':dataset_di.key},\n                               wait=True)\n</code></pre> <p>The results has been saved as an artifact in the data store:</p> <pre><code>spire_di = project.get_dataitem('dataset-spire')\nspire_df = spire_di.as_df()\n</code></pre> <p>Now you can view the results with <code>spire_df.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot:</p> <pre><code>keys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = dataset_df[columns]\n</code></pre> <p>Derive dataset for recorded traffic within each time slot for each spire:</p> <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function:</p> <pre><code>%%writefile \"src/process-measures.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00',\n        '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00',\n        '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00',\n        '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00',\n        '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00',\n        '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@handler(outputs=[\"dataset-measures\"])\ndef process(project, di):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre> <p>Register it:</p> <pre><code>process_measures_func = project.new_function(name=\"process-measures\",\n                                             kind=\"python\",\n                                             python_version=\"PYTHON3_10\",\n                                             code_src=\"src/process-measures.py\",\n                                             handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_measures_run = process_measures_func.run(\"job\",\n                                                 inputs={'di':dataset_di.key},\n                                                 wait=True)\n</code></pre> <p>Inspect the resulting data artifact:</p> <pre><code>measures_di = project.get_dataitem('dataset-measures')\nmeasures_df = measures_di.as_df()\nmeasures_df.head()\n</code></pre> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/etl/streamlit/","title":"Visualize data with Streamlit","text":"<p>We can take this one step further and visualize our data in a graph using Streamlit, a library to create web apps and visualize data by writing simple scripts. Let's get familiar with it.</p>"},{"location":"scenarios/etl/streamlit/#setup","title":"Setup","text":"<p>From the Jupyter notebook you've been using, write the result of the API call to a file:</p> <pre><code>with open(\"result.json\", \"w\") as file:\n    file.write(res['data'])\n</code></pre> <p>Create the script that Streamlit will run:</p> <pre><code>%%writefile 'streamlit-app.py'\n\nimport pandas as pd\nimport streamlit as st\n\nrdf = pd.read_json(\"result.json\", orient=\"records\")\n\n# Replace colons in column names as they can cause issues with Streamlit\nrdf.columns = rdf.columns.str.replace(\":\", \"\")\n\nst.write(\"\"\"My data\"\"\")\nst.line_chart(rdf, x=\"codice spira\", y=\"value\")\n</code></pre>"},{"location":"scenarios/etl/streamlit/#launch-app","title":"Launch app","text":"<p>In a new code cell, run the following to install Streamlit in the workspace. Note that if you want to run a shell command from Jupyter cell, prepone it with <code>!</code>. If you want to install a package in the workspace, prepone <code>pip install</code> with <code>%</code>.</p> <pre><code>%pip install streamlit\n</code></pre> <p>Similarly, run the following command. This will start hosting the Streamlit web app, so the cell will remain running. The <code>browser.gatherUsageStats</code> flag is set to <code>false</code> because, otherwise, Streamlit will automatically gather usage stats and print a warning about it.</p> <pre><code>!streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Next, go to your Coder instance and access the Jupyter workspace you've been using.</p> <p></p> <p></p> <p>Click on Ports, type <code>8501</code> (Streamlit's default port), then click the button next to it. It will open a tab to the Streamlit app, where you can visualize data!</p> <p></p> <p>The graph we displayed is very simple, but you are welcome to experiment with more Streamlit features. Don't forget to stop the above code cell, to stop the app.</p> <p>Connect to workspace remotely</p> <p>Alternatively to running shell commands from Jupyter and port-forwarding through the Coder interface, you could connect your local shell to the workspace remotely. You do not need to do this if you already used the method above.</p> <p>Login to Coder with the following command. A tab will open in your browser, containing a token you must copy and paste to the shell (it may ask for your credentials, if your browser isn't already logged in).</p> <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> <p>With this, your shell is authenticated to the Coder instance, and the following command will be able to connect your shell to the workspace remotely, while tunneling port 8501:</p> <pre><code>ssh -L 8501:localhost:8501 coder.my-jupyter-workspace\n</code></pre> <p>Install streamlit:</p> <pre><code>pip install streamlit\n</code></pre> <p>Run the app:</p> <pre><code>streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Access <code>localhost:8501</code> on your browser to view the app!</p>"},{"location":"scenarios/etl/streamlit/#as-docker-container","title":"As Docker container","text":"<p>Streamlit apps can be run as Docker containers. For this section, we will run the same application locally as a container, so you will need either Docker or Podman installed on your machine. Instructions refer to Docker, but if you prefer to use Podman, commands are equivalent: simply replace instances of <code>docker</code> with <code>podman</code>.</p> <p>Download the <code>result.json</code> file obtained previously on your machine, as we will need its data for the app. Also download the <code>streamlit-app.py</code> file.</p> <p>Create a file named <code>Dockerfile</code> and paste the following contents in it:</p> <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY streamlit-app.py streamlit-app.py\nCOPY result.json result.json\n\nRUN pip3 install altair pandas streamlit\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit-app.py\", \"--browser.gatherUsageStats=false\"]\n</code></pre> <p>The Dockerfile describes how the image for the container will be built. In short, it installs the required libraries, copies the files you downloaded, then launches the Streamlit script.</p> <p>Make sure the three files are in the same directory, then open a shell in it and run the following, which builds the Docker image:</p> <pre><code>docker build -t streamlit .\n</code></pre> <p>Once it's finished, you can verify the image exists with:</p> <pre><code>docker images\n</code></pre> <p>Now, run a container:</p> <pre><code>docker run -p 8501:8501 --name streamlit-app streamlit\n</code></pre> <p>Port already in use</p> <p>If you run into an error, it's likely that you didn't quit the remote session you opened while following the previous section, meaning port 8501 is already in use.</p> <p>Open your browser and visit <code>localhost:8501</code> to view the data!</p> <p>To stop the container, simply press Ctrl+C, then run the following to remove the container:</p> <pre><code>docker rm -f streamlit-app\n</code></pre>"},{"location":"scenarios/etl-core/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute the ETL step:</p> <p>Create a source directory for the workflow code:</p> <pre><code>from pathlib import Path\n\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>And create the workflow code file:</p> <pre><code>%%writefile \"src/pipeline.py\"\n\nfrom digitalhub_runtime_hera.dsl import step\nfrom hera.workflows import DAG, Parameter, Workflow\n\n\ndef pipeline():\n    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"employees\")) as w:\n        with DAG(name=\"dag\"):\n            A = step(\n                template={\n                    \"action\": \"transform\",\n                    \"inputs\": {\"employees\": \"{{workflow.parameters.employees}}\"},\n                    \"outputs\": {\"output_table\": \"department-50\"},\n                },\n                function=\"transform-employees\",\n            )\n    return w\n</code></pre> <p>Here we use the Hera-based DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method maps to Hera templates and produces an Argo workflow descriptor which performs the remote execution of the corresponding job. Note that the syntax for <code>step</code> is similar to that of function execution.</p> <p>Register the workflow:</p> <pre><code>workflow = project.new_workflow(name=\"pipeline_dbt\",\n                                kind=\"hera\",\n                                code_src=\"src/pipeline.py\",\n                                handler=\"pipeline\")\n</code></pre> <p>You MUST build the workflow before running it. This is necessary to compose the Argo descriptor which will be used to execute the workflow:</p> <pre><code>run_build = workflow.run(\"build\", wait=True)\n</code></pre> <p>The Argo descriptor is saved as encoded base64 string into the workflow spec under the build attribute. Once the workflow is built, you can run it, passing the URL key as a parameter:</p> <pre><code>workflow.run(\"pipeline\", parameters={\"employees\": di.key}, wait=True)\n</code></pre> <p>It is possible to monitor the execution in the Core console.</p>"},{"location":"scenarios/etl-core/scenario/","title":"ETL with digitalhub-core and DBT scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding organizations, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. Alternatively, the final notebook for this scenario can be found in the tutorial repository.</p>"},{"location":"scenarios/etl-core/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-dbt-ci\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl-core/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The DBT runtime will use the dataitem specifications to fetch the data and perform the <code>transform</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees-dbt\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/etl-core/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a tranformation on data with DBT. Our function will be an SQL query that selects all the employees of department 60.</p> <pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '50'\n\"\"\"\n</code></pre> <p>We create the function from the project object:</p> <pre><code>function = project.new_function(name=\"transform-employees\",\n                                kind=\"dbt\",\n                                code=sql)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>dbt</code>.</li> <li><code>code</code> contains the code that is the SQL we'll execute in the function.</li> </ul>"},{"location":"scenarios/etl-core/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>transform</code>)</li> <li>the inputs map the refereced table in the DBT query (<code>{{ ref('employees') }}</code>) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be <code>department-50</code> and will be the sql query table name result and the output dataitem name.</li> </ul> <pre><code>run = function.run(\"transform\",\n                   inputs={\"employees\": di.key},\n                   outputs={\"output_table\": \"department-50\"},\n                   wait=True)\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling <code>run.refresh()</code> will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/etl-core/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. We can fetch the output table and explore it with <code>pandas</code>.</p> <pre><code>df = run.output('department-50').as_df()\ndf.head()\n</code></pre> <p>In the next section, we will see how to convert this example in a workflow.</p>"},{"location":"scenarios/flower/flower/","title":"Federated Learning with Flower","text":"<p>Flower Federated Learning framework allows for running federated learning tasks where different client nodes perform local training and cooperatively create a more robust solution without exchanging the data but only the weight necessary to progress the training process. </p> <p>The platform support this approach natively integrating the Flower framework in the following manner:</p> <ul> <li>support creating a federation, with central Superlink node and a set of client Supernodes distributed potentially outside of the platform in a secure manner (with TLS verification and client authentication)</li> <li>activate the training procedures defined with the server coordination code and client training code managed by the platform. </li> </ul> <p>See more details in the description of the corresponding Flower runtime.</p> <p>This tutorial  demonstrates how to use the Flower FL framework for execution of federated learning tasks. The tutorial is based on official Pandas example of Flower framework.</p>"},{"location":"scenarios/flower/flower/#initalize-the-project","title":"Initalize the project","text":"<p>Create a project where we define and start the federation Superlink node and the training function.</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"test-flower\")\n</code></pre>"},{"location":"scenarios/flower/flower/#create-and-start-server-part-superlink","title":"Create and Start Server part: Superlink","text":"<p>The following defines the Superlink function, build the corresponding image, and activates the server container with the dependencies. Note that the server is started in <code>insecure</code> mode, meaning that no TLS verification is performed. While this is ok for the purpose of this tutorial, should not be used in a production federation.</p> <p>In secure mode the server will be equipped with a custom platform-level TLS certificate that should also be used by client SuperNode nodes.</p> <pre><code>server_function = project.new_function(\n    name=\"my-flower-server\",\n    kind=\"flower-server\",\n    requirements=[\"pandas==2.2.3\", \"flwr-datasets[vision]==0.5.0\"]\n)\n# Build the server\nserver_function.run(action=\"build\", wait=True)\n\n# Deploy the server\nrun = server_function.run(action=\"deploy\", insecure=True)\n</code></pre>"},{"location":"scenarios/flower/flower/#create-and-start-client-part-supernode","title":"Create and Start Client part: Supernode","text":"<p>The following defines the Supernode function, build the corresponding image, and activates 3 client nodes container with the dependencies and the node-specific configuration.  </p> <p>Note that the server is started in <code>insecure</code> mode as no root certificate is provided. To enable secure mode, it is necessary to specify the <code>root_certificates</code> attribute to each run containing the body of the public root certificate.</p> <p>Also the node authentication is not enabled in this scenario. The node authentication allows for controlling which nodes are allowed to communicate with the server. To achieve this, it is necessary</p> <ul> <li>at each node define a public-private key pair and store the values in project secrets.</li> <li>pass the secret names as <code>public_key_secret</code> and <code>private_key_secret</code> parameters to the supernode spec</li> <li>ensure the public key is included in <code>auth_public_keys</code> field of the server Superlink node</li> </ul> <p>Each client is started with the own set of parameters (<code>node_config</code>) and a reference to the <code>superlink</code> pointing to the superlink address (port 9092 by default).</p> <pre><code>server_url = run.refresh().status.service['url'].split(':')[0] + ':9092' \n\nclient_function = project.new_function(\n    name=\"my-flower-client\",\n    kind=\"flower-client\",\n    requirements=[\"pandas==2.2.3\", \"flwr-datasets[vision]==0.5.0\"]\n)\n\nclient_function.run(action=\"build\", wait=True)\n\n# Deploy client 1\nrun = client_function.run(action=\"deploy\", superlink=server_url, node_config={\n        \"partition-id\": 0,\n        \"num-partitions\": 3,\n        \"local-epochs\": 2\n})\n\n# Deploy client 2\nrun = client_function.run(action=\"deploy\", superlink=server_url, node_config={\n        \"partition-id\": 1,\n        \"num-partitions\": 3,\n        \"local-epochs\": 2\n})\n\n# Deploy client 3\nrun = client_function.run(action=\"deploy\", superlink=server_url, node_config={\n        \"partition-id\": 2,\n        \"num-partitions\": 3,\n        \"local-epochs\": 2\n})\n</code></pre>"},{"location":"scenarios/flower/flower/#create-and-start-the-training-execution","title":"Create and Start the training execution","text":"<p>To perform the actual training procedure, we define a new flower app function with the application code for client and server. Specifically, it is necessary to provide either the reference to a complete Git project or, as in this case, the reference to the client and server source code files:</p> <ul> <li><code>client_src</code> provides reference to the  client source code, while <code>client_app</code> defines the reference in the code to the ClientApp instance.</li> <li><code>server_src</code> provides reference to the  server source code, while <code>server_app</code> defines the reference in the code to the ServerApp instance.</li> </ul> <pre><code>app_function = project.new_function(\n    name=\"my-flower-app\",\n    kind=\"flower-app\",\n    client_src=\"src/client.py\",\n    server_src=\"src/server.py\",\n    client_app=\"app\",\n    server_app=\"app\"\n\n)\n</code></pre> <p>The code of the client looks as follows:</p> <pre><code>\"\"\"pandas_example: A Flower / Pandas app.\"\"\"\n\nimport warnings\n\nimport numpy as np\nfrom flwr_datasets import FederatedDataset\nfrom flwr_datasets.partitioner import IidPartitioner\n\nfrom flwr.client import ClientApp\nfrom flwr.common import Context, Message, MetricRecord, RecordDict\n\nfds = None  # Cache FederatedDataset\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n\ndef get_clientapp_dataset(partition_id: int, num_partitions: int):\n    # Only initialize `FederatedDataset` once\n    global fds\n    if fds is None:\n        partitioner = IidPartitioner(num_partitions=num_partitions)\n        fds = FederatedDataset(\n            dataset=\"scikit-learn/iris\",\n            partitioners={\"train\": partitioner},\n        )\n\n    dataset = fds.load_partition(partition_id, \"train\").with_format(\"pandas\")[:]\n    # Use just the specified columns\n    return dataset[[\"SepalLengthCm\", \"SepalWidthCm\"]]\n\n\n# Flower ClientApp\napp = ClientApp()\n\n\n@app.query()\ndef query(msg: Message, context: Context):\n    \"\"\"Construct histogram of local dataset and report to `ServerApp`.\"\"\"\n\n    # Read the node_config to fetch data partition associated to this node\n    partition_id = context.node_config[\"partition-id\"]\n    num_partitions = context.node_config[\"num-partitions\"]\n\n    dataset = get_clientapp_dataset(partition_id, num_partitions)\n\n    metrics = {}\n    # Compute some statistics for each column in the dataframe\n    for feature_name in dataset.columns:\n        # Compute histogram\n        freqs, _ = np.histogram(dataset[feature_name], bins=np.linspace(2.0, 10.0, 10))\n        metrics[feature_name] = freqs.tolist()\n\n        # Compute weighted average\n        metrics[f\"{feature_name}_avg\"] = dataset[feature_name].mean() * len(dataset)\n        metrics[f\"{feature_name}_count\"] = len(dataset)\n\n    reply_content = RecordDict({\"query_results\": MetricRecord(metrics)})\n\n    return Message(reply_content, reply_to=msg)\n</code></pre> <p>The code of the server part looks like follows:</p> <pre><code>\"\"\"pandas_example: A Flower / Pandas app.\"\"\"\n\nimport random\nimport time\nfrom collections.abc import Iterable\nfrom logging import INFO\n\nimport numpy as np\n\nfrom flwr.common import Context, Message, MessageType, RecordDict\nfrom flwr.common.logger import log\nfrom flwr.server import Grid, ServerApp\n\napp = ServerApp()\n\n\n@app.main()\ndef main(grid: Grid, context: Context) -&gt; None:\n    \"\"\"This `ServerApp` construct a histogram from partial-histograms reported by the\n    `ClientApp`s.\"\"\"\n\n    num_rounds = context.run_config[\"num-server-rounds\"]\n    min_nodes = 2\n    fraction_sample = context.run_config[\"fraction-sample\"]\n\n    for server_round in range(num_rounds):\n        log(INFO, \"\")  # Add newline for log readability\n        log(INFO, \"Starting round %s/%s\", server_round + 1, num_rounds)\n\n        # Loop and wait until enough nodes are available.\n        all_node_ids: list[int] = []\n        while len(all_node_ids) &lt; min_nodes:\n            all_node_ids = list(grid.get_node_ids())\n            if len(all_node_ids) &gt;= min_nodes:\n                # Sample nodes\n                num_to_sample = int(len(all_node_ids) * fraction_sample)\n                node_ids = random.sample(all_node_ids, num_to_sample)\n                break\n            log(INFO, \"Waiting for nodes to connect...\")\n            time.sleep(2)\n\n        log(INFO, \"Sampled %s nodes (out of %s)\", len(node_ids), len(all_node_ids))\n\n        # Create messages\n        recorddict = RecordDict()\n        messages = []\n        for node_id in node_ids:  # one message for each node\n            message = Message(\n                content=recorddict,\n                message_type=MessageType.QUERY,  # target `query` method in ClientApp\n                dst_node_id=node_id,\n                group_id=str(server_round),\n            )\n            messages.append(message)\n\n        # Send messages and wait for all results\n        replies = grid.send_and_receive(messages)\n        log(INFO, \"Received %s/%s results\", len(replies), len(messages))\n\n        # Aggregate partial histograms\n        aggregated_hist = aggregate_partial_histograms(replies)\n\n        # Display aggregated histogram\n        log(INFO, \"Aggregated histogram: %s\", aggregated_hist)\n\n\ndef aggregate_partial_histograms(messages: Iterable[Message]):\n    \"\"\"Aggregate partial histograms.\"\"\"\n\n    aggregated_hist = {}\n    total_count = 0\n    for rep in messages:\n        if rep.has_error():\n            continue\n        query_results = rep.content[\"query_results\"]\n        # Sum metrics\n        for k, v in query_results.items():\n            if k in [\"SepalLengthCm\", \"SepalWidthCm\"]:\n                if k in aggregated_hist:\n                    aggregated_hist[k] += np.array(v)\n                else:\n                    aggregated_hist[k] = np.array(v)\n            if \"_count\" in k:\n                total_count += v\n\n    # Verify aggregated histogram adds up to total reported count\n    assert total_count == sum([sum(v) for v in aggregated_hist.values()])\n    return aggregated_hist\n</code></pre>"},{"location":"scenarios/flower/flower/#running-the-training-function","title":"Running the training function","text":"<p>The execution of the application run is a single job that bundles the application and interacts with the superlink to deploy, activate, and coordinate the federated learning execution. </p> <p>Each execution requires the reference to the corresponding execution API of the Superlink (port 9093) and optionally a set of hyperparameters.</p> <p>Note that the server is started in <code>insecure</code> mode as no root certificate is provided. To enable secure mode, it is necessary to specify the <code>root_certificates</code> attribute to each run containing the body of the public root certificate.</p> <pre><code>server_url = server_url.split(':')[0] + ':9093' \n\napp_run = app_function.run(\"train\", superlink=server_url, parameters={\n    \"num-server-rounds\":3,\n    \"fraction-sample\": 1.0\n})\n</code></pre> <p>The status and log of the execution may be obtained from the corresponding app run, e.g., through the Core Web UI. While not included in this example, the server procedure may create a model that can be logged using the platform SDK and used later for further operations (e.g., inference).</p>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function: we should define the inference operation and the initialization operation where the model is loaded.</p> <p>Create a model serving function and provide the model:</p> <pre><code>%%writefile \"src/serve_darts_model.py\"\nimport json\nfrom zipfile import ZipFile\n\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nfrom darts.metrics import mae, mape, smape\nfrom darts.models import NBEATSModel\nfrom digitalhub_runtime_python import handler\n\ndef init_context(context, model_key):\n    \"\"\"\n    Initialize serving context by loading the trained model\n    \"\"\"\n    model = context.project.get_model(model_key)\n    path = model.download()\n    local_path_model = \"extracted_model/\"\n\n    # Extract model from zip file\n    with ZipFile(path, \"r\") as zip_ref:\n        zip_ref.extractall(local_path_model)\n\n    # Load the NBEATS model\n    input_chunk_length = 24\n    output_chunk_length = 12\n    name_model_local = local_path_model + \"predictor_model.pt\"\n    mm = NBEATSModel(input_chunk_length, output_chunk_length).load(name_model_local)\n\n    setattr(context, \"model\", mm)\n\n\ndef serve_predictions(context, event):\n    \"\"\"\n    Serve time series predictions via REST API\n    \"\"\"\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n\n    context.logger.info(f\"Received event: {body}\")\n    inference_input = body[\"inference_input\"]\n\n    # Convert input to Darts TimeSeries format\n    pdf = pd.DataFrame(inference_input)\n    pdf[\"date\"] = pd.to_datetime(pdf[\"date\"], unit=\"ms\")\n\n    ts = TimeSeries.from_dataframe(pdf, time_col=\"date\", value_cols=\"value\")\n\n    # Make predictions\n    output_chunk_length = 12\n    result = context.model.predict(n=output_chunk_length * 2, series=ts)\n\n    # Convert result to JSON format\n    jsonstr = result.pd_dataframe().reset_index().to_json(orient=\"records\")\n    return json.loads(jsonstr)\n</code></pre> <pre><code>model = train_run.output(\"model\")\n</code></pre> <p>Register it:</p> <pre><code>func = project.new_function(name=\"serve-darts-model\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\",\n                            code_src=\"src/serve_darts_model.py\",\n                            handler=\"serve_predictions\",\n                            init_function=\"init_context\")\n</code></pre> <p>Given the dependencies, it is better to have the image ready, using <code>build</code> action of the function:</p> <pre><code>run_build_model_serve = func.run(\"build\",\n                                 instructions=[\"pip3 install torch'&lt;2.6.0' darts==0.30.0 patsy\"],\n                                 wait=True)\n</code></pre> <p>Now we can deploy the function:</p> <pre><code>serve_run = func.run(\"serve\", init_parameters={\"model_key\": model.key}, labels=[\"time-series-service\"], wait=True)\n</code></pre> <p>Create a test input:</p> <pre><code># Load test data\nseries = AirPassengersDataset().load()\nval = series[-24:]  # Last 24 points for prediction\njson_value = json.loads(val.to_json())\n\n# Prepare input data in the expected format\ndata = map(\n    lambda x, y: {\"value\": x[0], \"date\": datetime.timestamp(datetime.strptime(y, \"%Y-%m-%dT%H:%M:%S.%f\")) * 1000},\n    json_value[\"data\"],\n    json_value[\"index\"],\n)\ninputs = {\"inference_input\": list(data)}\n</code></pre> <p>And finally test the endpoint:</p> <pre><code>serve_run.invoke(json=inputs).json()\n</code></pre>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"Custom ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying generic machine learning applications using the functionalities of the platform. For this purpose, we use ML algorithms for the time series management provided by the Darts framework.</p> <p>We will train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/ml/intro/#set-up","title":"Set-up","text":"<p>First install locally the dependencies:</p> <pre><code># Install darts locally for testing (if not already installed)\n%pip install darts==0.30.0 torch'&lt;2.6.0' --quiet\n</code></pre> <p>then import necessary libraries and create a project to host the functions and executions.</p> <pre><code>import json\nfrom datetime import datetime\nfrom darts.datasets import AirPassengersDataset\nimport digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-cml-darts-ci\")\n</code></pre>"},{"location":"scenarios/ml/intro/#create-dir-for-the-code","title":"Create dir for the code","text":"<p>Create a directory for the code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre>"},{"location":"scenarios/ml/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function. For the sake of simplicity, we use predefined \"Air Passengers\" dataset of Darts.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport json\nfrom zipfile import ZipFile\n\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nfrom darts.metrics import mae, mape, smape\nfrom darts.models import NBEATSModel\nfrom digitalhub_runtime_python import handler\n\n\n@handler(outputs=[\"model\"])\ndef train_model(project):\n    \"\"\"\n    Train a NBEATS model on the Air Passengers dataset\n    \"\"\"\n    # Load Air Passengers dataset\n    series = AirPassengersDataset().load()\n    train, test = series[:-36], series[-36:]\n\n    # Configure and train NBEATS model\n    model = NBEATSModel(input_chunk_length=24, output_chunk_length=12, n_epochs=200, random_state=0)\n    model.fit(train)\n\n    # Make predictions for evaluation\n    pred = model.predict(n=36)\n\n    # Save model artifacts\n    model.save(\"predictor_model.pt\")\n    with ZipFile(\"predictor_model.pt.zip\", \"w\") as z:\n        z.write(\"predictor_model.pt\")\n        z.write(\"predictor_model.pt.ckpt\")\n\n    # Calculate metrics\n    metrics = {\"mape\": mape(test, pred), \"smape\": smape(test, pred), \"mae\": mae(test, pred)}\n\n    # Register model in DigitalHub\n    model_artifact = project.log_model(\n        name=\"air-passengers-forecaster\",\n        kind=\"model\",\n        source=\"predictor_model.pt.zip\",\n        algorithm=\"darts.models.NBEATSModel\",\n        framework=\"darts\",\n    )\n    model_artifact.log_metrics(metrics)\n    return model_artifact\n</code></pre> <p>In this code we create a NBEATS DL model, store it locally zipping the content, extract some metrics, and log the model to the platform with a generic <code>model</code> kind.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-time-series-model\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n)\n</code></pre> <p>and run it with build instruction:</p> <pre><code>train_build = train_fn.run(\"build\",\n                           instructions=[\"pip3 install torch'&lt;2.6.0' darts==0.30.0 patsy\"],\n                           wait=True)\n</code></pre> <p>Once the build is completed, launch the training.</p> <pre><code>train_run = train_fn.run(\"job\", wait=True)\n</code></pre> <p>As a result of train execution, a new model is registered in the Core and may be used by different inference operations.</p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlllm/llm/","title":"Managing LLM Models","text":"<p>With the platform it is possible to create and serve LLM HuggingFace-compatible-models. Specifically, it is possible to serve directly the LLM models from the HuggingFace catalogue provided the id of the model or to serve the fine-tuned model from the specified path, such as S3.</p> <p>LLM implementation relies on the KServe LLM runtime and therefore supports one of the corresponding LLM tasks:</p> <ul> <li>Text Generation</li> <li>Text2Text Generation</li> <li>Fill Mask</li> <li>Text (Sequence) Classification</li> <li>Token Classification</li> </ul> <p>Based on the type of the task the API of the exposed service may differ. Generative models (text generation and text2text generation) use OpenAI's Completion and Chat Completion API.</p> <p>The other types of tasks like token classification, sequence classification, fill mask are served using KServe's Open Inference Protocol v2 API.</p> <p>You can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-classification-models","title":"Exposing Predefined Text Classification Models","text":"<p>In case of predefined HuggingFace non-generative model it is possible to use <code>huggingfaceserve</code> runtime to expose the corresponding inference  API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english</code>.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-classification\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=\"huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", wait=True)\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>As in other scenarios, you need to wait a bit for the service to become available. Once the service becomes available, it is possible to make the calls:</p> <pre><code>json = {\n    \"inputs\": [\n        {\n            \"name\": \"input-0\",\n            \"shape\": [2],\n            \"datatype\": \"BYTES\",\n            \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"],\n        },\n    ]\n}\n\nllm_run.invoke(model_name=\"mymodel\", json=json).json()\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    \"model_name\": \"mymodel\",\n    \"model_version\": None,\n    \"id\": \"cab30aa5-c10f-4233-94e2-14e4bc8fbf6f\",\n    \"parameters\": None,\n    \"outputs\": [\n        {\n            \"name\": \"output-0\",\n            \"shape\": [2],\n            \"datatype\": \"INT64\",\n            \"parameters\": None,\n            \"data\": [1, 0],\n        },\n    ],\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-generation-models","title":"Exposing Predefined Text Generation Models","text":"<p>In case of predefined HuggingFace generative model it is possible to use <code>huggingfaceserve</code> runtime to expose the OpenAI compatible API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://meta-llama/meta-llama-3-8b-instruct</code>.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import the necessary libraries and create a project to host the functions and runs:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-generation\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=\"huggingface://meta-llama/meta-llama-3-8b-instruct\")\n</code></pre> <p>Next, we serve the model. This particular one is protected, so you need to provide a HuggingFace token with access to it. As the model is large, we use a profile with more resources.</p> <pre><code>hf_token = \"&lt;HUGGINGFACE TOKEN&gt;\"\nllm_run = llm_function.run(action=\"serve\",\n                           profile=\"1xa100-80GB\",\n                           envs = [{\"name\": \"HF_TOKEN\", \"value\": hf_token}],\n                           wait=True)\n</code></pre> <p>Deployment time</p> <p>Mind that when requesting a GPU node for the service, it may take some time for the service to start, in some cases up to 10 minutes.</p> <p>As in case of classification models, the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls. For example, for the completion requests:</p> <pre><code>service_url = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nurl = f\"http://{service_url}/openai/v1/completions\"\n\njson = {\n    \"model\": \"mymodel\",\n    \"prompt\": \"Hello! How are you?\",\n    \"stream\": False,\n    \"max_tokens\": 30\n}\n\nllm_run.invoke(url=url, json=json).json()\n</code></pre> <p>Here the expected output should have the following form:</p> <pre><code>{\n   \"id\":\"cmpl-69dd8b1ea70c477fbf80c353ac73b52e\",\n   \"choices\":[\n      {\n         \"finish_reason\":\"length\",\n         \"index\":0,\n         \"logprobs\":\"None\",\n         \"text\":\" Hope you're having a great day!\\n\\nHere I'd like to share some news about my new podcast, where I'll be exploring the world of...\"\n      }\n   ],\n   \"created\":1761210462,\n   \"model\":\"mymodel\",\n   \"system_fingerprint\":\"None\",\n   \"object\":\"text_completion\",\n   \"usage\":{\n      \"completion_tokens\":30,\n      \"prompt_tokens\":7,\n      \"total_tokens\":37\n   }\n}\n</code></pre> <p>In case of chat requests:</p> <pre><code>service_url = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nurl = f'http://{service_url}/openai/v1/chat/completions'\n\njson = {\n    \"model\": \"mymodel\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are an assistant that speaks like Shakespeare.\"},\n        {\"role\": \"user\", \"content\": \"Write a poem about colors\"}\n    ],\n    \"max_tokens\": 30,\n    \"stream\": False\n}\n\nllm_run.invoke(url=url, json=json).json()\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"id\": \"cmpl-9aad539128294069bf1e406a5cba03d3\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"  O, fair and vibrant colors, how ye doth delight\\nIn the world around us, with thy hues so bright!\\n\",\n        \"tool_calls\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null\n      },\n      \"logprobs\": null\n    }\n  ],\n  \"created\": 1718638005,\n  \"model\": \"mymodel\",\n  \"system_fingerprint\": null,\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 30,\n    \"prompt_tokens\": 37,\n    \"total_tokens\": 67\n  }\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llm/#fine-tuned-llm-model","title":"Fine-tuned LLM model","text":"<p>When it comes to custom LLM model, it is possible to create a HuggingFace-based fine-tuned model, log it and serve it from the model path.</p> <p>When using the SDK, this may be accomplished as follows.</p> <p>First, import the necessary libraries and create a project to host the functions and runs:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create a directory for the code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Create the training procedure that logs the model to the platform:</p> <pre><code>%%writefile \"src/train_model.py\"\n\nimport os\n\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom digitalhub_runtime_python import handler\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\n@handler()\ndef train(project):\n    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n    metric = evaluate.load(\"accuracy\")\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    dataset = load_dataset(\"yelp_review_full\")\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\")\n\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\n    save_model = \"model\"\n    if not os.path.exists(save_model):\n        os.makedirs(save_model)\n\n    save_dir = \"model\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    trainer.save_model(save_dir)\n    tokenizer.save_pretrained(save_dir)\n\n    project.log_model(\n        name=\"test_llm_model\",\n        kind=\"huggingface\",\n        base_model=\"google-bert/bert-base-cased\",\n        source=save_dir,\n    )\n</code></pre> <p>Register the function:</p> <pre><code>train_func = project.new_function(name=\"train_model\",\n                                  kind=\"python\",\n                                  python_version=\"PYTHON3_10\",\n                                  code_src=\"src/train_model.py\",\n                                  handler=\"train\",\n                                  requirements=[\"evaluate\", \"transformers[torch]\", \"torch\", \"torchvision\", \"accelerate\"])\n</code></pre> <p>Run it: <pre><code>train_run = train_func.run(action=\"job\", profile=\"1xa100\", wait=True)\n</code></pre></p> <p>A new model should have been created in the project. We need its path: <pre><code>llm_model_path = project.get_model(\"test_llm_model\").spec.path\n</code></pre></p> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-classification\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=llm_model_path)\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", volumes=[{\n                        \"volume_type\": \"persistent_volume_claim\",\n                        \"name\": \"volume-llmpa\",\n                        \"mount_path\": \"/shared\",\n                        \"spec\": { \"size\": \"10Gi\" }}]\n                    )\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment. Also, in case of large models the default disk space may be insufficient and an extra volume should be configured for the underlying deployment.</p> <p>Once the service becomes available, it is possible to make the calls:</p> <pre><code>json = {\n    \"inputs\": [\n        {\n            \"name\": \"input-0\",\n            \"shape\": [2],\n            \"datatype\": \"BYTES\",\n            \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"],\n        }\n    ]\n}\n\nllm_run.refresh().invoke(model_name=\"mymodel\", json=json).json()\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    \"model_name\": \"mymodel\",\n    \"model_version\": None,\n    \"id\": \"cab30aa5-c10f-4233-94e2-14e4bc8fbf6f\",\n    \"parameters\": None,\n    \"outputs\": [\n        {\n            \"name\": \"output-0\",\n            \"shape\": [2],\n            \"datatype\": \"INT64\",\n            \"parameters\": None,\n            \"data\": [4, 0],\n        }\n    ],\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llmkubeai/","title":"Managing LLM Models with KubeAI Runtime","text":"<p>To support the set of LLM scenarios within the platform it is possible to use the KubeAI runtime when the KubeAI operator is enabled. </p> <p>For what concerns LLM tasks, currently KubeAI runtime allows for deploying the models for the following tasks:</p> <ul> <li><code>TextGeneration</code>: text generation tasks with  the OpenAI-compatible API</li> <li><code>TextEmbedding</code>: creating embeddings from the text following the OpenAI-compatible API</li> </ul> <p>To accomplish this, it is possible to use one of the KubeAI-supported runtimes, namely vLLM, OLlama, and Infinity. in case of vLLM also adapters are supported.</p> <p>For details about the specification, see the corresponding section of Modelserve reference.</p>"},{"location":"scenarios/mlllm/llmkubeai/#exposing-text-generation-models","title":"Exposing Text Generation Models","text":"<p>To expose the text generation model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>inference engine (one of <code>VLLM</code>, <code>OLlama</code>, <code>Infinity</code>) to use</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix), from S3 storage of the platform (<code>s3://</code>), or OLlama compatible model (<code>ollama://</code> prefix in case of OLlama engine).</li> <li>feature should be set to <code>TextGeneration</code>.</li> <li>in case of vLLM engine it is also possible to add list of adapters for the main model. Each adapter is specified with its own name and URL of the corresponding adapter.</li> </ul> <p>To serve the text generation model, the function should be run with the <code>serve</code> action, specifying additional parameters. In particular, it may be necessary to specify the HW profile to use with number of processors (since GPU may be required) or resource specification, and further parameters and arguments accepted by the KubeAI model specification:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values. For example, in case HuggingFace token is needed, create <code>HF_TOKEN</code> secret at the project with the token value to use. </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model with an adapter from HuggingFace, the following procedure may be used:</p> <pre><code>llm_function = project.new_function(\"llm\",\n                                    kind=\"kubeai-text\",\n                                    model_name=\"tinyllama-chat\",\n                                    url=\"hf://TinyLlama/TinyLlama-1.1B-Chat-v0.3\",\n                                    engine=\"VLLM\",\n                                    features=[\"TextGeneration\"],\n                                    adapters=[{\"name\": \"colorist\", \"url\": \"hf://jashing/tinyllama-colorist-lora\"}])\n\n\nllm_run = llm_function.run(action=\"serve\",\n                           profile=\"1xa100\",\n                           args=[\"--enable-prefix-caching\", \"--max-model-len=8192\"])                                    \n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform. The run status (see <code>openai</code> and <code>service</code> section) contains the information about the name of the model and the endpoints of the KubeAI API exposed </p> <pre><code>import requests \n\nmodel_name = f\"tinyllama-chat-123xyz_colorist\"\n\ninput = {\"prompt\": \"Hi\", \"model\": model_name}\n\nres = requests.post(\"http://kubeai:80/openai/v1/completions\", json=input)\nprint(res.json())\n</code></pre> <p>Model name</p> <p>Please note how the model name is defined: it is composed of the name of the model as specified in the function and the random value. In case of adapter the name of adapter should be added: <code>&lt;model_name&gt;-&lt;random&gt;_&lt;adapter-name&gt;</code>.</p> <p>It is also possible to use OpenAI client for interacting with the model. </p>"},{"location":"scenarios/mlllm/llmkubeai/#exposing-text-embedding-models","title":"Exposing Text Embedding Models","text":"<p>To expose the text embedding model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>inference engine (one of <code>VLLM</code>, or <code>Infinity</code>) to use</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix), from S3 storage of the platform (<code>s3://</code>).</li> <li>feature should be set to <code>TextEmbedding</code>.</li> </ul> <p>To serve the text embedding model, the function should be run with the <code>serve</code> action, specifying additional parameters.  Normally embedding models do not require extra resources. However,  further parameters and arguments accepted by the KubeAI model specification may be added:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model from HuggingFace, the following procedure may be used:</p> <pre><code>llm_function = project.new_function(\"llm\",\n                                    kind=\"kubeai-text\",\n                                    model_name=\"embedding\",\n                                    url=\"hf://BAAI/bge-small-en-v1.5\",\n                                    engine=\"Infinity\",\n                                    features=[\"TextEmbedding\"])\n\n\nllm_run = llm_function.run(action=\"serve\")                                    \n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform or OpenAI client:</p> <pre><code>from openai import OpenAI\n\nmodel_name = f\"embedding-123qwe\"\n\nclient = OpenAI(api_key=\"ignored\", base_url=\"http://kubeai:80/openai/v1\")\nresponse = client.embeddings.create(\n    input=\"Your text goes here.\",\n    model=model_name\n)\n</code></pre>"},{"location":"scenarios/mlmlflow/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a MLFLow model is easy: <code>mlflowserve</code> runtime supports this functionality out of the box. It is sufficient to specify the path to the model artifact and optionally the name of the model to expose.</p> <p>It is important to note that the path is model key. is created from MLFlow run artifact path, besides the <code>model</code> folder it may contain additional artifacts.</p> <p>Register it and deploy:</p> <pre><code>serve_func = project.new_function(\n    name=\"serve-mlflow-model\",\n    kind=\"mlflowserve\",\n    model_name=model.name,\n    path=model.key,\n)\n\nserve_run = serve_func.run(\"serve\", wait=True)\n</code></pre> <p>You can now create a dataset for testing the endpoint:</p> <pre><code>from sklearn import datasets\n\niris = datasets.load_iris()\ndata = iris.data[0:2].tolist()\njson={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [-1, 4],\n        \"datatype\": \"FP64\",\n        \"data\": data\n        }\n    ]\n}\n</code></pre> <p>Finally, you can test the endpoint. When the model is ready to be used, invoke the endpoint:</p> <pre><code>serve_run.invoke(model_name=model.name, json=json).json()\n</code></pre> <p>If it does not work, wait for sometimes, because it takes a while to load the model.</p> <p>Please note that the MLFLow model serving exposes also the Open API specification under <code>/v2/docs</code> path.</p>"},{"location":"scenarios/mlmlflow/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlmlflow/intro/","title":"MLFLow ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a machine learning application based on model tracked with MLFlow framework using the functionalities of the platform.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlmlflow/intro/#set-up","title":"Set-up","text":"<p>Create folder for source code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Then, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-mlflow-model-ci\")\n</code></pre>"},{"location":"scenarios/mlmlflow/intro/#generate-data","title":"Generate data","text":"<p>For the sake of simplicity, we use the predefined IRIS dataset.</p>"},{"location":"scenarios/mlmlflow/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport mlflow\nfrom digitalhub import from_mlflow_run, get_mlflow_model_metrics\nfrom digitalhub_runtime_python import handler\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n\n@handler(outputs=[\"model\"])\ndef train_model(project):\n    \"\"\"\n    Train an SVM classifier on the Iris dataset with hyperparameter tuning using MLflow\n    \"\"\"\n    # Enable MLflow autologging for sklearn\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    # Load Iris dataset\n    iris = datasets.load_iris()\n\n    # Define hyperparameter search space\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    # Train model with grid search\n    clf.fit(iris.data, iris.target)\n\n    # Get MLflow run information\n    run_id = mlflow.last_active_run().info.run_id\n\n    # Extract MLflow run artifacts and metadata for DigitalHub integration\n    model_params = from_mlflow_run(run_id)\n    metrics = get_mlflow_model_metrics(run_id)\n\n    # Register model in DigitalHub with MLflow metadata\n    model = project.log_model(name=\"iris-classifier\", kind=\"mlflow\", **model_params)\n    model.log_metrics(metrics)\n    return model\n</code></pre> <p>The function creates an SVC model with the scikit-learn framework. Note that here we use the autologging functionality of MLFlow and then construct the necessary model metadata out of the tracked MLFlow model. Specifically, MLFlow creates a series of artifacts that describe the model and the corresponding model files, as well as additional files representing the model properties and metrics.</p> <p>We then log the model of <code>mlflow</code> kind using the extract metadata as kwargs.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-mlflow-model\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n    requirements=[\"numpy&lt;2\", \"mlflow&lt;3\", \"scikit-learn &lt;= 1.6.1\"],\n)\n</code></pre> <p>and run it locally:</p> <pre><code>train_model_run = train_fn.run(action=\"job\", wait=True)\n</code></pre> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = train_model_run.output(\"model\")\nmodel.spec.path\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlsklearn/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a scikit-learn model is easy: <code>sklearnserve</code> runtime supports this functionality out of the box. It is sufficient to specify the key of the model and optionally the name of the model to expose.</p> <p>Register it and deploy:</p> <pre><code>serve_func = project.new_function(\n    name=\"serve-classifier\",\n    kind=\"sklearnserve\",\n    path=model.key,\n)\nserve_run = serve_func.run(\"serve\", wait=True)\n</code></pre> <p>You can now create a dataset for testing the endpoint:</p> <pre><code>import numpy as np\n\n# Generate sample data for prediction\ndata = np.random.rand(2, 30).tolist()\njson_payload = {\"inputs\": [{\"name\": \"input-0\", \"shape\": [2, 30], \"datatype\": \"FP32\", \"data\": data}]}\n</code></pre> <p>Finally, you can test the endpoint. To do so, you need to refresh the serve run. This is needed because the backend monitors the deployment every minute and the model status, where the endpoint is exposed, is updated after a minute. When the model is ready, invoke the endpoint:</p> <pre><code>result = serve_run.refresh().invoke(json=json_payload).json()\nprint(\"Prediction result:\")\nprint(result)\n</code></pre> <p>Please note that the scikit-learn model serving exposes also the Open API specification under <code>/docs</code> path.</p>"},{"location":"scenarios/mlsklearn/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlsklearn/intro/","title":"Scikit-learn ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a scikit-learn machine learning application using the functionalities of the platform.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlsklearn/intro/#set-up","title":"Set-up","text":"<p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-ml-ci\")\n</code></pre> <p>Create folder for source code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre>"},{"location":"scenarios/mlsklearn/intro/#generate-data","title":"Generate data","text":"<p>Define the following function, which generates the dataset as required by the model:</p> <pre><code>%%writefile \"src/data-prep.py\"\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef data_generator():\n    \"\"\"\n    A function which generates the breast cancer dataset from scikit-learn\n    \"\"\"\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"target\"])\n    breast_cancer_dataset = pd.concat([breast_cancer_dataset, breast_cancer_labels], axis=1)\n    return breast_cancer_dataset\n</code></pre> <p>Register it:</p> <pre><code>data_gen_fn = project.new_function(name=\"data-prep\",\n                                   kind=\"python\",\n                                   python_version=\"PYTHON3_10\",\n                                   code_src=\"src/data-prep.py\",\n                                   handler=\"data_generator\")\n</code></pre> <p>Run it:</p> <pre><code>gen_data_run = data_gen_fn.run(\"job\",wait=True)\n</code></pre> <p>You can view the state of the execution with <code>gen_data_run.status</code> or its output with <code>gen_data_run.outputs()</code>. You can see a few records from the output artifact:</p> <pre><code>gen_data_run.output(\"dataset\").as_df().head()\n</code></pre> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/mlsklearn/training/","title":"Training the model","text":"<p>Let us define the training function.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom pickle import dump\nimport sklearn.metrics\nfrom digitalhub_runtime_python import handler\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n@handler(outputs=[\"model\"])\ndef train_model(project, di):\n    \"\"\"\n    Train an SVM classifier on the breast cancer dataset and log metrics\n    \"\"\"\n    df_cancer = di.as_df()\n    X = df_cancer.drop([\"target\"], axis=1)\n    y = df_cancer[\"target\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n    svc_model = SVC()\n    svc_model.fit(X_train, y_train)\n    y_predict = svc_model.predict(X_test)\n\n    if not os.path.exists(\"model\"):\n        os.makedirs(\"model\")\n\n    with open(\"model/breast_cancer_classifier.pkl\", \"wb\") as f:\n        dump(svc_model, f, protocol=5)\n\n    metrics = {\n        \"f1_score\": sklearn.metrics.f1_score(y_test, y_predict),\n        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_predict),\n        \"precision\": sklearn.metrics.precision_score(y_test, y_predict),\n        \"recall\": sklearn.metrics.recall_score(y_test, y_predict),\n    }\n    model = project.log_model(name=\"breast_cancer_classifier\", kind=\"sklearn\", source=\"./model/\")\n    model.log_metrics(metrics)\n    return model\n</code></pre> <p>The function takes the analysis dataset as input, creates an SVC model with the scikit-learn framework and logs the model with its metrics.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-classifier\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n    requirements=[\"numpy&lt;2\"],\n)\n</code></pre> <p>and run it:</p> <pre><code>dataset = gen_data_run.output(\"dataset\")\ntrain_run = train_fn.run(action=\"job\", inputs={\"di\": dataset.key}, wait=True)\n</code></pre> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = train_run.output(\"model\")\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlspeech/kubeaispeech/","title":"Managing Speech-to-Text Models with KubeAI Runtime","text":"<p>To support the speech-to-text scenario within the platform it is possible to use the KubeAI runtime when the KubeAI operator is enabled. </p> <p>To accomplish this, it is possible to use the KubeAI-supported runtime, namely FasterWhisper.</p> <p>For details about the specification, see the corresponding section of Modelserve reference.</p>"},{"location":"scenarios/mlspeech/kubeaispeech/#exposing-speech-to-text-models","title":"Exposing Speech-to-Text  Models","text":"<p>To expose the  speech-to-text  model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix) or from S3 storage of the platform (<code>s3://</code>)).</li> </ul> <p>To serve the text speech-to-text model, the function should be run with the <code>serve</code> action, specifying additional parameters. In particular, it may be necessary to specify the HW profile to use with number of processors or resource specification, and further parameters and arguments accepted by the KubeAI model specification:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model from HuggingFace, the following procedure may be used:</p> <pre><code>audio_function = project.new_function(\"audio\",\n                                    kind=\"kubeai-speech\",\n                                    model_name=\"audiomodel\",\n                                    url=\"hf://Systran/faster-whisper-medium.en\")\n\n\nrun = audio_function.run(action=\"serve\")                                    \n</code></pre> <p>Downloading the model:</p> <pre><code>!wget -O kubeai.mp4 https://github.com/user-attachments/assets/711d1279-6af9-4c6c-a052-e59e7730b757\n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform (<code>/openai/v1/transcriptions</code> endpoint):</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://{KUBEAI_ENDPOINT}/openai/v1\", api_key=\"ignore\")\naudio_file= open(\"kubeai.mp4\", \"rb\")\n\ntranscription = client.audio.transcriptions.create(\n    model=f\"audiomodel-123zxc\", \n    file=audio_file\n)\n\nprint(transcription.text)\n</code></pre> <p>By default, the <code>KUBEAI_ENDPOINT</code> is <code>kubeai</code>.</p> <p>Model name</p> <p>Please note how the model name is defined: it is composed of the name of the model as specified in the function and a random value: <code>&lt;model_name&gt;-&lt;run_id&gt;</code>. The name of the generated model as well as the endpoint information can be seen in the run specification (see <code>service</code> section)</p>"},{"location":"scenarios/postgrest/apigw/","title":"Expose service externally","text":"<p>Finally, we make the PostgREST service available externally. Access API Gateways on the left menu and click <code>CREATE</code>.</p> <p>Fill the fields as follows:</p> <ul> <li>Name: name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Service: select the one referring to the PostgREST service you created. Its name may be something like <code>postgrest-my-postgrest</code>. Port will automatically be filled.</li> <li>Host: full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>pgrest.services.example.com</code>.</li> <li>Path: Leave the default <code>/</code>.</li> <li>Authentication: <code>None</code>.</li> </ul> <p></p> <p>Save and the API gateway will be created. You can try a simple query like the following, even in your browser, to view its results (remember to change domain according to your case): <pre><code>https://pgrest.services.example.com/readings?limit=3\n</code></pre></p>"},{"location":"scenarios/postgrest/data/","title":"Insert data into the database","text":"<p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We will now insert some data into the database we created earlier. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file is available in the <code>documentation/examples/postgrest</code> path within the repository of this documentation.</p> <p>Import required libraries:</p> <pre><code>import os\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport requests\n</code></pre> <p>Connect to the database. You will need the value of POSTGRES_URL you got from the owner's secret in the first stage of the scenario.</p> <pre><code>engine = create_engine('postgresql://owner-pANdsG:UEw2jZyd2cBkeFQ@platform-postgres-cluster-rw/mydb')\n</code></pre> <p>Download a CSV file and parse it (may take a few minutes):</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?limit=50000&amp;lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n\nwith requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n\ndf = pd.read_csv(filename, sep=\";\")\n</code></pre> <p>The following will create a table and insert the dataframe into it. If it fails, resources allocated to the Jupyter workspace may be insufficient. The table will be created automatically, or replaced if it already exists. <pre><code>df.to_sql(\"readings\", engine, if_exists=\"replace\")\n</code></pre></p> <p>Run a test select query to check data has been successfully inserted: <pre><code>select = \"SELECT * FROM readings LIMIT 3\"\nselect_df = pd.read_sql(select,con=engine)\nselect_df.head()\n</code></pre></p> <p>If everything went right, a few rows are returned. We will now create a PostgREST service to expose this data via a REST API.</p>"},{"location":"scenarios/postgrest/intro/","title":"PostgREST scenario introduction","text":"<p>In this scenario, we download some data into a Postgres database, then use PostgREST - a tool to make Postgres tables accessible via REST API - to expose this data and run a simple request to view the results.</p>"},{"location":"scenarios/postgrest/intro/#database-set-up","title":"Database set-up","text":"<p>Let's start by setting up the database. Access your KRM instance and Postgres DBs on the left menu, then click Create.</p> <ul> <li><code>Metadata name</code>: This is just an identifier for Kubernetes. Type <code>my-db</code>.</li> <li><code>Database</code>: The actual name on the database. Type <code>mydb</code>.</li> <li>Toggle on <code>Drop on delete</code>, which conveniently deletes the database when you delete the custom resource.</li> </ul> <p></p> <p>Click Save. You should now see your database listed.</p>"},{"location":"scenarios/postgrest/intro/#add-users-to-database","title":"Add users to database","text":"<p>PostgREST needs a user to authenticate and a user to assume its role when the API is called. These can be separate users, but for our purposes, we will create just one to fill both roles. Click Show on your database's entry and then Add user on the bottom. Enter values as follows:</p> <ul> <li><code>Metadata name</code>: An identifier for Kubernetes. Type <code>db-owner</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>owner</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Owner</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>owner-secret</code>.</li> </ul> <p></p>"},{"location":"scenarios/postgrest/intro/#retrieve-postgres_url","title":"Retrieve POSTGRES_URL","text":"<p>Together with the user, a secret has also been created. Go to Secrets on the left menu; the list should contain a secret with a name referring the user you created. Find it and click Show.</p> <p>Write down the following information somewhere, as we will need it later:</p> <ul> <li>Name of the secret</li> <li>Value of <code>POSTGRES_URL</code> (click on Decode to obtain it)</li> <li>Value of <code>ROLE</code></li> </ul> <p></p>"},{"location":"scenarios/postgrest/postgrest/","title":"Create PostgREST service","text":"<p>We will go back to KRM to create a PostgREST service and expose the database's table via API.</p>"},{"location":"scenarios/postgrest/postgrest/#creating-the-postgrest-service","title":"Creating the PostgREST service","text":"<p>Click PostgREST Data Services on the left and then Create.</p> <p>Fill the first few fields as follows:</p> <ul> <li><code>Name</code>: Anything you'd like, for example <code>my-postgrest</code>, as it's once again an identifier for Kubernetes.</li> <li><code>Schema</code>: <code>public</code></li> <li>Toggle on <code>With existing DB user</code>.</li> <li><code>Existing DB user name</code>: Value of ROLE in the owner's secret.</li> </ul> <p>Under Connection, fill the fields as follows:</p> <ul> <li>Toggle on <code>With existing secret</code>.</li> <li><code>Secret name</code>: Name of the owner's secret.</li> </ul> <p>When you hit Save, the PostgREST instance will be launched.</p> <p></p>"},{"location":"scenarios/rag/intro/","title":"Retrieval-Augmented Generation","text":"<p>In this scenario, we create a Retrieval-Augmented Generation (RAG) application, a chatbot able to take new documents, learn from their contents and answer questions related to them.</p> <p>When a LLM model is asked a question concerning information not found in the data it has been trained on, it is unable to provide a correct response and will provide a (perhaps even somewhat convincing) wrong answer or, at best, admit it doesn't know.</p> <p>Rather than re-train the model on a larger dataset, the idea is to provide a document - such as a file or a link - to be analyzed and used as context for the response.</p>"},{"location":"scenarios/rag/intro/#scenario","title":"Scenario","text":"<p>In this scenario, we will face a common situation, where the question we wish to ask is related to the information contained within a PDF file. As the subject is so precise, existing LLM models likely will not be able to give a good answer right away, but by providing such document and letting the application study it, it will become able to answer questions related to the document's contents.</p> <p>The steps will be as follows:</p> <ul> <li>Prepare a LLM model</li> <li>Extract text from the PDF file and generate embeddings</li> <li>Prepare the RAG application</li> <li>Create a UI for the application</li> </ul>"},{"location":"scenarios/rag/intro/#code-for-the-functions","title":"Code for the functions","text":"<p>Three functions will be used during this tutorial. To prepare them in advance, run the following code cells on your Jupyter notebook.</p> <p>Create the directory for the code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Function for text extraction:</p> <pre><code>%%writefile \"src/extract.py\"\nimport requests\n\ndef extract_text(tika_url, artifact, project):\n    print(f\"Downloading artifact {artifact.name}...\")\n    fp = artifact.as_file()\n    if not (tika_url)[:4] == \"http\": \n        tika_url = \"http://\"+tika_url\n    print(f\"Sending {fp} to {tika_url}...\")    \n    response = requests.put(tika_url+\"/tika\",headers={\"Accept\":\"text/html\"}, data=open(fp,'rb').read())\n    if response.status_code == 200:\n        print(\"Extracted text with success\")\n        res = \"/tmp/output.html\"\n        with open(res, \"w\") as tf:\n            tf.write(response.text)\n        project.log_artifact(kind=\"artifact\", name=artifact.name+\"_output.html\", source=res)\n        return res\n    else:\n        print(f\"Received error: {response.status_code}\")\n        raise Exception(\"Error\")\n</code></pre> <p>Function for embedding generation:</p> <pre><code>%%writefile \"src/embedder.py\"\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_postgres import PGVector\nimport os\nimport requests\nfrom langchain_openai import OpenAIEmbeddings\nfrom openai import OpenAI\n\n\nPG_USER = os.environ[\"DB_USERNAME\"]\nPG_PASS = os.environ[\"DB_PASSWORD\"]\nPG_HOST = os.environ[\"DB_HOST\"]\nPG_PORT = os.environ[\"DB_PORT\"]\nDB_NAME = os.environ[\"DB_DATABASE\"]\nACCESS_TOKEN = os.environ[\"DHCORE_ACCESS_TOKEN\"]\n\n\ndef process(input):\n    print(f\"process input {input.id}...\")\n    url = (\n        os.environ[\"DHCORE_ENDPOINT\"]\n        + \"/api/v1/-/\"\n        + input.project\n        + \"/\"\n        + input.kind\n        + \"s/\"\n        + input.id\n        + \"/files/download\"\n    )\n    print(f\"request download link for input {input.id} from {url}\")\n    res = requests.get(url, headers={\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"})\n    print(f\"Download url status {res.status_code}\")\n    if res.status_code == 200:\n        j = res.json()\n        print(f\"{j}\")\n        #if \"url\" in j:\n            #return embed(j[\"url\"])\n\n    print(\"End.\")\n\n\ndef embed(url):\n    PG_CONN_URL = (\n        f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{DB_NAME}\"\n    )\n    print(f\"process url {url}...\")\n    embedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\n    embedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\n\n    class CEmbeddings(OpenAIEmbeddings):\n        def embed_documents(self, docs):\n            client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n            emb_arr = []\n            for doc in docs:\n                #sanitize string: replace NUL with spaces\n                d=doc.replace(\"\\x00\", \"-\")\n                embs = client.embeddings.create(\n                    input=d,\n                    model=embedding_model_name\n                )\n                emb_arr.append(embs.data[0].embedding)\n            return emb_arr\n\n    custom_embeddings = CEmbeddings(api_key=\"ignore\")\n\n    vector_store = PGVector(\n        embeddings=custom_embeddings,\n        collection_name=f\"{embedding_model_name}_docs\",\n        connection=PG_CONN_URL,\n    )\n\n    loader = WebBaseLoader(\n        web_paths=(url,),\n        bs_kwargs=dict(\n            parse_only=bs4.SoupStrainer(\n                class_=(\"post-content\")\n            )\n        ),\n    )    \n    docs = loader.load()\n    print(f\"document loaded, generate embeddings...\")\n\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    all_splits = text_splitter.split_documents(docs)\n    print(f\"store documents in vector db...\")\n\n    vector_store.add_documents(documents=all_splits)\n    print(\"Done.\")\n</code></pre> <p>Function to serve the RAG application:</p> <pre><code>%%writefile \"src/serve.py\"\nimport bs4\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_postgres import PGVector\nimport os\nfrom langchain_openai import OpenAIEmbeddings\nfrom openai import OpenAI\n\nfrom langchain import hub\nfrom langchain_core.documents import Document\nfrom typing_extensions import List, TypedDict\n\nfrom langgraph.graph import START, StateGraph\nfrom langchain.chat_models import init_chat_model\n\nPG_USER = os.environ[\"DB_USERNAME\"]\nPG_PASS = os.environ[\"DB_PASSWORD\"]\nPG_HOST = os.environ[\"DB_HOST\"]\nPG_PORT = os.environ[\"DB_PORT\"]\nDB_NAME = os.environ[\"DB_DATABASE\"]\nACCESS_TOKEN = os.environ[\"DHCORE_ACCESS_TOKEN\"]\n\nclass State(TypedDict):\n    question: str\n    context: List[Document]\n    answer: str\n\n\ndef init(context):\n    chat_model_name = os.environ[\"CHAT_MODEL_NAME\"]\n    chat_service_url = os.environ[\"CHAT_SERVICE_URL\"]\n    embedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\n    embedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\n\n    class CEmbeddings(OpenAIEmbeddings):\n        def embed_documents(self, docs):\n            client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n            emb_arr = []\n            for doc in docs:\n                embs = client.embeddings.create(\n                    input=doc,\n                    model=embedding_model_name\n                )\n                emb_arr.append(embs.data[0].embedding)\n            return emb_arr\n\n    custom_embeddings = CEmbeddings(api_key=\"ignored\")\n    PG_CONN_URL = (\n        f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{DB_NAME}\"\n    )\n    vector_store = PGVector(\n        embeddings=custom_embeddings,\n        collection_name=f\"{embedding_model_name}_docs\",\n        connection=PG_CONN_URL,\n    )\n\n    os.environ[\"OPENAI_API_KEY\"] = \"ignore\"\n\n    llm = init_chat_model(chat_model_name, model_provider=\"openai\", base_url=f\"{chat_service_url}/v1/\")\n    prompt = hub.pull(\"rlm/rag-prompt\")\n\n    def retrieve(state: State):\n        retrieved_docs = vector_store.similarity_search(state[\"question\"])\n        return {\"context\": retrieved_docs}\n\n    def generate(state: State):\n        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n        messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n        response = llm.invoke(messages)\n        return {\"answer\": response.content}\n\n    graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n    graph_builder.add_edge(START, \"retrieve\")\n    graph = graph_builder.compile()\n\n    setattr(context, \"graph\", graph)\n\ndef serve(context, event):\n    graph = context.graph\n    context.logger.info(f\"Received event: {event}\")\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n\n    question = body[\"question\"]\n    response = graph.invoke({\"question\": question})\n    return {\"answer\": response[\"answer\"]}\n</code></pre>"},{"location":"scenarios/rag/knowledge-base/","title":"Building the knowledge base","text":"<p>We now define the process to extract text content from the PDF file and generate embeddings from it.</p>"},{"location":"scenarios/rag/knowledge-base/#text-extraction","title":"Text extraction","text":""},{"location":"scenarios/rag/knowledge-base/#deploy-a-text-extraction-service","title":"Deploy a text extraction service","text":"<p>We will use Apache Tika, a tool for extracting text from a variety of formats. Create the function, run it and obtain the URL of the service:</p> <pre><code>tika_function = project.new_function(\"tika\", kind=\"container\", image=\"apache/tika:latest-full\")\n</code></pre> <pre><code>tika_run = tika_function.run(\"serve\", service_ports = [{\"port\": 9998, \"target_port\": 9998}], wait=True)\n</code></pre> <pre><code>service = tika_run.refresh().status.service\nprint(\"Service status:\", service)\n</code></pre> <pre><code>TIKA_URL = tika_run.status.to_dict()[\"service\"][\"url\"]\nprint(TIKA_URL)\n</code></pre>"},{"location":"scenarios/rag/knowledge-base/#extract-the-text","title":"Extract the text","text":"<p>We create a python function which will read an artifact from the platform's repository and leverage the Tika service to extract the textual content and write it to a HTML file.</p> <pre><code>extract_function = project.new_function(\n    name=\"extract\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/extract.py\",\n    handler=\"extract_text\"\n)\n</code></pre> <p>We store the PDF file as artifact and download it. You are free to change the address to whichever PDF file you would like.</p> <pre><code>pdf = project.new_artifact(\"document.pdf\",kind=\"artifact\", path=\"https://raw.githubusercontent.com/scc-digitalhub/digitalhub-tutorials/master/s7-rag/resources/document.pdf\")\npdf.download(\"document.pdf\")\n</code></pre> <p>Then, we run the function by passing it the artifact and the URL to Tika:</p> <pre><code>extract_run = extract_function.run(\"job\", inputs={\"artifact\": pdf.key}, parameters={\"tika_url\": TIKA_URL}, wait=True)\n</code></pre> <p>Let's read the file and check the content is correct:</p> <pre><code>html_artifact = project.get_artifact(\"document.pdf_output.html\")\nhtml_artifact.download()\nwith open('./artifact/output.html', 'r') as file:\n    file_content = file.read()\n    print(file_content)\n</code></pre>"},{"location":"scenarios/rag/knowledge-base/#embeddings","title":"Embeddings","text":"<p>Embeddings are vectors of floating-point numbers that represent words and indicate how strong the connection between certain words is.</p> <p>We need to deploy a suitable model to generate embeddings from the extracted text.</p> <pre><code>embed_function = project.new_function(\n    \"embed\",\n    kind=\"kubeai-text\",\n    model_name=\"embmodel\",\n    features=[\"TextEmbedding\"],\n    engine=\"VLLM\",\n    url=\"hf://thenlper/gte-base\",\n)\n</code></pre> <pre><code>embed_run = embed_function.run(\"serve\", wait=True)\n</code></pre> <pre><code>status = embed_run.refresh().status\nprint(\"Service status:\", status.state)\n</code></pre> <pre><code>EMBED_URL = status.to_dict()[\"service\"][\"url\"]\nEMBED_MODEL = status.to_dict()[\"openai\"][\"model\"]\nprint(f\"service {EMBED_URL} with model {EMBED_MODEL}\")\n</code></pre> <p>Let's check that the model is ready. We need the OpenAI client installed:</p> <pre><code>%pip install -qU openai\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key=\"ignored\", base_url=f\"{EMBED_URL}/v1\")\nresponse = client.embeddings.create(\n    input=\"Your text goes here.\",\n    model=EMBED_MODEL\n)\n</code></pre> <pre><code>response\n</code></pre>"},{"location":"scenarios/rag/knowledge-base/#embedding-generation","title":"Embedding generation","text":"<p>We define a function to read the text from the repository and push the data into the vector store.</p> <pre><code>embedder_function = project.new_function(\n    name=\"embedder\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    requirements=[\n        \"transformers==4.50.3\",\n        \"psycopg_binary\",\n        \"openai\",\n        \"langchain-text-splitters\",\n        \"langchain-community\",\n        \"langgraph\",\n        \"langchain-core\",\n        \"langchain-huggingface\",\n        \"langchain_postgres\",\n        \"langchain[openai]\",\n        \"beautifulsoup4\",\n    ],\n    code_src=\"src/embedder.py\",\n    handler=\"process\",\n)\n</code></pre> <p>Parameters are as follows:</p> <ul> <li>Embed model is served at <code>EMBED_URL</code> with <code>EMBED_MODEL</code>.</li> <li>Input artifact (HTML) is <code>html_artifact</code>.</li> </ul> <pre><code>embedder_run = embedder_function.run(\n    \"job\",\n    inputs={\"input\": html_artifact.key},\n    envs=[\n        {\n            \"name\": \"EMBEDDING_SERVICE_URL\",\n            \"value\": EMBED_URL\n        },\n        {    \"name\": \"EMBEDDING_MODEL_NAME\",\n            \"value\": EMBED_MODEL,\n        }\n    ],\n    wait=True,\n)\n</code></pre> <p>Check that the run has completed:</p> <pre><code>embedder_run.status.state\n</code></pre>"},{"location":"scenarios/rag/llm/","title":"LLM","text":"<p>The first step is to deploy and serve a pre-trained Large Language Model. We'll work with the Llama model for text generation.</p>"},{"location":"scenarios/rag/llm/#project-initialization","title":"Project initialization","text":"<p>Initialize a project on the platform:</p> <pre><code>import digitalhub as dh\nimport getpass as gt\n\nUSERNAME = gt.getuser()\n\nproject = dh.get_or_create_project(f\"{USERNAME}-tutorial-project\")\nprint(project.name)\n</code></pre>"},{"location":"scenarios/rag/llm/#model-configuration","title":"Model configuration","text":"<p>We'll create a function to serve the LLama3.2 model directly. The model path may use different protocols, such as <code>ollama://</code> or <code>hf://</code>, to directly reference models from the corresponding hub, without manual downloading.</p> <pre><code>llm_function = project.new_function(\n    name=\"llama32-1b\",\n    kind=\"kubeai-text\",\n    model_name=f\"{USERNAME}-model\",\n    url=\"ollama://llama3.2:1b\",\n    engine='OLlama',\n    features=['TextGeneration']\n)\n</code></pre>"},{"location":"scenarios/rag/llm/#model-serving","title":"Model serving","text":"<p>To deploy the model, we use a GPU profile to accelerate the generation.</p> <pre><code>llm_run = llm_function.run(\"serve\", profile=\"1xa100-80GB\", wait=True)\n</code></pre> <p>Let's check that our service is running and ready to accept requests:</p> <pre><code>service = llm_run.refresh().status.service\nprint(\"Service status:\", service)\n</code></pre> <p>When the service is ready, we need to wait for the model to be downloaded and deployed.</p> <pre><code>status = llm_run.refresh().status.k8s.get(\"Model\")['status']\nprint(\"Model status:\", status)\n</code></pre> <p>Once ready, we save the URL and model:</p> <pre><code>CHAT_URL = llm_run.status.to_dict()[\"service\"][\"url\"]\nCHAT_MODEL = llm_run.status.to_dict()[\"openai\"][\"model\"]\nprint(f\"service {CHAT_URL} with model {CHAT_MODEL}\")\n</code></pre>"},{"location":"scenarios/rag/llm/#test-the-llm-api","title":"Test the LLM API","text":"<p>Let's test our deployed model with a prompt:</p> <pre><code>model_name =llm_run.refresh().status.k8s.get(\"Model\").get(\"metadata\").get(\"name\")\njson_payload = {'model': model_name, 'prompt': 'Describe MLOps'}\n</code></pre> <pre><code>import pprint\npp = pprint.PrettyPrinter(indent=2)\nresult = llm_run.invoke(model_name=model_name, json=json_payload, url=service['url']+'/v1/completions').json()\nprint(\"Response:\")\npp.pprint(result)\n</code></pre> <p>The response contains the answer, as well as some usage parameters.</p>"},{"location":"scenarios/rag/rag/","title":"RAG application with LangChain","text":"<p>This step will define the agent which connects the embedding model, the chat model and the vector store to fullfill the RAG scenario.</p> <p>You should have the URLs and models for the latest <code>RUNNING</code> runs of the two functions from the previous steps of the scenario:</p> <pre><code>print(f\"Service {EMBED_URL} with model {EMBED_MODEL}\")\nprint(f\"Service {CHAT_URL} with model {CHAT_MODEL}\")\n</code></pre>"},{"location":"scenarios/rag/rag/#create-the-agent","title":"Create the agent","text":"<p>We will register a python function implementing the RAG agent with LangChain:</p> <pre><code>serve_func = project.new_function(\n    name=\"rag-service\", \n    kind=\"python\", \n    python_version=\"PYTHON3_10\",\n    code_src=\"src/serve.py\",     \n    handler=\"serve\",\n    init_function=\"init\",\n    requirements=[\"transformers==4.50.3\", \"psycopg_binary\", \"openai\", \"langchain-text-splitters\", \"langchain-community\", \"langgraph\", \"langchain-core\", \"langchain-huggingface\", \"langchain_postgres\", \"langchain[openai]\"]\n)\n</code></pre> <p>Then, we can run an instance connecting the model services together. It may take a while for this run to finish initialization. If the execution fails, it is probably due to the large number of dependencies required.</p> <pre><code>serve_run = serve_func.run(\n    action=\"serve\",\n    resources={\n        \"cpu\": \"4\",\n        \"mem\": \"4Gi\",\n    },\n    envs=[\n            {\"name\": \"CHAT_MODEL_NAME\", \"value\": CHAT_MODEL},\n            {\"name\": \"CHAT_SERVICE_URL\", \"value\": CHAT_URL},\n            {\"name\": \"EMBEDDING_MODEL_NAME\", \"value\": EMBED_MODEL},\n            {\"name\": \"EMBEDDING_SERVICE_URL\", \"value\": EMBED_URL}\n         ],\n    wait=True\n)\n</code></pre> <pre><code>AGENT_URL = serve_run.status.to_dict()[\"service\"][\"url\"]\nprint(AGENT_URL)\n</code></pre> <p>To test our API, we make a call to the service endpoint, providing JSON text with an example question.</p> <pre><code>import requests\n\nres = requests.post(f\"http://{AGENT_URL}\",json={\"question\": \"What does the platform integrate as data stores?\"})\nprint(res.json())\n</code></pre>"},{"location":"scenarios/rag/webui/","title":"Agent Web UI","text":"<p>Finally, we build a web interface to test the agent. The interface will be available via browser by proxying the port through the workspace.</p>"},{"location":"scenarios/rag/webui/#deploy-the-ui","title":"Deploy the UI","text":"<p>We use Streamlit to serve a simple webpage with an input field connected to the agent API.</p> <p>Streamlit is a Python framework to create browser applications with little code.</p> <pre><code>%pip install -qU streamlit langgraph langchain-core langchain-postgres \"langchain[openai]\" psycopg_binary\n</code></pre> <p>Add the models' names and service URLs to the environment file:</p> <pre><code>with open(\"./streamlit.env\", \"w\") as env_file:\n    env_file.write(f\"CHAT_MODEL_NAME={CHAT_MODEL}\\n\")\n    env_file.write(f\"CHAT_SERVICE_URL={CHAT_URL}\\n\")\n    env_file.write(f\"EMBEDDING_MODEL_NAME={EMBED_MODEL}\\n\")\n    env_file.write(f\"EMBEDDING_SERVICE_URL={EMBED_URL}\\n\")\n</code></pre> <p>Write the function implementing the RAG UI to file:</p> <pre><code>%%writefile 'rag-streamlit-app.py'\nimport os\nimport bs4\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom langchain import hub\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_postgres import PGVector\nfrom langgraph.graph import START, StateGraph\nfrom openai import OpenAI\nfrom pathlib import Path\nfrom typing_extensions import List, TypedDict\n\n# Read environment variables\nadd_env_path = Path('.') / 'streamlit.env'\nload_dotenv(dotenv_path=add_env_path, override=True)\n\nPG_USER = os.environ[\"DB_USERNAME\"]\nPG_PASS = os.environ[\"DB_PASSWORD\"]\nPG_HOST = os.environ[\"DB_HOST\"]\nPG_PORT = os.environ[\"DB_PORT\"]\nDB_NAME = os.environ[\"DB_DATABASE\"]\nACCESS_TOKEN = os.environ[\"DHCORE_ACCESS_TOKEN\"]\n\nchat_model_name = os.environ[\"CHAT_MODEL_NAME\"]\nchat_service_url = os.environ[\"CHAT_SERVICE_URL\"]\nembedding_model_name = os.environ[\"EMBEDDING_MODEL_NAME\"]\nembedding_service_url = os.environ[\"EMBEDDING_SERVICE_URL\"]\nPG_CONN_URL = (\n    f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{DB_NAME}\"\n)\n\n# Embedding model\nclass CEmbeddings(OpenAIEmbeddings):\n    def embed_documents(self, docs):\n        client = OpenAI(api_key=\"ignored\", base_url=f\"{embedding_service_url}/v1\")\n        emb_arr = []\n        for doc in docs:\n            #sanitize string: replace NUL with spaces\n            d=doc.replace(\"\\x00\", \"-\")            \n            embs = client.embeddings.create(\n                input=d,\n                model=embedding_model_name\n            )\n            emb_arr.append(embs.data[0].embedding)\n        return emb_arr\n\ncustom_embeddings = CEmbeddings(api_key=\"ignored\")\n\n# Vector store\nvector_store = PGVector(\n    embeddings=custom_embeddings,\n    collection_name=f\"{embedding_model_name}_docs\",\n    connection=PG_CONN_URL,\n)\n\n# Chat model\nos.environ[\"OPENAI_API_KEY\"] = \"ignore\"\nllm = init_chat_model(chat_model_name, model_provider=\"openai\", base_url=f\"{chat_service_url}/v1/\")\n\n# Define prompt and operations\nprompt = hub.pull(\"rlm/rag-prompt\")\n\nclass State(TypedDict):\n    question: str\n    context: List[Document]\n    answer: str\n\ndef retrieve(state: State):\n    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n    return {\"context\": retrieved_docs}\n\ndef generate(state: State):\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n    response = llm.invoke(messages)\n    return {\"answer\": response.content}\n\n# Define graph of operations\ngraph_builder = StateGraph(State).add_sequence([retrieve, generate])\ngraph_builder.add_edge(START, \"retrieve\")\ngraph = graph_builder.compile()\n\n# Streamlit setup\nst.title(\"RAG App\")\nst.write(\"Welcome to the RAG (Retrieval-Augmented Generation) app.\")\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nqa = st.container()\n\nwith st.form(\"rag_form\", clear_on_submit=True):\n    question = st.text_input(\"Question\", \"\")\n    submit = st.form_submit_button(\"Submit\")\n\nif submit:\n    # Load and chunk contents\n    if question:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n        with qa.chat_message(\"user\"):\n            st.write(question)\n\n        response = graph.invoke({\"question\": question})\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"answer\"]})\n        with qa.chat_message(\"assistant\"):\n            st.write(response[\"answer\"])\n    else:\n        with qa.chat_message(\"assistant\"):\n            st.write(\"You didn't provide a question!\")\n</code></pre>"},{"location":"scenarios/rag/webui/#launch-and-test-the-streamlit-app","title":"Launch and test the Streamlit app","text":"<p>This command launches the Streamlit app, based on the file written by the previous cell. To access the app, you will need to forward port 8501 in Coder. </p> <p>Try asking the app a question.</p> <pre><code>!streamlit run rag-streamlit-app.py --browser.gatherUsageStats false\n</code></pre>"},{"location":"scenarios/whisper-fine-tuning/convert-test/","title":"Convert to faster-whisper and test","text":"<p>We will convert the model to be compatible with faster-whisper, a faster re-implementation of the Whisper model.</p> <p>Download the model:</p> <pre><code>model = project.get_model(\"whisper-ft\")\nmodel.download(\"./model/whisper-ft\", overwrite=True)\n</code></pre> <p>Install <code>faster-whisper</code>:</p> <pre><code>%pip install faster-whisper transformers torch==2.8.0\n</code></pre> <p>Convert the model:</p> <pre><code>from ctranslate2.converters import TransformersConverter\n\ntc = TransformersConverter(\"./model/whisper-ft\", copy_files=['tokenizer.json', 'preprocessor_config.json'])\ntc.convert('./model/faster-whisper-ft', quantization=\"float16\")\n</code></pre> <pre><code>from faster_whisper import WhisperModel\n\nmodel = WhisperModel('./model/faster-whisper-ft', device=\"cpu\")\n</code></pre> <p>Download a sample file:</p> <pre><code>!wget -O kubeai.mp4 \"https://github.com/user-attachments/assets/711d1279-6af9-4c6c-a052-e59e7730b757\"\n</code></pre> <p>Finally, we run the model on the sample file to test the results:</p> <pre><code>segments, info = model.transcribe(\"kubeai.mp4\", beam_size=5)\n\nprint(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n\nfor segment in segments:\n    print(\"[%.2fs -&gt; %.2fs] %s\" % (segment.start, segment.end, segment.text))\n</code></pre>"},{"location":"scenarios/whisper-fine-tuning/create-dataset/","title":"Create and process dataset","text":"<p>In this step, we create a function to pre-process the dataset and store it as an artifact.</p> <p>Make sure the file specified in the <code>code_src</code> parameter is present.</p> <pre><code>func = project.new_function(\n    name=\"create-dataset\", \n    kind=\"python\", \n    python_version=\"PYTHON3_10\", \n    code_src=\"src/fine_tuning_seq2seq.py\",  \n    handler=\"preprocess_dataset\",\n    requirements=[\"datasets[audio]==3.6.0\", \"transformers==4.56.1\", \"torch==2.8.0\", \"accelerate==1.10.1\", \"evaluate==0.4.5\", \"jiwer==4.0.0\"]\n)\n</code></pre> <p>Run the function. It may take over 10 minutes. Note that here for the tutorial purposes only a small subset of records is considered (<code>max_train_samples</code> and <code>max_eval_samples</code>).</p> <pre><code>train_run = func.run(action=\"job\",\n                     parameters={\n                         \"model_id\": \"openai/whisper-small\",\n                         \"artifact_name\": \"audio-dataset\",\n                         \"dataset_name\": \"fsicoli/common_voice_17_0\",\n                         \"language\": \"Italian\",\n                         \"language_code\": \"it\",\n                         \"max_train_samples\": 100,\n                         \"max_eval_samples\": 100\n                     },\n                     secrets=[\"HF_TOKEN\"],\n                     envs=[\n                        {\"name\": \"HF_HOME\", \"value\": \"/local/data/huggingface\"},\n                        {\"name\": \"TRANSFORMERS_CACHE\", \"value\":  \"/local/data/huggingface\"}\n                     ],\n                     volumes=[{\n                        \"volume_type\": \"persistent_volume_claim\",\n                        \"name\": \"volume-llmpa\",\n                        \"mount_path\": \"/local/data\",\n                        \"spec\": { \"size\": \"300Gi\" }}]\n                    )\n</code></pre> <p>If the run fails</p> <p>If the run fails, inspect its logs on the console. If you see mention of the dataset being <code>a gated dataset on the Hub</code>, you likely did not enable your HuggingFace account to have access to this repository. Additionally, building the function before running the job may help. The build process typically takes around 10 minutes. <pre><code>train_build = func.run(action=\"build\")\n</code></pre></p> <p>Insufficient resources</p> <p>Depending on the amount of data, the processing may require significant amount of resources. If the platform is configured in a way that the default amount of memory is limited, ensure it is sufficient for the task. Otherwise specify the required amount explicitly, passing the resource requirements to the spec, e.g., <pre><code>resources={\"mem\": \"8Gi\"}\n</code></pre></p>"},{"location":"scenarios/whisper-fine-tuning/fine-tuning/","title":"Fine-tuning","text":"<p>Next, we create and run the function for fine-tuning.</p> <p>Create the function:</p> <pre><code>func = project.new_function(\n    name=\"train-whisper\", \n    kind=\"python\", \n    python_version=\"PYTHON3_10\", \n    code_src=\"src/fine_tuning_seq2seq.py\",  \n    handler=\"train_and_log_model\",\n    requirements=[\"datasets[audio]==3.6.0\", \"transformers==4.52.0\", \"torch==2.8.0\", \"accelerate==1.10.1\", \"evaluate==0.4.5\", \"jiwer==4.0.0\"]\n)\n</code></pre> <p>Run the function. This may take even longer than the previous one.</p> <pre><code>train_run = func.run(action=\"job\",\n                     parameters={\n                         \"model_id\": \"openai/whisper-small\",\n                         \"model_name\": \"whisper-ft\",\n                         \"dataset_name\": \"fsicoli/common_voice_17_0\",\n                         \"language\": \"Italian\",\n                         \"language_code\": \"it\",\n                         \"max_train_samples\": 100,\n                         \"max_eval_samples\": 100,\n                         \"eval_steps\": 100,\n                         \"save_steps\": 100,\n                         \"max_steps\": 500,\n                         \"warmup_steps\": 50\n                     },\n                     profile=\"1xa100\",\n                     secrets=[\"HF_TOKEN\"],\n                     envs=[\n                        {\"name\": \"HF_HOME\", \"value\": \"/local/data/huggingface\"},\n                        {\"name\": \"TRANSFORMERS_CACHE\", \"value\":  \"/local/data/huggingface\"}\n                     ],\n                     volumes=[{\n                        \"volume_type\": \"persistent_volume_claim\",\n                        \"name\": \"volume-llmpa\",\n                        \"mount_path\": \"/local/data\",\n                        \"spec\": { \"size\": \"100Gi\" }}]\n                    )\n</code></pre>"},{"location":"scenarios/whisper-fine-tuning/intro/","title":"Fine-tuning speech-to-text","text":"<p>In this scenario, we fine-tune Whisper, a model for speech-to-text recognition.</p>"},{"location":"scenarios/whisper-fine-tuning/intro/#requirements","title":"Requirements","text":"<p>You'll need a HuggingFace token that has access to the voice library we will use. Enable your token to access this repository.</p> <p>Create a workspace on Coder for Jupyter. When the workspace is up, access Jupyter and create a new notebook.</p>"},{"location":"scenarios/whisper-fine-tuning/intro/#set-up","title":"Set-up","text":"<p>Import the platform's library and create a project:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"whisper-fine-tuning\")\n</code></pre> <p>Create a secret as follows, make sure you replace the value with the token that has access to the aforementioned repository:</p> <pre><code>project.new_secret(name=\"HF_TOKEN\", secret_value=\"my-token\")\n</code></pre> <p>The functions we will run use code from a Python file. Due to the many lines this file has, instead of presenting it in this documentation, we invite you to download it from the subfolder related to this scenario. Use the <code>src</code> directory and ensure it is at the same level of the notebook you're using. </p>"},{"location":"sdk/intro/","title":"Python SDK Reference","text":"<p>Python SDK provides the functionality of the Core API and allows for working with the platform entities directly from interactive workspaces or your prefered development environment.</p> <p>See here the SDK documentation (refer to the version corresponding to the version of the platform):</p> <ul> <li>SDK Reference</li> </ul>"},{"location":"tasks/code-source/","title":"Code source","text":"<p>In several runtime objects, it is possible to execute a program (e.g. a function, a query or a workflow) written in a source. This source can be a single code file, a plain text string or a base64 encoded string, a zip archive or a git repository. Beside the source, you need also to define a <code>handler</code>, which is the entrypoint of the code.</p>"},{"location":"tasks/code-source/#code-source-definition","title":"Code source definition","text":"<p>In the SDK, there are three different types of source code:</p> <ul> <li><code>code</code> which is a plain string source.</li> <li><code>base64</code> which is a base64 encoded string source.</li> <li><code>code_src</code> which is a code source URI.</li> </ul>"},{"location":"tasks/code-source/#plain-text-source","title":"Plain text source","text":"<p>You can define a plain text source using the <code>code</code> parameter.</p> <p>Here follow an example of a plain text source with the Python runtime:</p> <pre><code>my_code = \"\"\"\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"filtered\"])\ndef myfunction(di: Dataitem, col1: str):\n    df = di.as_df()\n    df = df[[\"col1\"]]\n    return df\n\"\"\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will execute the code in the <code>my_code</code> variable.</p>"},{"location":"tasks/code-source/#base64-encoded-source","title":"Base64 encoded source","text":"<p>You can define a base64 encoded source using the <code>base64</code> parameter.</p> <p>Here follow an example of a base64 encoded source with the Python runtime:</p> <pre><code># Same function as above encoded in base64\nbase64_code = \"ZnJvbSBkaWdpdGFsaHViX3J1bnRpbWVfcHl0aG9uIGltcG9ydCBoYW5kbGVyCgpAaGFuZGxlcihvdXRwdXRzPVsiZmlsdGVyZWQiXSkKZGVmIG15ZnVuY3Rpb24oZGk6IERhdGFpdGVtLCBjb2wxOiBzdHIpOgogICAgZGYgPSBkaS5hc19kZigpCiAgICBkZiA9IGRmW1siY29sMSJdXQogICAgcmV0dXJuIGRm\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       base64=base64_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will execute the code in the <code>base64_code</code> variable.</p>"},{"location":"tasks/code-source/#code-source-uri","title":"Code source URI","text":"<p>You can define a code source URI using the <code>code_src</code> parameter. We support the following types of URIs:</p> <ul> <li>Local file path</li> <li>Git repository</li> <li>S3 zip archive</li> </ul>"},{"location":"tasks/code-source/#local-file-path","title":"Local file path","text":"<p>The local file path can be specified with the <code>path/to/file.ext</code> format.</p> <pre><code>my_code = \"src/my-func.py\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will import the code in the <code>src/my-func.py</code> file, encodes it in base64 and then executes it. If the file is not found, the function will raise an exception.</p>"},{"location":"tasks/code-source/#remote-git-repository","title":"Remote git repository","text":"<p>The remote git repository can be specified with the <code>git+https://repo-host.com/some-user/some-repo</code> format. The anteposition <code>git+</code> is required, the rest of the URL is the repository URL.</p> <pre><code>my_repo = \"git+https://repo-host/some-user/some-repo\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_repo,\n                       handler=\"path:function\")\n</code></pre> <p>When a git repository is specified, the function try to clone the repository and execute the code in the specified in the <code>handler</code> entrypoint (See below). If the repository does not exist, the function will raise an exception.</p>"},{"location":"tasks/code-source/#credentials","title":"Credentials","text":"<p>You may need credentials to access the repository. The credentials can be specified in three environment variables: <code>GIT_TOKEN</code>, <code>GIT_USER</code>and <code>GIT_PASSWORD</code>.</p> <p>Note</p> <p>You must set the env variable before defining the function. Token auth is recommended and takes precedence over basic auth. Please also verify that the repo provider supports basic auth.</p>"},{"location":"tasks/code-source/#token","title":"Token","text":"<p>The token is a plain string. It will be passed in the URL according to the git provider.</p> <p>Example:</p> <pre><code># GitHub token\nos.environ[\"GIT_TOKEN\"] = \"github_pat_...\"\n\n# GitLab token\nos.environ[\"GIT_TOKEN\"] = \"glpat...\"\n\n# function definition\n</code></pre>"},{"location":"tasks/code-source/#user-and-password","title":"User and password","text":"<p>User and password are plain strings.</p> <pre><code>os.environ[\"GIT_USER\"] = \"some-user\"\nos.environ[\"GIT_PASSWORD\"] = \"some-password\"\n\n# function definition\n</code></pre>"},{"location":"tasks/code-source/#remote-zip-s3-archive","title":"Remote zip s3 archive","text":"<p>The remote zip s3 archive can be specified with the <code>zip+s3://some-bucket/some-key.zip</code> format. The anteposition <code>zip+</code> is required, the rest of the URL is an S3 URL in the form <code>s3://some-bucket/some-key.zip</code>. The code source is archived in a zip file, which is unpacked at runtime.</p> <pre><code>my_archive = \"zip+s3://some-bucket/some-key.zip\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_archive,\n                       handler=\"path:function\")\n</code></pre>"},{"location":"tasks/code-source/#handler","title":"Handler","text":"<p>The <code>handler</code> parameter is the entrypoint of the code. There are some rules to follow when defining it.</p> <p>If the source code is:</p> <ul> <li>Plain text</li> <li>Base64 encoded</li> <li>Local file path</li> </ul> <p>Then the entrypoint should be the function name.</p> <pre><code>my_code = \"\"\"\ndef myfunction():\n    ...\n\"\"\"\n\nfunc = dh.new_function(...,\n                       code=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>If the source code is:</p> <ul> <li>S3 zip archive</li> <li>Git repository</li> </ul> <p>Then the entrypoint should be the path to the file where the code is stored (expressed with <code>.</code> separator) and the name of the function separated by a <code>:</code>.</p> <pre><code>my_code = \"git+https://repo-host/some-user/some-repo\"\n\nfunc = dh.new_function(...,\n                       code_src=my_code,\n                       handler=\"src.subdir.etc:myfunction\")\n</code></pre>"},{"location":"tasks/data/","title":"Data and Transformations","text":"<p>The platform supports data of different types to be stored and operated by the underlying storage subsystems.</p> <p>Specifically, the platform natively supports two types of storages:</p> <ul> <li>persistence object storage (datalake S3 Minio), which manages immutable data in the form of files.</li> <li>operational relational data storage (PostgreSQL database), which is used for efficient querying of mutable data. Postgres is rich with extensions, most notably for geo-spatial and time-series data.</li> </ul> <p>The data is represented in the platform as entities of different types, depending on its usage and format. More specifically, we distinguish:</p> <ul> <li>data items, which represent immutable data sets resulting from different transformation operations and are ready for use in differerent types of analysis. Data items are enriched with metadata (versions, lineage, stats, profiling, schema, ...) and unique keys and managed and persisted to the datalake directly by the platform in the form of Parquet files. It is possible to treat tabular data (items of <code>table</code> kind) as, for example, DataFrames, using conventional libraries.</li> <li>artifacts, which represent arbitrary files, not limited to tabular format, stored to the datalake with some extra metadata.</li> </ul> <p>Each data entity may be accessed and manipulated by the platform via UI or using the API, for example with SDK.</p>"},{"location":"tasks/data/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/data/#artifacts","title":"Artifacts","text":"<p>Artifacts can be managed as entities from the UI. You can access them from the left menu. You can:</p> <ul> <li><code>create</code> a new artifact</li> <li><code>filter</code> artifacts by name and kind</li> <li><code>expand</code> an artifact to see its 5 latest versions</li> <li><code>show</code> the details of an artifact</li> <li><code>edit</code> an artifact</li> <li><code>delete</code> an artifact</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete artifacts using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a two-step form will be shown:</p> <p></p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the artifact</li> <li><code>Kind</code>: kind of the artifact</li> <li>(Spec) <code>Path</code>: remote path where the artifact is stored. If you instead upload the artifact at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later.</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description of the artifact</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the artifact</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Relationships</code>: relationships with other entities</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Spec) <code>Source path</code>: local path to the artifact, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view an artifact's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update","title":"Update","text":"<p>You can update an artifact by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete","title":"Delete","text":"<p>You can delete an artifact from either its detail page or the list of artifacts, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#data-items","title":"Data items","text":"<p>Data items can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new data item</li> <li><code>expand</code> a data item and see its 5 latest versions</li> <li><code>show</code> the details of a data item</li> <li><code>edit</code> a data item</li> <li><code>delete</code> a data item</li> <li><code>filter</code> data items by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete data items using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create_1","title":"Create","text":"<p>Click <code>CREATE</code> and a two-step form will be shown:</p> <p></p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name of the dataitem</li> <li><code>Kind</code>: kind of the dataitem</li> <li>(Spec) <code>Path</code>: remote path where the data item is stored. If you instead upload the data item at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later:</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the data item</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Relationships</code>: relationships with other entities</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> </ul>"},{"location":"tasks/data/#kind","title":"Kind","text":"<p>There are 2 possible kinds for dataitems:</p> <ul> <li><code>Dataitem</code>: indicates it is a generic data item. There are no specific attributes in the creation page.</li> <li><code>table</code>: indicates that the data item points to a table. The optional parameter is the schema of the table in table_schema format.</li> </ul>"},{"location":"tasks/data/#read_1","title":"Read","text":"<p>Click <code>SHOW</code> to view a data item's details.</p> <p></p> <p>Based on the <code>kind</code>, there may be a <code>schema</code>, indicating that the dataitem point to a table.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update_1","title":"Update","text":"<p>You can update a data item by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete_1","title":"Delete","text":"<p>You can delete a data item from either its detail page or the list of data items, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#data-exploration-in-s3-browser","title":"Data exploration in S3 browser","text":"<p>The S3 object storage can be explored in a dedicated page. Check the File Browser section for more information.</p>"},{"location":"tasks/data/#management-via-sdk","title":"Management via SDK","text":""},{"location":"tasks/data/#managing-artifacts-with-sdk","title":"Managing artifacts with SDK","text":"<p>Artifacts can be created and managed as entities with the SDK CRUD methods. Check the SDK Artifacts documentation for more information.</p>"},{"location":"tasks/data/#managing-dataitems-with-sdk","title":"Managing dataitems with SDK","text":"<p>Dataitems can be created and managed as entities with the SDK CRUD methods. Check the SDK Dataitem documentation for more information.</p>"},{"location":"tasks/files/","title":"File Browser","text":"<p>The File Browser is an integrated component within the web-based administrative console that enables users to explore, view, and manage files stored in the project\u2019s repository, directly from the browser interface. </p> <p>The browser shows a full, unfiltered view of the project data store: differently from the catalog's view, the browser presents content organized in a tree, with nested folders and files. </p> <p>By default, every content registered in the catalog will be stored under a path resembling the following schema:</p> <p><code>/[entity]/[name]/[version]/..</code></p> <p></p> <p>Do note that adding/moving/removing files from those paths will probably result in an inconsistent data layout: do it at your risk.</p>"},{"location":"tasks/files/#key-features","title":"Key Features","text":"<ul> <li>Hierarchical Navigation:</li> </ul> <p>Displays the repository\u2019s directory structure in an expandable tree view, allowing users to easily browse folders and subfolders.</p> <ul> <li>File Preview and Metadata</li> </ul> <p>Supports viewing file details such as name, size, type, modification date, and hash. Text-based files (e.g., .txt, .md, .json, .xml, .java) and images (*.png, .jpg) can be previewed inline within the console.</p> <ul> <li>Contextual Actions:</li> </ul> <p>Users can perform actions such as download, upload on single or multiple files.</p> <ul> <li>Access Control:</li> </ul> <p>The file browser respects user roles and permissions, ensuring only authorized users can modify or access certain files.</p> <ul> <li>File sharing</li> </ul> <p>Users can generate read-only links to share files (and folders) with external collaborators.</p>"},{"location":"tasks/files/#exploring-files","title":"Exploring Files","text":"<p>By opening the Files page from the left menu of the console, users are presented with a familiar view showing the content of the storage.</p> <p></p> <p>Navigation is performed by clicking on folder names, or via the interactive breadcrumb.</p> <p>When a single file is selected, the browser will show a contextual view with the file details. The following actions can be performed (when applicable) on the file via the corresponding buttons:</p> <ul> <li>download: initiate the file download via a temporary secure link</li> <li>preview: open the file's preview (for supported files)</li> <li>share: open the file sharing dialog</li> <li>delete: delete the file</li> </ul> <p></p> <p>Under the actions toolbar the side view reports all the metadata available for the selected file:</p> <ul> <li>Name</li> <li>Path</li> <li>Content type</li> <li>Size</li> <li>Last modified (date)</li> <li>Hash (etag/md5/sha)</li> </ul>"},{"location":"tasks/files/#file-uploading","title":"File Uploading","text":"<p>The <code>UPLOAD</code> button on the top right of the File Browser opens a dialog for users to upload any kind of file to the project's repository. </p> <p>The uploader supports:</p> <ul> <li>a single file</li> <li>multiple files</li> <li>a folder</li> </ul> <p></p> <p>By default, no limitations on file size, content type or path are applied. Administrators may configure more restrictive conditions for specific environments.</p>"},{"location":"tasks/files/#file-sharing","title":"File Sharing","text":"<p>Files stored in the project's repository are private by default: only members of the project can access the store. In order to enable file downloads by external collaborators, the browser supports file sharing via temporay pre-authorized secure links.</p> <p>Such links are generated with a signature derived from the user's credentials, and will expire after a configurable amount of time. </p> <p>Any user with the full link will be able to directly download the file: no specific client or code is required.</p> <p>To share a file:</p> <ol> <li>open the file details</li> <li>click on the <code>SHARE</code> button </li> <li>define the desired duration and click <code>SHARE</code></li> <li>copy the link and send to users.</li> </ol> <p>Do note that the generated secure link won't be shown again.</p> <p></p>"},{"location":"tasks/functions/","title":"Functions and Runtimes","text":"<p>Functions are the logical description of something that the platform may execute and track for you. A function may represent code to run as a job, an ML model inference to be used as batch procedure or as a service, a data validation, etc.</p> <p>In the platform we perform actions over functions (also referred to as \"tasks\"), such as job execution, deploy, container image build. A single action execution is called run, and the platform keeps track of these runs, with metadata about function version, operation parameters, and runtime parameters for a single execution.</p> <p>They are associated with a given runtime, which implements the actual execution and determines which actions are available. Examples are DBT, Container, Python, etc. Runtimes  are highly specialized components which can translate the representation of a given execution, as expressed in the run, into an actual execution operation performed via libraries, code, external tools etc.</p> <p>Runtimes define the key point of extension of the platform: new runtimes may be added in order to implement the low-level logic of \"translating\" the high level operation definition into an executable run. For example, DBT runtime allows for defining the transformation as a task that, given the input table reference, produces a datastt appyling the function defined as SQL code. The runtime in this case is responsible for converting the specification and the references to a dedicated Kubernetes Job that runs DBT transformation and stores the corresponding dataset.</p> <p>The set of the supported runtimes is documented in Runtimes References section. Independently of the specific runtime implementation, the flow of actions with respect to the function definition and execution is the following:</p> <ul> <li>define a new function providing its name, runtime, definition (e.g., source code), and configuration (e.g., dependencies). The function definition is saved by the project. Each change to the function spec creates a new function version so that the executions of different function versions are independently tracked.</li> <li>execute a task over the function providing the configuration of the task (e.g., the K8S resources needed for the execution), the execution parameters and inputs (if any). This creates a new task specification and a new run instance tracked by the platform.</li> </ul> <p>The definition and execution of the functions may be performed either via UI or via Python SDK.</p>"},{"location":"tasks/functions/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/functions/#functions","title":"Functions","text":"<p>Functions can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new function</li> <li><code>expand</code> a function to see its 5 latest versions</li> <li><code>show</code> the details of a function</li> <li><code>edit</code> a function</li> <li><code>delete</code> a function</li> <li><code>filter</code> functions by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete functions using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/functions/#create","title":"Create","text":"<p>Functions can be created either from scratch or from a preconfigured template. Templates for common use cases, such as deploying LLMs or performing machine learning related tasks with Python, are available.</p> <p>Click <code>CREATE</code> and a three-step form will be shown, the first step being the selection of either a template or a kind:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the function</li> <li><code>Kind</code>: kind of function</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Audit</code>: author of creation and modification</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Relationships</code>: relationships with other entities</li> <li><code>Versioning</code>: version of the function</li> </ul> <p>Spec fields are specified in the third step and will change depending on the function's kind.</p>"},{"location":"tasks/functions/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a function's details.</p> <p></p> <p>Tabs next to <code>SPEC</code> will change depending on the function's <code>kind</code>. Some of them allow you to create runs, but we will see this in a later section.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/functions/#update","title":"Update","text":"<p>You can update a function by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/functions/#delete","title":"Delete","text":"<p>You can delete a function from either its detail page or the list of functions, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/functions/#runs","title":"Runs","text":""},{"location":"tasks/functions/#create_1","title":"Create","text":"<p>A run represents the execution of a task through a function. As such, the starting point to create a run is the function it is based on. Select one of the functions you created. You will notice multiple tabs at the top: some of the <code>kind</code>-specific tabs correspond to a task that can be executed.</p> <p></p> <p>Click <code>CREATE</code> to create a new run. You will start a 3-steps process to create a run.</p> <p>The first step will ask for parameters that depend on the function's <code>kind</code> and the task you are creating the run for, but will generally also ask if you wish to configure resources to allocate, environment variables, secrets, volumes and node modules.</p> <p>The second step will ask, if applicable, to specify inputs, outputs and parameters.</p> <p>The third step will simply present a recap.</p> <p></p>"},{"location":"tasks/functions/#view-and-manage","title":"View and manage","text":"<p>By going through a function's tabs, you can access the corresponding runs, but you may also access all runs from the Runs section in the left menu (also available as Jobs and runs in the dashboard).</p> <p></p> <p>Click on a run to view its details, such as its specifications, inputs, outputs, logs and metrics.</p> <p></p>"},{"location":"tasks/functions/#management-via-sdk","title":"Management via SDK","text":"<p>Functions can be created and managed as entities with the SDK CRUD methods. Check the SDK Functions documentation for more information.</p>"},{"location":"tasks/kubernetes-resources/","title":"Using Kubernetes Resources for Runs","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on Kubernetes functionalities and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul>"},{"location":"tasks/kubernetes-resources/#how-to-define-resource-requirements","title":"How to define resource requirements","text":"<p>In the platform, kubernetes resource requirements may be defined in two ways:  * by users at run time, by requesting resources to be allocated for a given run, * by administrators at deployment time, by configuring defaults and limits along with pre-configured templates and profiles</p>"},{"location":"tasks/kubernetes-resources/#request-resources-at-runtime","title":"Request resources at runtime","text":"<p>The platform lets users require additional k8s resource to be allocated to a given function's run, either via Core UI or via SDK. Please note that it is possible to describe only some properties, leaving the rest blank without constraints. All the defaults are managed by the platform in accordance with the underlying Kubernetes deployment.</p> <p>To define requirements for single runs, developers need to include in the run specification the resource definition, in accordance with the schema.</p> <p>For example, to request for a certain amount of compute resources, the spec must contain the detailed definition as follows:</p> <pre><code>resources:\n  cpu: \"8\"\n  mem: 32Gi\n  gpu: \"1\"\n</code></pre> <p>In order to provide such definitions, users can leverage the SDK or the Core UI to programmatically or interactively define their request. Please see the Kubernetes Resources section of the documentation for more information.</p>"},{"location":"tasks/kubernetes-resources/#resource-templates-and-profiles","title":"Resource templates and profiles","text":"<p>It is possible to rely on a set of preconfigured HW profiles defined during the platform deployment.  The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. Please see Resource templates section of the documentation for more information.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p>"},{"location":"tasks/kubernetes-resources/#available-resources","title":"Available resources","text":"<p>The section lists all the resources available to users for runs. </p>"},{"location":"tasks/kubernetes-resources/#volumes","title":"Volumes","text":"<p>Users can ask for a persistent volume claim (pvc) to be created and mounted on the container being launched by the task. You need to declare the volume type as <code>persistent_volume_claim</code>, a name for the PVC for the user (e.g., <code>my-pvc</code>), the mount path on the container and a spec with the size of the PV to be reserved. The platform will create the volume and bind it to the pod lifecycle.</p> <pre><code>volumes:\n        - volume_type: \"persistent_volume_claim\",\n          name: \"my-pvc\",\n          mount_path: \"/data\",\n          spec: \n            size: \"10Gi\",\n</code></pre> <p>Note: the platform can be configured to block the usage of pre-existing volumes for security reasons. Volumes created by the platform for specific runs as ReadWriteOnce and used exclusively by the platform.</p>"},{"location":"tasks/kubernetes-resources/#hardware-resources","title":"Hardware Resources","text":"<p>Users can request a specific amount of hardware resources (cpu, memory, gpu) for a given run by declaring them  via the <code>resources</code> spec parameter.</p> <p>Supported resources are:</p> <ul> <li>CPU</li> <li>RAM memory</li> <li>GPU</li> </ul>"},{"location":"tasks/kubernetes-resources/#cpu","title":"CPU","text":"<p>To request a specific amount of CPU for the run, declare the resource type as <code>cpu</code> and specify request and/or limit values.</p> <pre><code>resources:\n  cpu: \"10\"\n</code></pre>"},{"location":"tasks/kubernetes-resources/#ram-memory","title":"RAM memory","text":"<p>To request a specific amount of RAM memory for the run, declare the resource type as <code>mem</code> and specify request and/or limit values.</p> <pre><code>resources:\n  mem: 32Gi\n</code></pre>"},{"location":"tasks/kubernetes-resources/#gpu","title":"GPU","text":"<p>To request GPU resources, specify the resource type <code>gpu</code> and set the requested value as a limit.</p> <pre><code>resources:\n  gpu: \"1\"    \n</code></pre>"},{"location":"tasks/kubernetes-resources/#secrets","title":"Secrets","text":"<p>Users can request a secret injection into the run being launched by passing the identifier inside the <code>secrets</code> field. Secrets must be stored via the platform: externally defined secrets (for example in k8s) are not accessible to users for security reasons.</p> <pre><code>secrets:\n  - my-secret-key\n</code></pre>"},{"location":"tasks/kubernetes-resources/#envs","title":"Envs","text":"<p>User can inject environment variables injection into the container being launched by passing definition of variables as key/value inside the <code>envs</code> field.</p> <pre><code>envs:\n  - name: ENV1\n    value: VALU123123\n  - name: ENV2\n    value: VALU123123  \n</code></pre>"},{"location":"tasks/kubernetes-resources/#node-selection","title":"Node selection","text":"<p>Users can request a node selector for the run being launched by defining the selector(s) as a key/value list.  The platform will add the selectors as-is to k8s resources such as Jobs, Pods, Deployments when appropriate.</p> <pre><code>node_selector:\n  - key: selectorKey\n    value: selectValue\n</code></pre> <p>See K8s Documentation for reference.</p>"},{"location":"tasks/kubernetes-resources/#tolerations","title":"Tolerations","text":"<p>To define tolerations add the definition inside the <code>tolerations</code> field of the spec, following Kubernetes specifications. Please see Kubernetes documentation.</p>"},{"location":"tasks/kubernetes-resources/#affinity","title":"Affinity","text":"<p>To define affinity add the definition inside the <code>affinity</code> field of the spec, following Kubernetes specifications. Please see Kubernetes documentation.</p>"},{"location":"tasks/kubernetes-resources/#fs-group","title":"FS group","text":"<p>To properly map volumes mounted for runs, users can specify the group id used for mount operations. This step is required when the USER used to run the process does not match the default. Define the <code>fs_group</code> field by specifying the group id as integer.</p> <pre><code>fs_group: 1000\n</code></pre>"},{"location":"tasks/kubernetes-resources/#run-as-user","title":"Run as user","text":"<p>The process run inside the container is owned by the USER defined in the container manifest. For security reasons, the platform does not allow containers to be run as root. User can ask for a different, specific user id to be used, by defining the <code>run_as_user</code> field. It accepts an integer value.</p> <pre><code>run_as_user: 1000\n</code></pre>"},{"location":"tasks/kubernetes-resources/#run-as-group","title":"Run as group","text":"<p>The process run inside the container is owned by the GROUP defined in the container manifest. For security reasons, the platform does not allow containers to be run as root. User can ask for a different, specific group id to be used, by defining the <code>run_as_group</code> field.  It accepts an integer value.</p> <pre><code>run_as_group: 1000\n</code></pre>"},{"location":"tasks/metrics/","title":"Tracking and Metrics","text":"<p>Traceability in AI solutions refers to the ability to track and document every stage of a machine learning (ML) system\u2019s lifecycle\u2014from data collection and preprocessing to model training, deployment, and monitoring. It ensures transparency, reproducibility, and accountability by maintaining records of data sources, model versions, parameters, and performance metrics.</p> <p>Model and data metrics are key components of this traceability.</p> <ul> <li> <p>Data metrics assess the quality, balance, and drift of datasets (e.g., missing values, bias, or changes in data distribution).</p> </li> <li> <p>Model metrics evaluate performance, such as accuracy, precision, recall, or fairness indicators, and help compare model versions over time.</p> </li> </ul> <p>In MLOps (Machine Learning Operations), traceability and metrics are essential for maintaining reliable, compliant, and scalable AI systems. They enable teams to audit decisions, reproduce results, detect performance degradation, and ensure models remain aligned with business and ethical requirements throughout their operational lifecycle.</p> <p>Within the platform metrics and trracking are first-class elements and are implemented as follows.</p>"},{"location":"tasks/metrics/#model-and-run-metrics","title":"Model and Run Metrics","text":"<p>First, it is possible to associate quantitiative metrics to the ML models that are being created and logged by the platform. In this way it is possible to associate single-value metric value (e.g., accuracy) or value series (e.g., loss over epochs) to a model. To perform this operation one can use Python SDK and perform <code>log_metrics</code> oepration over model instance (see Python SDK documentation for details).</p> <p>Second, the metrics may be associated to the function runs. Similarly to model metrics, the run metrics may be logged using Python SDK.</p> <p></p> <p>Once reported, the metrics of the entities may be compared (e.g., training over different hyper parameter sets).</p> <p>While it is possible to log arbitrary metrics collected within the execution, in some cases it is possibile to rely on the functionality provided by the ML frameworks.</p> <p>In case of MLFlow model, it is possible to use <code>autolog</code> functionality of the framework to collect and assocaite metrics. For example</p> <pre><code>import mlflow\nfrom digitalhub import from_mlflow_run, get_mlflow_model_metrics\nfrom digitalhub_runtime_python import handler\n\n...\n\n    # Enable MLflow autologging for sklearn\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    # training\n    ...\n\n    # Get MLflow run information\n    run_id = mlflow.last_active_run().info.run_id\n    # Extract MLflow run artifacts and metadata for DigitalHub integration\n    model_params = from_mlflow_run(run_id)\n    metrics = get_mlflow_model_metrics(run_id)\n</code></pre> <p>In case of models built / fine-tuned with HuggingFace Transformers, one can use the callbacks to report the metrics during the execution. For example</p> <pre><code>from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    EarlyStoppingCallback,\n    TrainerCallback\n)\n\nclass LoggingCallback(TrainerCallback):\n\n    def __init__(self, run):\n        self.run = run\n\n    def on_log(self, args, state, control, logs, model=None, **kwargs):\n        metrics = {}\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                metrics[k] = v\n            elif isinstance(v, torch.Tensor) and v.numel() == 1:\n                metrics[k] = v.item()\n            else:\n                logger.warning(\n                    f'Trainer is attempting to log a value of \"{v}\" of type {type(v)} for key \"{k}\" as a metric. '\n                    \"MLflow's log_metric() only accepts float and int types so we dropped this attribute.\"\n                )\n        self.run.log_metrics(metrics)\n\n...\n\n    trainer = SFTTrainer(\n        model=model,\n        processing_class=tokenizer,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        callbacks=callbacks,\n        ...\n</code></pre>"},{"location":"tasks/models/","title":"ML Models","text":"<p>Support for MLOps is one of the key functionalities of the platform. Creating, managing and serving ML models is supported by the platform via ML Model entities and the corresponding functionality for their registration and serving.</p> <p>ML Model entity represent the relevant information about the model - framework and algorithms used to create it, hyperparameters and metrics, necessary artifacts constituting the model, etc. The platform supports a list of standard model kinds as well as generic models. Specifically, it is possible to define models of the following kinds</p> <ul> <li><code>sklearn</code> - ML models created with Scikit-learn framework and packaged as a single artifact.</li> <li><code>mlflow</code> - ML models created with any MLFlow-compatible framework (or <code>flavor</code> in MLFlow terminology) and logged following the MLFlow model format.</li> <li><code>huggingface</code> - LLM created using the HuggingFace framework and format, either standard one or fine-tuned.</li> <li><code>model</code> - generic ML Model with custom packaging and framework.</li> </ul> <p>For the specific ML Model formats the platform provides the support for serving those models as inference API in line with the V2 open inference protocol. These is achieved with the corresponding model serving runtimes.</p>"},{"location":"tasks/models/#management-via-ui","title":"Management via UI","text":"<p>Models can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new model</li> <li><code>expand</code> a model to see its 5 latest versions</li> <li><code>show</code> the details of a model</li> <li><code>edit</code> a model</li> <li><code>delete</code> a model</li> <li><code>filter</code> models by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete models using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/models/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the model</li> <li><code>Kind</code>: kind of the model</li> <li>(Spec) <code>Path</code>: remote path where the model is stored. If you instead upload the model at the bottom of the form, this will be the path to where it will be stored.</li> </ul>"},{"location":"tasks/models/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a model's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/models/#update","title":"Update","text":"<p>You can update a model by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/models/#delete","title":"Delete","text":"<p>You can delete a model from either its detail page or the list of models, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/models/#management-via-sdk","title":"Management via SDK","text":"<p>Models can be created and managed as entities with the SDK CRUD methods. Check the SDK Model documentation for more information.</p>"},{"location":"tasks/pat/","title":"Personal Access Tokens (PATs)","text":"<p>Personal Access Tokens (PATs) are a special type of access token that users can generate directly from the platform\u2019s web console. PATs are designed to simplify user authentication for long-lived or automation-related use cases, offering the same functionality as standard access tokens with additional control features.</p> <p>Key Characteristics:</p> <ul> <li> <p>No Automatic Expiration by Default:     PATs do not expire automatically unless an expiration policy is enforced by platform administrators. This makes them suitable for use cases requiring persistent access without frequent reauthentication.</p> </li> <li> <p>Interoperable with Standard Access Token Workflows:     PATs can be used in any context where a standard access token is accepted, including calling APIs and authenticating with platform services.</p> </li> <li> <p>Supports Token Exchange (Security Token Service - STS):     PATs can be used in token exchange flows to obtain short-lived standard access tokens and associated credentials, aligning with security best practices for downstream service interactions.</p> </li> <li> <p>Named Tokens:     Each PAT must be assigned a human-readable name upon creation. This name helps users identify and manage their tokens more easily in the web console.</p> </li> <li> <p>User-Controlled Lifecycle Management:     PATs can be revoked at any time by their owner via the web console, providing a manual method of invalidating tokens if they are no longer needed or are believed to be compromised.</p> </li> <li> <p>Opaque Format:     PATs are opaque strings, meaning their contents are not encoded in a readable format like JWTs (JSON Web Tokens). Their structure and claims are not introspectable by clients and must be validated by the platform.</p> </li> </ul> <p>Typical Use Cases:</p> <ul> <li> <p>Command-line tools or scripts requiring persistent access without interactive login</p> </li> <li> <p>Integration with CI/CD pipelines</p> </li> <li> <p>Applications that require non-expiring or manually managed credentials</p> </li> <li> <p>Access delegation to long-lived background jobs</p> </li> </ul>"},{"location":"tasks/pat/#security-considerations","title":"Security Considerations","text":"<p>Because PATs typically do not expire automatically and are opaque, they should be treated as sensitive credentials. It is strongly recommended to:</p> <ul> <li> <p>Store PATs securely (e.g., in encrypted secrets managers)</p> </li> <li> <p>Use token exchange to obtain short-lived credentials for actual API calls</p> </li> <li> <p>Regularly review and rotate PATs</p> </li> <li> <p>Revoke unused or compromised tokens immediately</p> </li> </ul>"},{"location":"tasks/pat/#token-management-via-ui","title":"Token management via UI","text":"<p>When logged in the web console, users can create, review and revoke Personal Access Tokens from the user menu, accessible via the top right dropdown with the username. </p> <p></p> <p>Select Configuration to open the user management page and then scroll down to the Personal Access Tokens section.</p> <p></p> <p>When adding a PAT, the form asks for a name and then lets users select which kind of permissions will be given to the newly created token. On creation, the token value will be shown once, and then stored secretly in the platform.</p> <p>Copy the token value and store securely: it won't be readable anymore!</p> <p></p> <p>From now on, the token can be freely used wherever an access token would be required. At any given time, owner can revoke the token from the same page by selecting Delete and confirming the removal.</p>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project represents a data and AI application and is a container for different entities (code, assets, configuration, ...) that form the application. It is the context in which you can run functions and manage models, data, and artifacts. Projects may be created and managed from the UI, but also by using DH Core's API, for example via Python SDK.</p>"},{"location":"tasks/projects/#management-via-ui","title":"Management via UI","text":"<p>In the following sections we document project management via the <code>Core Console</code> UI.</p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A project is created by clicking <code>CREATE A NEW PROJECT</code> in the console's home page.</p> <p></p> <p>A form asking for the project's details is then shown:</p> <p></p> <p>The following parameters are mandatory:</p> <ul> <li><code>name</code>: name of the project, also acts as identifier of the project</li> </ul> <p><code>Metadata</code> parameters are optional and may be changed later:</p> <ul> <li><code>name</code>: name of the project</li> <li><code>description</code>: a human-readable description of the project</li> <li><code>labels</code>: list of labels</li> </ul> <p><code>Save</code> and the project will appear in the home page.</p>"},{"location":"tasks/projects/#read","title":"Read","text":"<p>All projects present in the database are listed in the home page. Each tile shows:</p> <ul> <li>Identifier of the project</li> <li>Name of the project (hidden if same as identifier)</li> <li>Description</li> <li>Author</li> <li>Date of creation</li> <li>Date of last modification</li> </ul> <p></p> <p>Click on the tile to access the project's dashboard:</p> <p></p> <p>This dashboard shows a summary of the resources associated with the project and allows you to access the management of these resources.</p> <ul> <li><code>Jobs and runs</code>: list and status of performed runs</li> <li><code>Functions and code</code>: number and list of latest functions</li> <li><code>Models</code>: number and list of latest models</li> <li><code>Data items</code>: number and list of latest data items</li> <li><code>Artifacts</code>: number and list of latest workflows</li> </ul> <p>You can return to the list of projects at any time by clicking Projects at the bottom of the left menu, or switch directly to a specific project by using the drop-down menu in the upper left of the interface.</p> <p></p>"},{"location":"tasks/projects/#update","title":"Update","text":"<p>To update a project's <code>Metadata</code>, first click <code>Configuration</code> in the left menu.</p> <p></p> <p>Click <code>Edit</code> in the top right and the edit form for <code>Metadata</code> properties will be shown. In the example below, a label was added.</p> <p></p> <p>When you're done updating the project, click Save.</p>"},{"location":"tasks/projects/#share","title":"Share","text":"<p>To allow other users to view and interact with the project, you must share it with them. From the <code>Configuration</code> page, click <code>Share</code> in the upper right.</p> <p></p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>You can delete a project from the <code>Configuration</code> page, by clicking <code>Delete</code>. You will be asked to confirm by entering the project's identifier.</p> <p></p>"},{"location":"tasks/projects/#management-via-sdk","title":"Management via SDK","text":"<p>Projects can be created and managed as entities with the SDK. Check the SDK Project documentation for more information.</p>"},{"location":"tasks/run-resources/","title":"Configuring resource-critical executions","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on the standard Kubernetes functionality and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul> <p>In the platform, these requirements may be defined in two ways.</p> <p>First, it is possible to configure them explictly when defining the function run, either via Core UI or via SDK. Please note that it is possible to describe only some of these properties, leaving the rest blank without constraints. All the defaults are managed by the underlying Kubernetes deployment.</p> <p>Second, it is possible to rely on a set of preconfigured HW profiles defined by the platform deployment. The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p>"},{"location":"tasks/secrets/","title":"Secret Management","text":"<p>Working with different operations may imply the usage of a sensitive values, such as external API credentials, storage credentials, etc.</p> <p>In order to avoid embedding the credentials in the code of functions, the platform supports an explicit management of credentials as secrets. This operation exploits the underlying secret management subsystem, such as Kubernetes Secret Manager.</p> <p>Besides the secrets managed natively by the platform to integrate e.g., default storage credentials, it is possible to define custom secrets at the level of a single project. The project secrets are managed as any other project-related entities, such as functions, dataitems, etc.</p> <p>At the level of the project the secrets are represented as key-value pairs. The management of secrets is delegated to a secret provider, and currently only Kubernetes Secret Manager is supported. Each project has its own Kubernetes secret, where all the key-value pairs are stored.</p> <p>To create a new secret value it is possible to use the Core UI console or directly via API, e.g., using the SDK.</p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-ui","title":"Creating and Managing Secrets via UI","text":"<p>Core console can be used to manage project secrets. To create a new one, it is necessary to provide a secret key and a value to be stored.</p> <p></p> <p>The entries may be then deleted and updated, as well as their metadata.</p>"},{"location":"tasks/secrets/#managing-secrets-via-sdk","title":"Managing Secrets via SDK","text":"<p>Secrets can be created and managed as entities with the SDK CRUD methods. Check the SDK Secrets documentation for more information.</p>"},{"location":"tasks/services/","title":"Services","text":"<p>When executing runs, tasks of type service will result in a deployment with an HTTP service published inside the platform.</p> <p>Those runs will receive a unique, addressable identifier along with an IP address and an internal service URL. The detailed information are stored in the run status field, under the <code>service</code> keyword. Together with the base service url, all well known urls will be registered and exposed to users.</p> <pre><code>service:\n  name: s-containerserve-9271ab33c0364d549c89f8b2834ae141\n  type: ClusterIP\n  clusterIP: 172.16.143.129\n  ports:\n    - name: port5678\n      port: 5678\n      protocol: TCP\n      targetPort: 5678\n  url: s-containerserve-9271ab33c0364d549c89f8b2834ae141:5678\n</code></pre> <p>By calling the URL, users are thus able to invoke their function interactively.</p> <p>The service view will present at any given time the status of active services, i.e. runs with a service exposed. Users can get a quick glimpse of the health of their deployments, with quick actions available.</p> <p></p> <p>As we can see from the screenshot, the current status is highlighted, and any contextual message is directly reported.</p> <p>Important!</p> <p>Do note that service URLs are not exposed outside the perimeter of the platform. Only clients from inside the platform, such as workspaces, other functions and the core console are able to access. In order to expose the service externally an api gateway is required.</p>"},{"location":"tasks/services/#service-invocation","title":"Service invocation","text":"<p>Services which are in an healthy state can be invoked from inside the platform, either manually (by custom code or standard HTTP clients) or via the python SDK <code>invoke</code> method on runs.</p> <pre><code>run = function.run(...)\n# invoke the service only when ready!\n\nresult = run.invoke(...)\n</code></pre> <p>See the SDK Reference documentation for details.</p>"},{"location":"tasks/services/#http-client","title":"HTTP client","text":"<p>The user console implements a basic HTTP client, able to perform operations on textual content (e.g. text/plain, application/json etc).</p> <p>By selecting a valid service run and clicking the CLIENT button, the console will open a simplified dialog as depicted in following figure.</p> <p></p> <p>From the client, users will be able to perform basic operations and visualize the results.</p>"},{"location":"tasks/services/#features","title":"Features","text":"<ul> <li>Supports GET/POST/PUT/DELETE operations</li> <li>Can invoke internal urls leveraging core as gateway</li> <li>Supports custom HTTP headers</li> <li>Full request/response history</li> <li>Basic raw text support</li> <li>Interactive preview for JSON and YAML</li> </ul>"},{"location":"tasks/services/#notes","title":"Notes","text":"<p>The service is not exposed to the user's browser: access is mediated by the console through the core backend. As such, only a limited set of content is allowed: text-based requests and responses with a reasonable size limit.</p>"},{"location":"tasks/triggers/","title":"Triggers and Automation","text":"<p>Triggers define a way for users to describe a set of conditions which should result in the execution of a function's task, i.e. a run.</p> <p>Triggers solve the problem of automatically executing a function when a specific condition is achieved, such as a schedule, an event, a change of status of a persisted entity. Their purpose is to let users define simple automations, which execute a task based on a given variable input, following the  not to implement pipelines.</p> <p>Every trigger must be bound to a specific function's task: when the trigger fires, the result will be an execution of that very specific task, i.e. a run.</p> <p>Triggers are handled by actuators, dedicated, internal components which will process a trigger's definition and then monitor the conditions required for firing the trigger. For example, with a scheduler trigger defining a cron expression, the actuator will fire the trigger every time the expression is satisfied.</p> <p>The platform handles the persistence, consistency and lifecycle of triggers by storing them as entities related to functions.</p> <p>Users can run or stop triggers at will, enabling or disabling the execution.</p>"},{"location":"tasks/triggers/#templates-and-inputs","title":"Templates and inputs","text":"<p>Triggers are designed to produce a function's run every time they are fired. In the platform, a run is a self described, fully enclosed definition of a function's execution. As such, the configuration (context, resources, parameters, inputs, secrets...) is passed as spec for the run. In the trigger flow, the spec is dynamically built by the platform, leveraging the context of the user and the triggers execution. Every parameter meant to go in the run's spec has to be either statically pre-defined or dynamically generated at runtime. </p> <p>The trigger's spec includes a <code>template</code> field which serves as the base used to derive the run's <code>spec</code>: static parameters are copied, while the context and the environment are built by core. This solution can satisfy basic needs, such as \"execute this function every day\", but fails to cover more complex uses, where the function's parameters must be dynamically populated.</p> <pre><code>template:\n  envs:\n    - name: MY_ENV\n      value: VALUE\n  parameters:\n    param1: value123\n  resources:\n    cpu: \"1\"\n    mem: 64Mi\n</code></pre> <p>To solve this need, triggers can produce inputs, based on their context and the specific execution event. These inputs can be consumed by user-defined functions to perform specific, parametrized tasks: for example a validation function could perform a check on a given data table. In order to consume inputs, the function must accept variable parameters.</p> <p>For example, a function could receive a parameter shaped as a core entity as follow:</p> <pre><code>def validate(project, di:DataItem):\n    # download dataitem as local file\n    path = di.download(overwrite=True)\n\n...\n</code></pre> <p>This function could be used via a lifecycle trigger to perform an automation which will validate every data table stored in the project's repository, by consuming the input produced by the trigger as function parameter.</p> <p>Core supports a simple templating language, based on Mustache, which lets developers define placeholders inside the run's template, and have those placeholders inflated with the actual value at runtime.</p> <p>Following the previous example, the template could expose the data item key for the python code to consume by defining the parametrized value in the template:</p> <pre><code>template:\n  inputs:\n    di: \"{{input.key}}\"\n</code></pre> <p>This definition will instruct the templating engine in producing a run with a named input <code>di</code> valorized as the store key of the data item triggering the execution. Every resulting run will then receive a different, specific input enabling the automation to work.</p> <p>The resulting spec for an execution triggered by entity <code>my-test-data</code> will be:</p> <pre><code>spec:\n  inputs:\n    di: store://my-prj/dataitem/table/my-test-data:00...\n</code></pre>"},{"location":"tasks/triggers/#scheduler-trigger","title":"Scheduler trigger","text":"<p>The platform supports a basic, cron-like scheduler which will execute the associated task every time the cron expression is verified.</p> <p>The only configuration parameter is:</p> <ul> <li>schedule: a cron-like expression describing the requested interval or time</li> </ul> <p>For example:</p> <pre><code>spec: \n    schedule: 0 * 0 ? * * * #every minute\n</code></pre> <p>The expression is based on Quartz syntax, see Quartz doc for reference.</p> <p>Additionally, the scheduler supports the <code>@</code> syntax for repeated tasks, supporting:</p> <ul> <li><code>@hourly</code> for tasks repeated every hour</li> <li><code>@daily</code> for tasks repeated every day</li> <li><code>@weekly</code> for tasks repeated every week</li> <li><code>@monthly</code> for tasks repeated every month</li> </ul> <p>Example <pre><code>spec: \n    schedule: '@hourly' #every hour\n</code></pre></p>"},{"location":"tasks/triggers/#lifecycle-trigger","title":"Lifecycle trigger","text":"<p>The lifecycle actuator monitors the lifecycle of entities stored in the platform and can trigger a function's execution on state changes, such as files being uploaded or models being created. This lets users define simple automations for reacting to changes in the platform's store such as validating a content, or deploying a new version of a given model.</p> <p>The template exposes an <code>input</code> object which contains the full definition of the entity triggering the specific event. Developers can consume the input as they see fit, by instrumenting their templates with variables and then consuming the content as parameters for the functions.</p> <p>In order to define which content is relevant for a given trigger, the spec requires the following parameters to be defined:</p> <ul> <li> <p>key: a store key accessor, with wildcard support, describing the content. The accessor can be as specific as a full key, identifying a single version of an item, or be more relaxed to match every version of the same item, all items of the same kind, or all items in the store. </p> </li> <li> <p>states: a list of states (CREATED,UPLOADING,READY,ERROR,DELETED) which will be used to detect an event of interest.</p> </li> </ul> <p>For example, the following will produce a function's run every time a table with the name ending in <code>.csv</code> is uploaded in the store and ready for usage:</p> <pre><code>spec:\n    key: \"store://my-proj/dataitem/table/*.csv\"\n    states:\n    - READY\n    template:\n      inputs:\n        di: \"{{input.key}}\"\n</code></pre> <p>Note: the key must match the current project.</p>"},{"location":"tasks/triggers/#management-via-ui","title":"Management via UI","text":"<p>Triggers can be managed via the user console, by navigating to the function's task of interest and then filling the <code>Create trigger</code> form in every detail.</p> <p></p> <p>Template parameters can be filled in under the Task and Run sections, using variable expansion via <code>{VAR}</code> when needed.</p> <p></p> <p>Afterwards, triggers can be managed from their section as a normal entity.</p> <p></p>"},{"location":"tasks/triggers/#management-via-sdk","title":"Management via SDK","text":"<p>Triggers are managed as entities with the SDK CRUD methods, connected to functions. Check the SDK Functions documentation for more information.</p>"},{"location":"tasks/workflows/","title":"Workflows","text":"<p>Workflows allow for organizing the single operations in advanced management pipelines, to perform a series operation of data processing, ML model training and serving, etc. Workflows represent long-running procedures defined as Directed Acyclic Graphs (DAGs) where each node is a single unit of work performed by the platform (e.g., as a Kubernetes Job).</p> <p>As in case of functions, it is possible for the platform to have different workflow runtimes. Currently, the only workflow runtime implemented is the one based on Hera infrastructure. See Hera Runtime for further details about how the workflow is defined and executed with the Hera component of the platform.</p> <p>Similarly to functions, workflows may be managed via console UI or via Python SDK.</p>"},{"location":"tasks/workflows/#management-via-ui","title":"Management via UI","text":"<p>Workflows can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new workflow</li> <li><code>expand</code> a workflow to see its 5 latest versions</li> <li><code>show</code> the details of a workflow</li> <li><code>edit</code> a workflow</li> <li><code>delete</code> a workflow</li> <li><code>filter</code> workflows by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete workflows using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/workflows/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a two-step form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the workflow</li> <li><code>Kind</code>: kind of workflow</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Audit</code>: author of creation and modification</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Relationships</code>: relationships with other entities</li> <li><code>Versioning</code>: version of the function</li> </ul> <p>In case of a <code>hera</code> workflow, the source code and handler fields are required as well.</p>"},{"location":"tasks/workflows/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a workflow's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p> <p>In case of <code>hera</code> workflows, the executions of the workflow instances can be monitored with the corresponding DAG viewer.</p> <p></p>"},{"location":"tasks/workflows/#update","title":"Update","text":"<p>You can update a workflow by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/workflows/#delete","title":"Delete","text":"<p>You can delete a workflow from either its detail page or the list of workflows, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/workflows/#management-via-sdk","title":"Management via SDK","text":"<p>Workflows can be created and managed as entities with the SDK CRUD methods. Check the SDK Workflows documentation for more information.</p>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p> <p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>Let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}