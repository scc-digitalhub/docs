{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Platform Administration Overview","text":"<p>BREAKING CHANGES: consult the section Upgrade notes for release 0.14 before upgrading your environment.</p> <p>When deploying in a production environment, the platform setup requires additional steps for its secure and efficient use. In this section we take into account aspects that are required for the platform setup. </p> <ul> <li>Authentication and Access Control</li> <li>Platform Configuration</li> </ul>"},{"location":"authentication/","title":"Platform Authentication and Access Control","text":"<p>The DigitalHub Platform supports authentication with an external provider.</p> <p>It is mandatory to set custom values for the platform, so Helm knowledge is required.</p> <p>This section will show how to set an authentication for the following:</p> <ul> <li>Coder</li> <li>Core</li> <li>Dashboard</li> <li>Kubernetes Resource Manager</li> </ul>"},{"location":"configuration/","title":"Platform Configuration","text":"<p>The DigitalHub Platform provides configuration options in the DigitalHub values.yaml file.</p> <p>The safest way to set up your custom values is to use a values file in which you will set up the options you are interested in.</p> <p>Thanks to the Helm hereditary properties, the platform values will change taking the values of your custom file, preserving the integrity of the originals and allowing you to use a shorter set of customized values.</p> <p>You can use a custom set of values from a file like the example below, in which we install digitalhub with custom values: <pre><code>helm upgrade -n &lt;YOUR_NAMESPACE&gt; &lt;YOUR_RELEASE&gt; digitalhub/digitalhub --install --create-namespace --timeout 45m0s --values &lt;YOUR_VALUES_FILE_PATH&gt;\n</code></pre> In this example, <code>--set global.registry.url=\"MINIKUBE_IP_ADDRESS\"</code> and <code>--set global.externalHostAddress=\"MINIKUBE_IP_ADDRESS\"</code> are not specified in the installation command, but they can be specified in your values file:</p> <pre><code>global:\n  registry:\n    url: \"YOUR_ADDRESS\"\n  externalHostAddress: &amp;globalExternalUrl \"YOUR_ADDRESS\"\n</code></pre>"},{"location":"ingress/","title":"Ingress configuration","text":"<p>The services of the platform can be exposed with Ingress by editing your values file.</p> <p>For every exposable component, you will find a value field for the ingress, set by default to enabled: false.</p> <p>After setting enabled to true to activate the Ingress creation, check the component's values.yaml file to see how you should structure your custom values file and set all the neeeded Ingress values.</p> <p>The example below is for the Core Ingress:</p> <pre><code>ingress:\n  enabled: true\n  className: \"youringressclass\"\n  hosts:\n    - host: your.host\n      paths: \n        - pathType: ImplementationSpecific\n          path: /\n  tls:\n  - secretName: yourTlsSecret\n</code></pre>"},{"location":"installation/","title":"Installation on cluster","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>A configured image registry</li> <li>A configured DNS</li> <li>Domains and Ingress Controller for service exposition</li> <li>A configured OAuth provider to enable authentication</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>Once you have set your custom values.yaml file, DigitalHub can be installed as follows:</p> <p>1) Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></p> <p>2) Install DigitalHub with Helm and your custom values.</p> <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --values PATH_TO_YOUR_VALUES_FILE --timeout 45m0s\n</code></pre> <p>5) Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></p> <p>Once installed, you should see the references (URLs) for the different tools of the platform, similar to the example below: <pre><code>##########################################################\n#   _____   _       _           _ _     _       _        #\n#  (____ \\ (_)     (_)_        | | |   | |     | |       #\n#   _   \\ \\ _  ____ _| |_  ____| | |__ | |_   _| | _     #\n#  | |   | | |/ _  | |  _)/ _  | |  __)| | | | | || \\    #\n#  | |__/ /| ( ( | | | |_( ( | | | |   | | |_| | |_) )   #\n#  |_____/ |_|\\_|| |_|\\___)_||_|_|_|   |_|\\____|____/    #\n#            (_____|                                     #\n#                                                        #\n##########################################################\n\nDigitalhub has been installed. Check its status by running:\n  kubectl --namespace digitalhub get pods\n\nDigitalhub componet URLs:\n  - Dashboard: http://192.168.76.2:30110\n  - Jupyter: http://192.168.76.2:30040 (Create jupyter workspace from template in the coder dashboard before use)\n  - Dremio: http://192.168.76.2:30120 (Create dremio workspace from template in the coder dashboard before use)\n  - Sqlpad: http://192.168.76.2:30140 (Create sqlpad workspace from template in the coder dashboard before use)\n  - Grafana: http://192.168.76.2:30130 (Create grafana workspace from template in the coder dashboard before use)\n  - Docker Registry: http://192.168.76.2:30150\n  - Coder: http://192.168.76.2:30170 (Username: test@digitalhub.test Password: Test12456@!)\n  - Core: http://192.168.76.2:30180\n  - Kubernetes Resource Manager: http://192.168.76.2:30160\n</code></pre></p>"},{"location":"priorityclass/","title":"Priority classes","text":"<p>The Platform's chart provides configuration options for the priority classes of the components provided.</p> <p>You can learn more about Priority and Priority Classes on the official Kubernetes documentation. </p> <p>Keep in mind that priority classes are cluster resources and will affect your whole environment, so previous knowledge of the resource is strictly required before using it.</p> <p>With the configuration options provided, you can:</p> <ul> <li>Set a priority to the components</li> <li>Let the platform chart create the Priority Classes and assign them to the components</li> <li>Use custom Priority Classes that you already defined in your environment</li> </ul> <p>If you enable priority through the Values file, you can specify two priority classes, a high priority one and a low priority:</p> <pre><code>global:\n  #  global.priority -- Priority class configuration\n  priority:\n    #  global.priority.enabled -- Enable/Disable priority classes\n    enabled: true\n    #  global.priority.highPriority -- High priority class configuration\n    highPriority:\n      #  global.priority.highPriority.className -- Name of the high priority class\n      className: &amp;highPriority \"HIGH_PRIORITY_CLASS\"\n      #  global.priority.highPriority.existingClass -- Set this to true if you have an existing priority class with the name specified in className\n      existingClass: false\n      #  global.priority.highPriority.value -- Value of the high priority class created by the chart\n      value: 1000\n    #  global.priority.lowPriority -- Low priority class configuration\n    lowPriority:\n      #  global.priority.lowPriority.className -- Name of the low priority class\n      className: &amp;lowPriority \"LOW_PRIORITY_CLASS\"\n      #  global.priority.lowPriority.existingClass -- Set this to true if you have an existing priority class with the name specified in className\n      existingClass: false\n      #  global.priority.lowPriority.value -- Value of the low priority class created by the chart\n      value: 100\n</code></pre> <p>If <code>global.priority.highPriority/lowPriority.existingClass</code> is set to <code>false</code>, the chart will create these priority classes for you, using <code>className</code> and setting the priority as of <code>value</code>. Vice versa, if <code>global.priority.highPriority/lowPriority.existingClass</code> is set to <code>true</code>, the priority classes will not be created and the ones with the same name as <code>className</code> present on your environment will be used.</p> <p>By default, these two classes use an anchor to assign the classes to the components quickly through the Values file, so check all the components before installing or upgrading the Platform.</p> <p>You are not limited to using only a high priority and a low priority class, you can use as many as you like as long as you create them yourself and specify them in the values.</p> <p>IMPORTANT: The pods generated by the users like Coder workspaces, Core runs and models, have a priority set to the default one present in your environment. Please, keep this in mind when you are planning your hierarchy!</p> <p>Currently, priority classes are supported by the following components with the respective anchor assigned by default:</p> <p>High Priority:</p> <ul> <li>Argo Workflows</li> <li>Core</li> <li>STS (Core)</li> <li>Docker Registry</li> </ul> <p>Low Priority:</p> <ul> <li>API Gateway Operator</li> <li>Dashboard</li> <li>Dremio Rest Server Operator</li> <li>External Postgres Operator</li> <li>Kubernetes Resource Manager</li> <li>Open Web UI</li> <li>PostgRest Operator</li> </ul> <p>To change the priority of a specific component, edit the value of the respective <code>priorityClassName</code> field.</p> <p>For example:</p> <pre><code>core:\n  #  core.priorityClassName -- Name of the priority class to use for the core pods. If not set, no priority class will be used.\n  priorityClassName: \"NAME_OF_YOUR_PRIORITY_CLASS\"\n</code></pre>"},{"location":"upgrading/","title":"Upgrading DigitalHub","text":""},{"location":"upgrading/#upgrade-notes-for-release-014","title":"Upgrade notes for release 0.14","text":"<p>Change of definition for Core runs</p> <p>The definition for Core runs in the database has changed, so it becomes necessary to finish or stop all the current runs before upgrading to the new version.</p> <p>Change of format for Core secrets</p> <p>Core no longer accepts secrets created with a double <code>-</code> in it's name.</p> <p>If you have an ongoing project that still needs your secrets, recreate them with the correct syntax.</p> <p>For example, <code>proj-secrets--test</code> must become <code>proj-secrets--test</code>.</p>"},{"location":"upgrading/#upgrade-procedure","title":"Upgrade procedure","text":"<p>Once the platform is installed, you may find yourself in need of tweaking it and upgrading it.</p> <p>With the command <code>helm upgrade</code> you will be able to change the values of the platform with your custom ones like the example below:</p> <pre><code>helm upgrade -n &lt;NAMESPACE&gt; &lt;RELEASE&gt; digitalhub/digitalhub --timeout 30m0s --values &lt;YOUR_VALUES_FILE_PATH&gt;\n</code></pre> <p>Upgrading Coder templates</p> <p>If you wish to upgrade the Coder templates, you can do so.</p> <p>You can find them in <code>digitalhub/charts/digitalhub/confs/coder</code>.</p> <p>However, it is mandatory to create and set your Coder access token in the values file.</p> <pre><code>coder:\n  template:\n    upgrade:\n      # Set it to true if you want to upgrade the Coder templates.\n      enabled: false\n      # In order to upgrade the templates, you will need to create and set here a Coder Token.\n      token: \"\"\n</code></pre>"},{"location":"authentication/coder/","title":"Coder","text":"<p>To enable the authentication with a provider for Coder, please consult the official Coder documentation.</p> <p>In your provider, the redirect url should correspond to <code>https://yourcoderurl/api/v2/users/oidc/callback</code>.</p>"},{"location":"authentication/core/","title":"Core","text":"<p>To enable the authentication with a provider for Core, you will need to set the values in the file Values.yaml of the chart digitalhub in the Core section.</p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>core:\n  authentication:\n    openId:\n      enabled: true\n      issuerUri: \"https://yourproviderurl\" # Set the issuer url of your provider\n      jwtAudience: \"\" # Set the audience\n      jwtClaim: \"\" # Set the claims\n      oidcClientId: \"\" # Use this if you want to hardcode your clientID\n      scope: \"\" # Specify the scopes\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n</code></pre> <p>In your provider, the redirect url should correspond to <code>https://yourcoreurl/console/auth-callback</code>.</p>"},{"location":"authentication/dashboard/","title":"Dashboard","text":"<p>To enable the authentication with a provider for the Dashboard, you will need to set the values in the file Values.yaml of the chart digitalhub in the Dashboard section.</p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>dashboard:\n  oidc:\n    enabled: true\n    audience:\n      clientId: \"\" # Use this if you want to hardcode your clientID\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n    config:\n      issuer: \"https://yourproviderurl\" # Set the issuer url of your provider\n</code></pre>"},{"location":"authentication/krm/","title":"Kubernetes Resource Manager","text":"<p>To enable the authentication with a provider for Kubernetes Resource Manager, you will need to set the values in the file Values.yaml of the chart digitalhub in the Kubernetes Resource Manager section. </p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>kubernetes-resource-manager:\n  oidc:\n    enabled: true\n    audience:\n      clientId: \"\" # Use this if you want to hardcode your clientID\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n    issuer: \"https://yourproviderurl\" # Set the issuer url of your provider\n    scope: \"\" # Set the scopes\n    authType: \"\" # Set the type of authentication\n</code></pre> <p>In your provider, the redirect url should correspond to <code>https://yourkubernetesresourcemanagerurl/console/auth-callback</code>.</p>"},{"location":"charts/core/keystore/","title":"Keystore","text":"<p>To set up a Keystore for Core, add the following section to your <code>values.yaml</code> file and configure the following fields:</p> <pre><code>core:\n  keystore:\n    existingSecret:\n      secretName: \"keystore-secret\" # Name of the secret containing the keystore\n      keyName: \"keystore.jwks\"    # Name of the key in your keystore secret, should correspond to the keystore file name\n    keystoreKid: \"\"  # Specify the key that the keystore should pick\n    keystorePath: \"/etc/keystore\" # Path where your keystore will be saved\n</code></pre> <p>In this example, a Keystore will be created in the path <code>/etc/keystore/keystore.jwks</code> from a secret called <code>keystore-secret</code>. The key of the secret, <code>keystore.jwks</code>, must contain the base64 encoded keystore.</p>"},{"location":"charts/core/lucene/","title":"Lucene","text":"<p>Lucene is the integrated and default option that Core gives you for performing indexing operations.</p> <p>Lucene is included in Core without the need to configure an external component, all you have to do is set the right values to activate it.</p> <p>Lucene will perform all operations in the directory specified by the value <code>core.lucene.indexPath</code> (defaults to <code>/lucene/</code>); setting this value to <code>false</code> will deactivate the tool entirely.</p> <p>Persistence options will allow you to persist data in a Persistent Volume Claim created for you by the Platform chart. Keep in mind that using <code>ReadWriteOnce</code> or <code>ReadWriteOncePod</code> as Access Mode will set Core's Deployment Strategy Type to <code>Recreate</code>.</p> <p>If you decide to persist your data, once the first indexing operation (during Core's application startup) is done, you can turn <code>core.lucene.reindex</code> value to <code>never</code>.</p> <p>On the opposite, should you disable persistance, it is strongly reccomended to leave the option to <code>always</code>: not doing so will make you lose your Lucene data in case of a restart/crash of the Core Pod.</p> <p>WARNING: Lucene's PVC is provided by the Platform Chart; if you uninstall the release, the PVC will be destroyed, so keep that in mind.</p> <p>Below is an example of Lucene's configuration through the values.yaml file:</p> <pre><code>core:\n  lucene:\n    #  core.lucene.indexPath -- Set the path for Lucene and enables it\n    indexPath: \"/lucene/\"\n    #  core.lucene.persistence -- Lucene persistence configuration\n    persistence:\n      #  core.lucene.persistence.enabled -- Enable persistence for Lucene\n      enabled: true\n      #  core.lucene.persistence.accessMode -- Access mode for the Lucene persistent volume claim\n      accessMode: ReadWriteOnce\n      #  core.lucene.persistence.size -- Size for the Lucene persistent volume claim\n      size: 10Gi\n      #  core.lucene.persistence.storageClass -- Storage class for the Lucene persistent volume claim; if not specified, the default class will be used\n      storageClass: \"\"\n    #  core.lucene.reindex -- Reindex of Lucene\n    reindex: always\n</code></pre>"},{"location":"charts/core/solr/","title":"Solr","text":"<p>Core supports Solr integration for indexing operations.</p> <p>The platform itself does not include Solr, so you will need to set it up by yourself in your environment.</p> <p>Keep in mind that enabling Solr will disable Lucene automatically.</p> <p>Make sure to already create the collection you want to use for Core in your instance.</p>"},{"location":"charts/core/solr/#using-solr-without-authentication","title":"Using Solr without authentication","text":"<p>If you are using a Solr deployment without Basic Authentication enabled, set the following values:</p> <pre><code>core:\n  #  core.solr -- Solr configuration\n  solr:\n    #  core.solr.enabled -- Set this value to true if you want to use Core with an existing Solr instance\n    enabled: true\n    #  core.solr.collection -- Solr collection configuration\n    collection:\n      #  core.solr.collection.name -- Name of the Solr collection\n      name: \"COLLECTION_NAME\"\n    #  core.solr.url -- URL of your Solr instance\n    url: \"SOLR_INSTANCE_URL\"\n</code></pre>"},{"location":"charts/core/solr/#using-solr-with-basic-auth","title":"Using Solr with Basic Auth","text":"<p>If you enabled authentication on your Solr instance you will still need to use the values mentioned in the previous section but you will need to set up some other things.</p> <p>You have two options for the configuration:</p> <ul> <li>Specify just the admin user</li> <li>Specify both admin user and a normal user with permissions to operate on that specific collection</li> </ul> <p>You will have to create a secret containing username and password for each user that you set in the values file.</p> <p>The configuration for the values section can be done like this:</p> <pre><code>core:\n  #  solr -- Solr configuration\n  solr:\n    #  solr.enabled -- Set this value to true if you want to use Core with an existing Solr instance\n    enabled: true\n    #  solr.basicAuth -- Basic Auth configuration of Solr\n    basicAuth:\n      #  solr.basicAuth.enabled -- Set this value to true if you use BasicAuth in your Solr instance\n      enabled: true\n      #  solr.credentials -- Solr credentials configuration\n      credentials:\n        # solr.basicAuth.credentials.existingSecrets -- Existing secrets for Solr Basic Auth configuration\n        existingSecrets:\n          # solr.basicAuth.credentials.existingSecrets.admin -- Existing secret for Solr Basic Auth admin user\n          admin:\n            #  solr.basicAuth.credentials.existingSecrets.admin.passwordKey -- Password key\n            passwordKey: \"PASSWORD_KEY\"\n            #  solr.basicAuth.credentials.existingSecrets.admin.secretName -- Secret name\n            secretName: \"SECRET_NAME\"\n            #  solr.basicAuth.credentials.existingSecrets.admin.usernameKey -- Username key\n            usernameKey: \"USER_KEY\"\n          #  solr.basicAuth.credentials.existingSecrets.user -- Existing secret for Solr Basic Auth user\n          user:\n            #  solr.basicAuth.credentials.existingSecrets.user.passwordKey -- Password key\n            passwordKey: \"PASSWORD_KEY\"\n            #  solr.basicAuth.credentials.existingSecrets.user.secretName -- Secret name\n            secretName: \"SECRET_NAME\"\n            #  solr.basicAuth.credentials.existingSecrets.user.usernameKey -- Username key\n            usernameKey: \"USER_KEY\"\n    #  solr.collection -- Solr collection configuration\n    collection:\n      #  solr.collection.name -- Name of the Solr collection\n      name: \"COLLECTION_NAME\"\n    #  solr.url -- URL of your Solr instance\n    url: \"SOLR_INSTANCE_URL\"\n</code></pre>"},{"location":"charts/core/sts/","title":"STS","text":"<p>WARNING: this feature cannot be used locally as it depends on an Authentication Provider that should be installed in your environment.</p> <p>STS allows you to work with temporary credentials to do operations with a Postgres database, avoiding the use of persistent ones and reducing the risk of a security breach.</p> <p>To activate STS, set <code>core.sts.enabled</code> to <code>true</code>.</p> <p>There are a lot of values to cover, so the example will be divided in three parts.</p>"},{"location":"charts/core/sts/#setting-sts-clientid-and-cliensecret","title":"Setting STS clientId and clienSecret","text":"<p>As a first step, set ClientID and ClientSecret for STS. You can either specify them hardcoded or via secret (better choice for production environments).</p> <pre><code>core:\n  sts:\n    #  sts.enabled -- Enable/Disable STS component for dynamic credentials\n    enabled: false\n    #  sts.client --\n    client:\n      #  sts.client.clientId -- ClientID used by STS\n      clientId: \"\"\n      #  sts.client.clientSecret -- ClientSecret used by STS\n      clientSecret: \"\"\n      #  sts.client.existingSecret --\n      existingSecret:\n        #  sts.client.existingSecret.clientIdKey -- Key corresponding to the STS ClientID\n        clientIdKey: \"CLIENTID\"\n        #  sts.client.existingSecret.clientSecretKey -- Key corresponding to the STS ClientSecret\n        clientSecretKey: \"CLIENTSECRET\"\n        #  sts.client.existingSecret.name -- Name of the secret containing STS ClientID and ClientSecret\n        name: \"YOUR_STS_SECRET\"\n</code></pre>"},{"location":"charts/core/sts/#configuring-sts-database","title":"Configuring STS database","text":"<p>STS itself needs it's own database so you'll need to set the connection with it as well.</p> <p>Here is an example configuration:</p> <pre><code>core:\n  sts:\n    #  core.sts.stsDb -- Values of the STS database\n    stsDb:\n      #  core.sts.stsDb.credentials -- Credentials of the STS database\n      credentials:\n        #  core.sts.stsDb.credentials.existingSecret -- Reference to the secret containing username and password of the STS database user.\n        #  These values have higher priority than the explicit declarations.\n        existingSecret:\n          #  core.sts.stsDb.credentials.existingSecret.name -- Name of the secret containing username and password of the STS database user\n          name: \"YOUR_STS_DB_OWNER\"\n          #  core.sts.stsDb.credentials.existingSecret.passwordKey -- Key corresponding to the STS database user password\n          passwordKey: \"OWNER_PASSWORD_KEY\"\n          #  core.sts.stsDb.credentials.existingSecret.usernameKey -- Key corresponding to the STS database user username\n          usernameKey: \"OWNER_USERNAME_KEY\"\n        #  core.sts.stsDb.credentials.password -- Explicit declaration of the STS database user password.\n        #  It has lower priority than the corresponding secret values.\n        password: \"\"\n        #  core.sts.stsDb.credentials.username -- Explicit declaration of the STS database user username.\n        #  It has lower priority than the corresponding secret values.\n        username: \"\"\n      #  core.sts.stsDb.database -- Name of the STS database\n      database: \"STS_DATABASE_NAME\"\n      #  core.sts.stsDb.driver -- Driver used by the STS database\n      driver: \"STS_DB_DRIVER\"\n      #  core.sts.stsDb.host -- Host of the STS database\n      host: \"STS_DATABASE_HOSTNAME\"\n      #  core.sts.stsDb.platform -- Which kind of database you are using for STS (For example, postgresql)\n      platform: \"postgresql\"\n      #  core.sts.stsDb.port -- STS Database port\n      port: \"5432\"\n      #  core.sts.stsDb.schema -- STS database schema\n      schema: \"public\"\n</code></pre>"},{"location":"charts/core/sts/#configuring-the-database-that-will-use-the-temporary-credentials","title":"Configuring the Database that will use the temporary credentials","text":"<p>Now that you have configured the connection with the STS database, all that's left is configuring the connection with the Platform's main database:</p> <pre><code>core:\n  sts:\n    #  sts.credentials --\n    credentials:\n      #  sts.credentials.roles -- Roles that will be mapped to the user for Database operations.\n      #  Must correspond to the owner user of the Platform's main database.\n      roles: \"OWNER_USER\"\n    #  sts.databaseProvider -- Values of the Platform's main database\n    databaseProvider:\n      #  sts.databaseProvider.enabled -- Enable/Disable dynamic credentials for Postgres operations.\n      enabled: true\n      #  sts.databaseProvider.credentials -- Credentials of the Platform's main database\n      credentials:\n        #  sts.databaseProvider.credentials.existingSecret -- Reference to the secret containing username and password of the Platform's main database owner user.\n        #  These values have higher priority than the explicit declarations.\n        existingSecret:\n          #  sts.databaseProvider.credentials.existingSecret.name -- Name of the secret containing username and password of the Platform's main database owner user\n          name: \"OWNER_SECRET\"\n          #  sts.databaseProvider.credentials.existingSecret.passwordKey -- Key corresponding to the Platform's main database owner user password\n          passwordKey: \"OWNER_PWD\"\n          #  sts.databaseProvider.credentials.existingSecret.usernameKey -- Key corresponding to the Platform's main database owner user username\n          usernameKey: \"OWNER_USERNAME\"\n        #  sts.databaseProvider.credentials.password -- Explicit declaration of the Platform's main database owner user password.\n        #  It has lower priority than the corresponding secret values.\n        password: \"\"\n        #  sts.databaseProvider.credentials.username -- Explicit declaration of the Platform's main database owner user username.\n        #  It has lower priority than the corresponding secret values.\n        username: \"\"\n    #  sts.jwt --\n    jwt:\n      #  sts.jwt.issuerUri -- URL of the JWT issuer.\n      issuerUri: ISSUER_URL\n</code></pre>"},{"location":"charts/core/templates/","title":"Core templates","text":""},{"location":"charts/core/templates/#what-are-core-templates","title":"What are Core templates?","text":"<p>You can create and use custom Core templates for your use cases.</p> <p>With this feature, you will be able to customize the resources generated by Core to achieve your needs, creating some baselines that the users can use for their projects. For example, you could:</p> <ul> <li>Set custom requests and limits</li> <li>Harden the securityContext</li> <li>Add tolerations for GPU usage</li> </ul> <p>Templates can be used as profiles when creating pods, deployments, services and secrets with Core.</p>"},{"location":"charts/core/templates/#create-a-template","title":"Create a template","text":"<p>In order to be used, the templates must be specified in a ConfigMap in the form of values of a key, while the key should be the name of the template file that will be mounted in Core.</p> <p>Be aware that you must create and apply the ConfigMap yourself.</p> <p>You can specify multiple templates in the form of key: value (filename: template) in the same ConfigMap.</p> <p>The following is an example of a template for Core:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: core-templates\ndata:\n  template-example.yaml: |-   # Name of the template file\n    id: template1   \n    name: template1\n    description: Lorem ipsum dolor sit amet, consectetur adipiscing elit. In imperdiet lectus arcu, eget mattis dui varius vitae. Morbi lorem augue, volutpat nec mi sagittis, vulputate congue ligula. Maecenas ac luctus mauris. Ut maximus est convallis nisi porta, vel sodales lorem dictum. Praesent ullamcorper enim accumsan diam pharetra feugiat. Integer maximus tortor et nulla fermentum commodo. In vitae massa nec leo fermentum interdum. Integer consectetur dolor vitae accumsan vulputate. Curabitur placerat suscipit justo tempor placerat. Nam euismod suscipit ante non sollicitudin. Integer at cursus sem.\n    runtimeClass: class1\n    priorityClass: class1\n    envs:\n      - name: ENV1\n        value: VALU123123\n      - name: ENV2\n        value: VALU123123\n    resources:\n      requests:\n        cpu: \"1\"\n        memory: \"1Gi\"\n        nvidia.com/gpu: \"1\"\n        ephemeral-storage: \"10Gi\"\n      limits:\n        cpu: \"5\"\n        memory: \"40Gi\"\n        nvidia.com/gpu: \"1\"\n        ephemeral-storage: \"60Gi\"\n    ---\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: pi\n    spec:\n      template:\n        spec:\n          containers:\n            - name: pi\n              image: perl:5.34.0\n              command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          restartPolicy: Never\n      backoffLimit: 4\n</code></pre> <p><code>template-example.yaml</code> is the key, and it is the name of the template file that will be mounted in Core. The actual template is the value of <code>template-example.yaml</code>, so, in this case:</p> <pre><code>id: template1   \nname: template1\ndescription: Lorem ipsum dolor sit amet, consectetur adipiscing elit. In imperdiet lectus arcu, eget mattis dui varius vitae. Morbi lorem augue, volutpat nec mi sagittis, vulputate congue ligula. Maecenas ac luctus mauris. Ut maximus est convallis nisi porta, vel sodales lorem dictum. Praesent ullamcorper enim accumsan diam pharetra feugiat. Integer maximus tortor et nulla fermentum commodo. In vitae massa nec leo fermentum interdum. Integer consectetur dolor vitae accumsan vulputate. Curabitur placerat suscipit justo tempor placerat. Nam euismod suscipit ante non sollicitudin. Integer at cursus sem.\nruntimeClass: class1\npriorityClass: class1\nenvs:\n  - name: ENV1\n    value: VALU123123\n  - name: ENV2\n    value: VALU123123\nresources:\n  requests:\n    cpu: \"1\"\n    memory: \"1Gi\"\n    nvidia.com/gpu: \"1\"\n    ephemeral-storage: \"10Gi\"\n  limits:\n    cpu: \"5\"\n    memory: \"40Gi\"\n    nvidia.com/gpu: \"1\"\n    ephemeral-storage: \"60Gi\"  \n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl:5.34.0\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>As you can see, the template in the above example is composed of two parts:</p> <ul> <li>The upper part (that will be refered to as Profile)</li> <li>The bottom part (that will be refered to as Base Template)</li> </ul> <p>The template merge process is mainly done in three steps:</p> <p>1) The Base Template is used as the starting point for the resource (in this case, a Job). This part can be modified by the user.</p> <p>2) Core appends it's own configuration to the resource</p> <p>3) The Profile is applied, adding the specified fields to the resource; as a result, the job will have the runtimeClass, priorityClass and two variables specified in the Profile. This part cannot be modified by the user.</p> <p>If you wish so, you can also decide to use a Profile without the Base Template if you want to set just certain values (for example, resources or tolerations).</p>"},{"location":"charts/core/templates/#setting-templates-in-the-valuesyaml-file","title":"Setting templates in the Values.yaml file","text":"<p>The following is a reference for the configuration of Core Templates in the Values.yaml file:</p> <pre><code>core:\n  templates:   # Every item in the list must match a template you have created.\n    - name: template-example\n      path: /templates/template-example.yaml\n  volumeMounts:\n    - name:  templates\n      mountPath:  /templates   # Directory in Core in which the template files will be saved\n  volumes:\n    - name:  templates   # Volume for the templates\n      configMap:\n        name: core-templates   # Must match the ConfigMap containing the templates\n</code></pre> <p>Now you can create resources with your template and profile from Core.</p>"},{"location":"charts/core/templates/#supported-fields","title":"Supported fields","text":"<p>The following is a list of all the fields that you can set in the Profile section. You can find an in depth explanation of how to set these fields in the documentation about Kubernetes Resources</p> Field Format envs List secrets List resources Object volumes List nodeSelector List affinity Object tolerations List runtimeClass String priorityClass String imagePullPolicy String"},{"location":"charts/krm/resources/","title":"Resource Management with KRM","text":"<p>Different platform entities are associated with and represented as Kubernetes resources: they are deployed as services, user volumes and secrets, captured as Custom Resources, etc. Kubernetes Resource Manager (KRM) component allows for performing various operations over these resources depending on their kind.</p> <p></p> <p>KRM navigation menu provides access to different types of resources. This includes both standard resources (Services, Deployments, Persistent Volume Claims, Secrets) and custom resources based on Custom Resource Definitions currently installed on the platform. Some custom resources are managed with the customized UI (e.g., PostgreSQL instances, PostgREST Data services o Dremio Data service), while the others may be managed with the standard UI based on their JSON schema.</p>"},{"location":"charts/krm/resources/#management-of-standard-kubernetes-resources","title":"Management of Standard Kubernetes Resources","text":"<p>KRM allows for accessing and managing the standard K8S resources relevant for the DigitalHub platform: space (through Persistent Volume Claims), services and deployments, and secrets.</p>"},{"location":"charts/krm/resources/#listing-k8s-services","title":"Listing K8S Services","text":"<p>Accessing the <code>Services</code> menu of the KRM, it is possible to list the (subset of) services deployed on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each service KRM shows its name, type (e.g., Coder workspace type), exposed port type and value. In the service details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"charts/krm/resources/#listing-k8s-deployments","title":"Listing K8S Deployments","text":"<p>Accessing the <code>Deployments</code> menu of the KRM, it is possible to list the (subset of) deployments on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each deployment KRM shows its name and availability of instances. In the deployment details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"charts/krm/resources/#managing-persistent-volume-claims","title":"Managing Persistent Volume Claims","text":"<p>In certain cases, the operations developed with the platform may require more substantial disk space, e.g., for training / producing significant amounts of data. In this case, it is possible to attach to the tasks the corresponding Persistent Volume Claim (PVC) references. To create a new PVC for the use of the pipeline or Job, KRM provides the corresponding interface.</p> <p>Accessing <code>Persistent Volume Claims</code> menu, it is possible to list and manage the PVCs of the platform.</p> <p></p> <p>For each PVC, you can see the status (Pending or Bound) of the PVC, the name of the volume (if specified), the storage class and the size in Gi. The details view provides further metadata regarding the PVC.</p> <p>It is also possible to delete the PVC and create new ones.</p> <p>Deleting PVC</p> <p>Please note that deleting a PVC bound to a Pod or a Job may affect negatively their execution.</p> <p>To create a new PVC, provide the following:</p> <ul> <li>Name: Name of the resource</li> <li>Space: Disk space request</li> <li>Storage class name: Storage class name (select one of the available in your deployment)</li> <li>Volume: Name of the volume (Important! Can be specified only if a volume already exists, otherwise, the PVC cannot be bound.)</li> <li>Access modes: Access modes (standard K8S values)</li> <li>Mode: PVC mode (Filesystem or Block)</li> </ul> <p></p>"},{"location":"charts/krm/resources/#listing-k8s-secrets","title":"Listing K8S Secrets","text":"<p>Accessing the <code>Secrets</code> menu of the KRM, it is possible to list the (subset of) secrets on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each secret KRM shows its name, type, and number of elements. In the secret details view it is possible to access other metadata and also a list of secret elements. The values are not available directly; to retrieve the actual value of the secret element, use <code>Decode</code> button that will copy the decoded content of the secret into Clipboard.</p> <p></p>"},{"location":"charts/krm/resources/#managing-custom-resources","title":"Managing Custom Resources","text":"<p>KRM allows for the management of generic CRs as well as for the management of some predefined ones, such as PostgreSQL instances, PostgREST and Dremio Data services.</p>"},{"location":"charts/krm/resources/#managing-postgresql-instances-with-krm","title":"Managing PostgreSQL instances with KRM","text":"<p>Using PostgreSQL operator (https://github.com/movetokube/postgres-operator) it is possible to create new DB instances and the DB users to organize the data storage.</p> <p>Accessing <code>Postgres DBs</code> menu of the KRM, it is possible to list, create, and delete PostgreSQL databases.</p> <p></p> <p>To create a new Database, provide the following:</p> <ul> <li>name of the database to create</li> <li>whether to drop the DB on resource deletion</li> <li>Comma-separated list of PostgreSQL extensions to enable (e.g., timescale and/or postgis) as supported by the platform deployment (optional).</li> <li>Comma-separated list of schemas to create in DB (optional)</li> <li>Name of the master role for the DB access management (optional)</li> </ul> <p></p> <p>In the Database details view it is possible also to configure the DB users that can access and operate the Database (create, edit, view, delete). To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the user to be created</li> <li>access privileges (e.g., Owner, Read, or Write)</li> <li>name of the secret to be create to store the user credentials and DB access information. This results in creating a secret  <code>&lt;user-cr-name&gt;-&lt;secret-name&gt;</code> that can be accessed in the Secrets section of KRM.</li> </ul> <p></p>"},{"location":"charts/krm/resources/#managing-s3-resources-with-krm","title":"Managing S3 resources with KRM","text":"<p>If supported by the deployment, using Minio S3 operator (http://github.com/scc-digitalhub/minio-operator/) it is possible to create new S3 buckets, policies, and create/associate users to them.</p> <p>Accessing <code>S3 Buckets</code> menu of the KRM, it is possible to list, create, and delete S3 buckets, policies, and users.</p> <p></p> <p>To create a new Bucket, provide the following:</p> <ul> <li>name of the bucket to create</li> <li>optional quota for the bucket</li> </ul> <p></p> <p>In the S3 Policies tab view it is possible also to configure the S3 policies. To create a new policy, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the policy to be created</li> <li>standard S3 policy spec in JSON format.</li> </ul> <p></p> <p>In the S3 Users tab view it is possible also to configure the S3 users. To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>access key for the user to be created</li> <li>secret key for the user</li> <li>list of policies to associate to the user.</li> </ul> <p></p>"},{"location":"charts/krm/resources/#managing-postgrest-data-services-with-krm","title":"Managing PostgREST Data Services with KRM","text":"<p>You can deploy new PostgREST data services through KRM. A PostgREST service exposes a set of PostgreSQL tables through a REST API, allowing to query and even modify them.</p> <p>Access <code>PostgREST Data Services</code> on the left menu.</p> <p></p> <p>When creating a new PostgREST service, fields are as follows:</p> <ul> <li>Name of the resource</li> <li>Schema to expose</li> <li>Existing DB user (role) on behalf of which the service will operate OR the list of actions to enable and the list of tables which will be exposed and on which these actions may be performed. The user will be created automatically for this second option.</li> <li>Connection information.<ul> <li>If you choose not to provide an existing secret, Host, Database, User and Password are required.</li> <li>If you decide to use a secret, you must provide the secret's name. Two possible configurations are valid:<ul> <li>If the secret contains <code>POSTGRES_URL</code> (the full connection string, as <code>postgresql://user:password@host:port/database</code>), all other connection fields will be ignored.</li> <li>If the secret contains <code>USERNAME</code> and <code>PASSWORD</code>, Host and Database must be provided, or the resource will enter an error state.</li> </ul> </li> </ul> </li> </ul> <p>Port is optional and defaults to <code>5432</code> if not provided.</p> <p>Extra connection parameters may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable SSL: <code>sslmode=disable</code>.</p> <p>Connection information</p> <p>Please note that when no existing DB user is specified, the user specified in the Connection section must have sufficient privileges to manage roles. By default, the owner/writer/reader users created by the Postgres operator do not have this permission.</p> <p>Schema exposure</p> <p>PostgREST exposes all tables and views in the specified schema. In order to have better control over the exposed data, it is recommended to create a separate schema (e.g., <code>api</code>) and provide access to data through views or stored procedures. You can use SQLPad to do this.</p> <p></p> <p>Check out the official documentation for more information on PostgREST.</p>"},{"location":"charts/krm/resources/#managing-dremio-data-services-with-krm","title":"Managing Dremio Data Services with KRM","text":"<p>You can deploy new Dremio data services through KRM. A Dremio data service exposes Dremio data through a REST API.</p> <p>Access <code>Dremio Data Services</code> on the left menu.</p> <p></p> <p>To create a new service, provide the following:</p> <ul> <li>Name of the resource</li> <li>List of virtual datasets to expose</li> <li>Connection information.<ul> <li>Host is required.</li> <li>Port is optional and will default to <code>32010</code> if not provided.</li> <li>User and Password are required, unless you choose to use a secret, in which case the secret's name must be provided. The secret should contain <code>USER</code> and <code>PASSWORD</code>.</li> <li>Extra connection parameters are optional and may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable certificate verification: <code>useEncryption=false&amp;disableCertificateVerification=true</code>.</li> </ul> </li> </ul> <p>If you instantiated Dremio through Coder, the value of Host is the value of Arrow Flight Endpoint, stripped of <code>:</code> and port. User is <code>admin</code> and Password is the value you entered when creating the workspace.</p> <p></p> <p>A Dremio REST service will be deployed, connected to the specified Dremio instance and exposing a simple REST API over the listed datasets.</p>"},{"location":"charts/krm/resources/#exposing-services-externally","title":"Exposing services externally","text":"<p>Various APIs and services (e.g., PostgREST or Dremio data services, serverless functions) may be exposed externally, outside of the platform, on a public domain of the platform. Using KRM, the operation amounts to defining a new API gateway resource that will be transformed into the corresponding ingress routing specification.</p> <p></p> <p>To create a new API gateway, provide the following:</p> <ul> <li>Name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Kubernetes service to be exposed (select it from the dropdown list and port will automatically be provided).</li> <li>Host defines the full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>myservice.services.example.com</code>.</li> <li>Relative path to expose the service on.</li> <li>Authentication information. Currently, services may be unprotected (<code>None</code>) or protected with <code>Basic</code> authentication, specifying username and password.</li> </ul>"},{"location":"charts/krm/resources/#defining-and-managing-crd-schemas","title":"Defining and Managing CRD Schemas","text":"<p>To have a valid representation of the CRs in the system, it is necessary to have a JSON specification schema for each CRDs. Normally, such schema is provided with the CRD definition and is used by KRM to manage the resources. However, in certain cases a CRD may have no structured schema definition attached. To allow for managing such resources, it is possible to provide a custom schema for the CRD.</p> <p>Creating a schema is fairly simple. Access the Settings section from the left menu and click Create.</p> <p>The CRD drop-down menu will list all Custom Resource Definitions available on the Kubernetes instance; when you pick one, the Version field will automatically be filled with the version of the currently active schema.</p> <p>Provide the Schema definition and save it in KRM for future CR management.</p>"},{"location":"charts/krm/roles/","title":"KRM roles","text":"<p>Enabling authentication for the Kubernetes Resource Manager is required to use this feature.</p> <p>Setting up roles can be a great way for assigning permissions to the users of the Kubernetes Resource Manager, setting up limitations to what they can do and the resources they can have access to.</p> <p>To set up your custom KRM roles and permissions, follow this example and change the fields to your needs in your Values file:</p> <pre><code>kubernetes-resource-manager:\n  oidc:\n    roleClaim: \"krm_roles\"    # Name of the role used \n    access:\n      roles:\n        - role: ROLE_MY_ROLE  # Name of the role\n          # Resources associated to the role with permissions\n          resources: k8s_service, k8s_secret::read, mycrd/example.com::write\n</code></pre> <p>In this basic example we create a Role called ROLE_MY_ROLE that will have:</p> <ul> <li>Access to the services</li> <li>Access with read permissions to the secrets</li> <li>Access with write permissions to a custom CRD</li> </ul> <p>You will also have to setup your authentication provider accordingly, so that you can associate the correct role to the correct users.</p>"},{"location":"components/hpc/hcp-integration/","title":"Digitalhub Platform: HCP Workload Offloading via Interlink","text":""},{"location":"components/hpc/hcp-integration/#overview","title":"Overview","text":"<p>Digitalhub platform leverages Kubernetes-native orchestration with the capability to dynamically offload machine learning workloads to an external HPC (High Performance Computing) cluster using the InterLink Project. This hybrid approach combines the flexibility of Kubernetes with the raw computational power of HPC resources for demanding ML training and inference tasks.</p>"},{"location":"components/hpc/hcp-integration/#architecture","title":"Architecture","text":"<p>InterLink API and the plugin deployment can be arranged in three different ways across the kubernetes cluster and the remote HPC part. Check InterLink Project documentations to get more informations.</p> <p>In this example we will use the tunneled deployment scenario.</p> <p></p>"},{"location":"components/hpc/hcp-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (with Digitalhub platform installed)</li> <li>Access to HCP cluster with job scheduler (Slurm, PBS, etc.)</li> <li>Interlink project deployed and configured</li> <li>Network connectivity between clusters</li> <li>Appropriate authentication credentials</li> </ul>"},{"location":"components/hpc/hcp-integration/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ol> <li> <p>SSH Key Setup Generate an SSH key pair if you don't have one: <pre><code># Generate SSH key pair\nssh-keygen -t rsa -b 4096 -f ~/.ssh/interlink_rsa\n\n# Copy private key to remote server\nscp ~/.ssh/interlink_rsa user@remote-server:~\n\n# Test SSH connection\nssh -i ~/.ssh/interlink_rsa user@remote-server\n</code></pre></p> </li> <li> <p>Interlink Setup Deploy the Interlink on your kubernetes cluster:</p> </li> </ol> <pre><code>helm install --create-namespace -n interlink virtual-node \\\n  oci://ghcr.io/intertwin-eu/interlink-helm-chart/interlink \\\n  --values my-values.yaml\n</code></pre> my-values.yaml<pre><code>nodeName: interlink-socket-node\n\ninterlink:\n  enabled: true\n  socket: unix:///var/run/interlink.sock\n\nplugin:\n  socket: unix:///var/run/plugin.sock\n\nsshBastion:\n  enabled: true\n  clientKeys:\n    authorizedKeys: \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAI...\" # Previosly createt public key\n  port: 31022\n\nvirtualNode:\n  resources:\n    CPUs: 8\n    memGiB: 32\n    pods: 100\n</code></pre>"},{"location":"components/hpc/hcp-integration/#hpc-configuration","title":"HPC Configuration","text":"<ol> <li>Download interlink-slurm-plugin on your login node.</li> <li>Configure InterLink Slurm plugin to listen on a Unix socket instead of a TCP port: /root/SlurmConfig.yaml<pre><code>SidecarPort: \"\"\nSocket: \"unix:///var/run/plugin.sock\"\nSbatchPath: \"/usr/bin/sbatch\"\nScancelPath: \"/usr/bin/scancel\"\nSqueuePath: \"/usr/bin/squeue\"\nSinfoPath: \"/usr/bin/sinfo\"\nCommandPrefix: \"\"\nSingularityPrefix: \"\"\nSingularityPath: \"singularity\"\nExportPodData: true\nDataRootFolder: \".local/interlink/jobs/\"\nNamespace: \"vk\"\nTsocks: false\nTsocksPath: \"$WORK/tsocks-1.8beta5+ds1/libtsocks.so\"\nTsocksLoginNode: \"login01\"\nBashPath: /bin/bash\nVerboseLogging: true\nErrorsOnlyLogging: false\nContainerRuntime: singularity\nEnrootDefaultOptions: [\"--rw\"]\nEnrootPrefix: \"\"\nEnrootPath: enroot\n</code></pre></li> <li> <p>On the remote HCP login node, start your interLink plugin: <pre><code># Example: Start SLURM plugin on remote HPC system\ncd /path/to/plugin\nSLURMCONFIGPATH=/root/SlurmConfig.yaml SHARED_FS=true /path/to/plugin/slurm-sidecar\n</code></pre></p> </li> <li> <p>Forward slurm plugin Unix socket to ssh the bastion host: <pre><code>ssh -nNT -L /var/run/plugin.sock:/var/run/plugin.sock user@sshbastiononkubernetes\n</code></pre></p> </li> </ol>"},{"location":"components/hpc/hcp-integration/#post-installation","title":"Post-Installation","text":""},{"location":"components/hpc/hcp-integration/#verify-deployment","title":"Verify Deployment","text":"<pre><code># Check virtual node status\nkubectl get node &lt;nodeName&gt;\n\n# Check pod status\nkubectl get pods -n interlink\n\n# View virtual node details\nkubectl describe node &lt;nodeName&gt;\n\n# Check logs\nkubectl logs -n interlink deployment/&lt;nodeName&gt;-node -c vk\n</code></pre>"},{"location":"components/hpc/hcp-integration/#testing-the-virtual-node","title":"Testing the Virtual Node","text":"<pre><code># test-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-workload\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: &lt;nodeName&gt;\n  containers:\n  - name: test\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n</code></pre> <p>```bash kubectl apply -f test-pod.yaml kubectl get pod test-workload -o wide</p>"},{"location":"components/krm/krm-custom-views/","title":"KRM - Custom views","text":"<p>In Kubernetes Resource Manager, it is possible to customize the pages for creating, listing, inspecting and editing custom resources of a specific kind. As it involves writing React code, and using the React-admin library, familiarity with these technologies may be required, depending on the desired level of customization.</p>"},{"location":"components/krm/krm-custom-views/#create-a-custom-view","title":"Create a custom view","text":"<p>Create a new file, under <code>frontend/src/resources</code>, containing the name of the custom resource definition (CRD), in the format <code>cr.&lt;CRD&gt;.ts</code>. For example, if the CRD is <code>myresource.example.com</code>, the filename will be <code>cr.myresource.example.com.tsx</code>.</p> <p>You are encouraged to use other files within the path as reference on how to write this file. You are expected to define 4 functions, one for each of these actions: Create, List, Show, Edit. Each would have this structure: <pre><code>const CrCreate = () =&gt; { // example for Create\n    ...\n    return (\n        &lt;&gt;\n            ...\n        &lt;/&gt;\n    );\n};\n</code></pre></p> <p>The body of the function will perform any kind of computation that your page may need, while the return segment will contain the form or datagrid to display.</p> <p>Then, register these functions within a view and export it:</p> <pre><code>const CustomView: View = {\n    key: 'myresource.example.com',\n    name: 'My Resource',\n    list: CrList,\n    show: CrShow,\n    create: CrCreate,\n    edit: CrEdit,\n};\n\nexport default CustomView;\n</code></pre>"},{"location":"components/krm/krm-custom-views/#register-the-custom-view","title":"Register the custom view","text":"<p>Once the custom view file is ready, you simply need to register it. Open <code>frontend/src/App.tsx</code> and import the view:</p> <pre><code>import crExample from './resources/cr.myresource.example.com';\n</code></pre> <p>Then, register it by adding a line to the <code>customView</code> structure:</p> <pre><code>const customViews: { [index: string]: View } = {\n    'myresource.example.com': crExample,\n    ...\n};\n</code></pre>"}]}