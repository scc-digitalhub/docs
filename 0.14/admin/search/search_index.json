{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Platform Administration Overview","text":"<p>When deploying in a production environment, the platform setup requires additional steps for its secure and efficient use. In this  section we take into account aspects that are required for the platform setup. </p> <ul> <li>Authentication and Access Control</li> <li>Platform Configuration</li> </ul>"},{"location":"authentication/","title":"Platform Authentication and Access Control","text":"<p>The DigitalHub Platform supports authentication with an external provider.</p> <p>It is mandatory to set custom values for the platform, so Helm knowledge is required.</p> <p>This section will show how to set an authentication for the following:</p> <ul> <li>Coder</li> <li>Core</li> <li>Dashboard</li> <li>Kubernetes Resource Manager</li> <li>Kubeflow</li> <li>Minio</li> </ul>"},{"location":"configuration/","title":"Platform Configuration","text":"<p>The DigitalHub Platform provides configuration options in the DigitalHub values.yaml file.</p> <p>The safest way to set up your custom values is to use a values file in which you will set up the options you are interested in.</p> <p>Thanks to the Helm hereditary properties, the platform values will change taking the values of your custom file, preserving the integrity of the originals and allowing you to use a shorter set of customized values.</p> <p>You can use a custom set of values from a file like the example below, in which we install digitalhub with custom values: <pre><code>helm upgrade -n &lt;YOUR_NAMESPACE&gt; &lt;YOUR_RELEASE&gt; digitalhub/digitalhub --install --create-namespace --timeout 45m0s --values &lt;YOUR_VALUES_FILE_PATH&gt;\n</code></pre> In this example, <code>--set global.registry.url=\"MINIKUBE_IP_ADDRESS\"</code> and <code>--set global.externalHostAddress=\"MINIKUBE_IP_ADDRESS\"</code> are not specified in the installation command, but they can be specified in your values file:</p> <pre><code>global:\n  registry:\n    url: \"YOUR_ADDRESS\"\n  externalHostAddress: &amp;globalExternalUrl \"YOUR_ADDRESS\"\n</code></pre>"},{"location":"ingress/","title":"Ingress configuration","text":"<p>The services of the platform can be exposed with Ingress by editing your values file.</p> <p>For every exposable component, you will find a value field for the ingress, set by default to enabled: false.</p> <p>After setting enabled to true to activate the Ingress creation, check the component's values.yaml file to see how you should structure your custom values file and set all the neeeded Ingress values.</p> <p>The example below is for the Core Ingress:</p> <pre><code>ingress:\n  enabled: true\n  className: \"youringressclass\"\n  hosts:\n    - host: your.host\n      paths: \n        - pathType: ImplementationSpecific\n          path: /\n  tls:\n  - secretName: yourTlsSecret\n</code></pre>"},{"location":"installation/","title":"Installation on cluster","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>A configured image registry</li> <li>A configured DNS</li> <li>Domains and Ingress Controller for service exposition</li> <li>A configured OAuth provider to enable authentication</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>Once you have set your custom values.yaml file, DigitalHub can be installed as follows:</p> <p>1) Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></p> <p>2) Install DigitalHub with Helm and your custom values.</p> <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --values PATH_TO_YOUR_VALUES_FILE --timeout 45m0s\n</code></pre> <p>5) Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></p> <p>Once installed, you should see the references (URLs) for the different tools of the platform, similar to the example below: <pre><code>##########################################################\n#   _____   _       _           _ _     _       _        #\n#  (____ \\ (_)     (_)_        | | |   | |     | |       #\n#   _   \\ \\ _  ____ _| |_  ____| | |__ | |_   _| | _     #\n#  | |   | | |/ _  | |  _)/ _  | |  __)| | | | | || \\    #\n#  | |__/ /| ( ( | | | |_( ( | | | |   | | |_| | |_) )   #\n#  |_____/ |_|\\_|| |_|\\___)_||_|_|_|   |_|\\____|____/    #\n#            (_____|                                     #\n#                                                        #\n##########################################################\n\nDigitalhub has been installed. Check its status by running:\n  kubectl --namespace digitalhub get pods\n\nDigitalhub componet URLs:\n  - Dashboard: http://192.168.76.2:30110\n  - Jupyter: http://192.168.76.2:30040 (Create jupyter workspace from template in the coder dashboard before use)\n  - Dremio: http://192.168.76.2:30120 (Create dremio workspace from template in the coder dashboard before use)\n  - Sqlpad: http://192.168.76.2:30140 (Create sqlpad workspace from template in the coder dashboard before use)\n  - Grafana: http://192.168.76.2:30130 (Create grafana workspace from template in the coder dashboard before use)\n  - Vscode: http://192.168.76.2:30190 (Create vscode workspace from template in the coder dashboard before use)\n  - Docker Registry: http://192.168.76.2:30150\n  - Minio API: http://192.168.76.2:30080 (Username: minio Password: minio123)\n  - Minio UI: http://192.168.76.2:30090 (Username: minio Password: minio123)\n  - KubeFlow: http://192.168.76.2:30100\n  - Coder: http://192.168.76.2:30170 (Username: test@digitalhub.test Password: Test12456@!)\n  - Core: http://192.168.76.2:30180\n  - Kubernetes Resource Manager: http://192.168.76.2:30160\n</code></pre></p>"},{"location":"upgrading/","title":"Upgrading DigitalHub","text":"<p>Once the platform is installed, you may find yourself in need of tweaking it and upgrading it.</p> <p>With the command <code>helm upgrade</code> you will be able to change the values of the platform with your custom ones like the example below:</p> <pre><code>helm upgrade -n &lt;NAMESPACE&gt; &lt;RELEASE&gt; digitalhub/digitalhub --timeout 30m0s --values &lt;YOUR_VALUES_FILE_PATH&gt;\n</code></pre> <p>Upgrading Coder templates</p> <p>If you wish to upgrade the Coder templates, you can do so.</p> <p>You can find them in <code>digitalhub/charts/digitalhub/confs/coder</code>.</p> <p>However, it is mandatory to create and set your Coder access token in the values file.</p> <pre><code>coder:\n  template:\n    upgrade:\n      # Set it to true if you want to upgrade the Coder templates.\n      enabled: false\n      # In order to upgrade the templates, you will need to create and set here a Coder Token.\n      token: \"\"\n</code></pre> <p>Upgrading from Digitalhub 0.7 to 0.8</p> <p>This section is aimed only for environments with an already running instance of Digitalhub. </p> <p>Due to the removal of MLRun, the upgrading process needs some extra steps.</p> <p>Follow this guide to upgrade your installation while keeping your existing environment stable:</p> <p>1) Clone the repository of Digitalhub. You will need to apply some CRDS manually.</p> <p>2) Apply the CRDS inside the folder <code>digitalhub/charts/kubeflow-pipelines/crds</code> in your namespace. Most of the CRDS are the same as the one applied with the MLRun chart, but some are not and are needed for the standalone installation of Kubeflow Pipelines.</p> <p>3) The MLRun chart included a MySql depolyment, but, although the Kubeflow Pipelines chart uses the same approach, the MySql version used (8.0.26) is greater then the one installed with MLRun, therefore it is necessary to remove the existing MySql deployment and the associated PVC.</p> <p>If your environment, instead of the embedded one, used an external deployment of MySql with a version compatible with 8.0.26, you can choose to keep it setting the values accordingly. Please, check the MySql configuration guide for KFP for more details.</p> <p>4) You can now upgrade to Digitalhub 0.8.0. After the installation, make sure that the crds inside <code>digitalhub/charts/kubeflow-pipelines/crds</code> have been all correctly applied.</p>"},{"location":"authentication/coder/","title":"Coder","text":"<p>To enable the authentication with a provider for Coder, please consult the official Coder documentation.</p> <p>In your provider, the redirect url should correspond to <code>https://yourcoderurl/api/v2/users/oidc/callback</code>.</p>"},{"location":"authentication/core/","title":"Core","text":"<p>To enable the authentication with a provider for Core, you will need to set the values in the file Values.yaml of the chart digitalhub in the Core section.</p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>core:\n  authentication:\n    openId:\n      enabled: true\n      issuerUri: \"https://yourproviderurl\" # Set the issuer url of your provider\n      jwtAudience: \"\" # Set the audience\n      jwtClaim: \"\" # Set the claims\n      oidcClientId: \"\" # Use this if you want to hardcode your clientID\n      scope: \"\" # Specify the scopes\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n</code></pre> <p>In your provider, the redirect url should correspond to <code>https://yourcoreurl/console/auth-callback</code>.</p>"},{"location":"authentication/dashboard/","title":"Dashboard","text":"<p>To enable the authentication with a provider for the Dashboard, you will need to set the values in the file Values.yaml of the chart digitalhub in the Dashboard section.</p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>dashboard:\n  oidc:\n    enabled: true\n    audience:\n      clientId: \"\" # Use this if you want to hardcode your clientID\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n    config:\n      issuer: \"https://yourproviderurl\" # Set the issuer url of your provider\n</code></pre>"},{"location":"authentication/krm/","title":"Kubernetes Resource Manager","text":"<p>To enable the authentication with a provider for Kubernetes Resource Manager, you will need to set the values in the file Values.yaml of the chart digitalhub in the Kubernetes Resource Manager section. </p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>kubernetes-resource-manager:\n  oidc:\n    enabled: true\n    audience:\n      clientId: \"\" # Use this if you want to hardcode your clientID\n      externalSecret: # Use this if you want to get the clientID by secret.\n        name: \"\" # Name of the secret\n        key: \"\" # Key of the secret containing the clientID\n    issuer: \"https://yourproviderurl\" # Set the issuer url of your provider\n    scope: \"\" # Set the scopes\n    authType: \"\" # Set the type of authentication\n</code></pre> <p>In your provider, the redirect url should correspond to <code>https://yourkubernetesresourcemanagerurl/console/auth-callback</code>.</p>"},{"location":"authentication/kubeflow/","title":"Kubeflow","text":"<p>To enable the authentication with a provider for Kubeflow, you will need to set the values in the file Values.yaml of the chart digitalhub in the OAuth2 Proxy section. </p> <p>The applications using OAuth2 Proxy are specified as a list and should be added together one after the other.</p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>oauth2-proxy:\n  enabled: true\n  apps:\n    - redirectUrl: \"https://yourkubeflowurl/oauth2/callback\" # Set the redirect url for the application\n      oidcIssuerUrl: \"https://yourproviderurl\" # Set the url of your provider\n      existingSecret:\n        name: \"\" # Name of the secret containing clientID and clientSecret\n        clientId: \"clientid\" # Key of the secret containing the clientID\n        secretKey: \"clientsecret\" # Key of the secret containing the client secret\n</code></pre>"},{"location":"authentication/minio/","title":"Minio","text":"<p>To enable the authentication with a provider for Minio, you will need to set the values in the file Values.yaml of the chart digitalhub in the Minio section. </p> <p>The example below shows only the values concerning the authentication configuration.</p> <pre><code>minio:\n  oidc:\n    enabled: true\n    configUrl: \"https://yourproviderurl/.well-known/openid-configuration\" # Set the url of your provider\n    existingClientSecretName: \"\" # Name of the secret containing clientID and clientSecret\n    existingClientIdKey: \"\" # Key of the secret containing the clientID\n    existingClientSecretKey: \"\" # Key of the secret containing the client secret\n    claimName: \"\"  # Set the name of the JWT Claim\n    scopes: \"\" # Set the scopes\n    redirectUri: \"https://yourminiourl/oauth_callback\" # Set the redirect for the application\n    displayName: \"\" # Set the name of your provider\n</code></pre> <p>Please, consult the official Minio documentation for more details about the options used above.</p> <p>In your provider, the redirect url should correspond to <code>https://yourminiourl/oauth_callback</code>.</p>"},{"location":"charts/core/keystore/","title":"Keystore","text":"<p>To set up a Keystore for Core, add the following section to your <code>values.yaml</code> file and configure the following fields:</p> <pre><code>core:\n  keystore:\n    existingSecret:\n      secretName: \"keystore-secret\" # Name of the secret containing the keystore\n      keyName: \"keystore.jwks\"    # Name of the key in your keystore secret, should correspond to the keystore file name\n    keystoreKid: \"\"  # Specify the key that the keystore should pick\n    keystorePath: \"/etc/keystore\" # Path where your keystore will be saved\n</code></pre> <p>In this example, a Keystore will be created in the path <code>/etc/keystore/keystore.jwks</code> from a secret called <code>keystore-secret</code>. The key of the secret, <code>keystore.jwks</code>, must contain the base64 encoded keystore.</p>"},{"location":"charts/core/sts/","title":"STS","text":"<p>WARNING: this feature cannot be used locally as it depends on an Authentication Provider that should be installed in your environment.</p> <p>STS allows you to work with temporary credentials to do operations with Core to the Postgres database and the Minio bucket, avoiding the use of persistent ones and reducing the risk of a security breach.</p> <p>To activate STS, set <code>core.sts.enabled</code> to <code>true</code>. The values to activate Postgres and Minio credentials are, respectively, <code>core.sts.db.enabled</code> (set to <code>true</code> if activated) and <code>core.sts.minio.enabled</code> (set to <code>true</code> if activated).</p>"},{"location":"charts/core/sts/#credentials-duration","title":"Credentials duration","text":"<p>You can set the duration of the temporary credentials in two ways.</p> <ul> <li>Setting <code>core.sts.credentials.duration</code> to the time (in seconds) you desire will set the same time for the Postgres and Minio ones</li> <li>If you wish to set them case by case, leave <code>core.sts.credentials.duration</code> as an empty string and set the time of <code>core.sts.credentials.minio.duration</code> and <code>core.sts.credentials.db.duration</code></li> </ul> <p>NOTICE: currently, the time limit cannot be major than 28800.</p>"},{"location":"charts/core/templates/","title":"Core templates","text":""},{"location":"charts/core/templates/#what-are-core-templates","title":"What are Core templates?","text":"<p>You can create and use custom Core templates for your use cases.</p> <p>With this feature, you will be able to customize the resources generated by Core to achieve your needs, creating some baselines that the users can use for their projects. For example, you could:</p> <ul> <li>Set custom requests and limits</li> <li>Harden the securityContext</li> <li>Add tolerations for GPU usage</li> </ul> <p>Templates can be used as profiles when creating pods, deployments, services and secrets with Core.</p>"},{"location":"charts/core/templates/#create-a-template","title":"Create a template","text":"<p>In order to be used, the templates must be specified in a ConfigMap in the form of values of a key, while the key should be the name of the template file that will be mounted in Core.</p> <p>Be aware that you must create and apply the ConfigMap yourself.</p> <p>You can specify multiple templates in the form of key: value (filename: template) in the same ConfigMap.</p> <p>The following is an example of a template for Core:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: core-templates\ndata:\n  template-example.yaml: |-   # Name of the template file\n    id: template1   \n    name: template1\n    description: Lorem ipsum dolor sit amet, consectetur adipiscing elit. In imperdiet lectus arcu, eget mattis dui varius vitae. Morbi lorem augue, volutpat nec mi sagittis, vulputate congue ligula. Maecenas ac luctus mauris. Ut maximus est convallis nisi porta, vel sodales lorem dictum. Praesent ullamcorper enim accumsan diam pharetra feugiat. Integer maximus tortor et nulla fermentum commodo. In vitae massa nec leo fermentum interdum. Integer consectetur dolor vitae accumsan vulputate. Curabitur placerat suscipit justo tempor placerat. Nam euismod suscipit ante non sollicitudin. Integer at cursus sem.\n    runtimeClass: class1\n    priorityClass: class1\n    envs:\n      - name: ENV1\n        value: VALU123123\n      - name: ENV2\n        value: VALU123123    \n    ---\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: pi\n    spec:\n      template:\n        spec:\n          containers:\n            - name: pi\n              image: perl:5.34.0\n              command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          restartPolicy: Never\n      backoffLimit: 4\n</code></pre> <p><code>template-example.yaml</code> is the key, and it is the name of the template file that will be mounted in Core. The actual template is the value of <code>template-example.yaml</code>, so, in this case:</p> <pre><code>id: template1   \nname: template1\ndescription: Lorem ipsum dolor sit amet, consectetur adipiscing elit. In imperdiet lectus arcu, eget mattis dui varius vitae. Morbi lorem augue, volutpat nec mi sagittis, vulputate congue ligula. Maecenas ac luctus mauris. Ut maximus est convallis nisi porta, vel sodales lorem dictum. Praesent ullamcorper enim accumsan diam pharetra feugiat. Integer maximus tortor et nulla fermentum commodo. In vitae massa nec leo fermentum interdum. Integer consectetur dolor vitae accumsan vulputate. Curabitur placerat suscipit justo tempor placerat. Nam euismod suscipit ante non sollicitudin. Integer at cursus sem.\nruntimeClass: class1\npriorityClass: class1\nenvs:\n  - name: ENV1\n    value: VALU123123\n  - name: ENV2\n    value: VALU123123    \n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi\nspec:\n  template:\n    spec:\n      containers:\n        - name: pi\n          image: perl:5.34.0\n          command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>As you can see, the template in the above example is composed of two parts:</p> <ul> <li>The upper part (that will be refered to as Profile)</li> <li>The bottom part (that will be refered to as Base Template)</li> </ul> <p>The template merge process is mainly done in three steps:</p> <p>1) The Base Template is used as the starting point for the resource (in this case, a Job). This part can be modified by the user.</p> <p>2) Core appends it's own configuration to the resource</p> <p>3) The Profile is applied, adding the specified fields to the resource; as a result, the job will have the runtimeClass, priorityClass and two variables specified in the Profile. This part cannot be modified by the user.</p> <p>If you wish so, you can also decide to use a Profile without the Base Template if you want to set just certain values (for example, resources or tolerations).</p>"},{"location":"charts/core/templates/#setting-templates-in-the-valuesyaml-file","title":"Setting templates in the Values.yaml file","text":"<p>The following is a reference for the configuration of Core Templates in the Values.yaml file:</p> <pre><code>core:\n  templates:   # Every item in the list must match a template you have created.\n    - name: template-example\n      path: /templates/template-example.yaml\n  volumeMounts:\n    - name:  templates\n      mountPath:  /templates   # Directory in Core in which the template files will be saved\n  volumes:\n    - name:  templates   # Volume for the templates\n      configMap:\n        name: core-templates   # Must match the ConfigMap containing the templates\n</code></pre> <p>Now you can create resources with your template and profile from Core.</p>"},{"location":"charts/core/templates/#supported-fields","title":"Supported fields","text":"<p>The following is a list of all the fields that you can set in the Profile section. You can find an in depth explanation of how to set these fields in the documentation about Kubernetes Resources</p> Field Format envs List secrets List resources Object volumes List nodeSelector List affinity Object tolerations List runtimeClass String priorityClass String imagePullPolicy String"},{"location":"charts/krm/roles/","title":"KRM roles","text":"<p>Enabling authentication for the Kubernetes Resource Manager is required to use this feature.</p> <p>Setting up roles can be a great way for assigning permissions to the users of the Kubernetes Resource Manager, setting up limitations to what they can do and the resources they can have access to.</p> <p>To set up your custom KRM roles and permissions, follow this example and change the fields to your needs in your Values file:</p> <pre><code>kubernetes-resource-manager:\n  oidc:\n    roleClaim: \"krm_roles\"    # Name of the role used \n    access:\n      roles:\n        - role: ROLE_MY_ROLE  # Name of the role\n          # Resources associated to the role with permissions\n          resources: k8s_service, k8s_secret::read, mycrd/example.com::write\n</code></pre> <p>In this basic example we create a Role called ROLE_MY_ROLE that will have:</p> <ul> <li>Access to the services</li> <li>Access with read permissions to the secrets</li> <li>Access with write permissions to a custom CRD</li> </ul> <p>You will also have to setup your authentication provider accordingly, so that you can associate the correct role to the correct users.</p>"},{"location":"components/hpc/hcp-integration/","title":"Digitalhub Platform: HCP Workload Offloading via Interlink","text":""},{"location":"components/hpc/hcp-integration/#overview","title":"Overview","text":"<p>Digitalhub platform leverages Kubernetes-native orchestration with the capability to dynamically offload machine learning workloads to an external HPC (High Performance Computing) cluster using the InterLink Project. This hybrid approach combines the flexibility of Kubernetes with the raw computational power of HPC resources for demanding ML training and inference tasks.</p>"},{"location":"components/hpc/hcp-integration/#architecture","title":"Architecture","text":"<p>InterLink API and the plugin deployment can be arranged in three different ways across the kubernetes cluster and the remote HPC part. Check InterLink Project documentations to get more informations.</p> <p>In this example we will use the tunneled deployment scenario.</p> <p></p>"},{"location":"components/hpc/hcp-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (with Digitalhub platform installed)</li> <li>Access to HCP cluster with job scheduler (Slurm, PBS, etc.)</li> <li>Interlink project deployed and configured</li> <li>Network connectivity between clusters</li> <li>Appropriate authentication credentials</li> </ul>"},{"location":"components/hpc/hcp-integration/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<ol> <li> <p>SSH Key Setup Generate an SSH key pair if you don't have one: <pre><code># Generate SSH key pair\nssh-keygen -t rsa -b 4096 -f ~/.ssh/interlink_rsa\n\n# Copy private key to remote server\nscp ~/.ssh/interlink_rsa user@remote-server:~\n\n# Test SSH connection\nssh -i ~/.ssh/interlink_rsa user@remote-server\n</code></pre></p> </li> <li> <p>Interlink Setup Deploy the Interlink on your kubernetes cluster:</p> </li> </ol> <p><pre><code>helm install --create-namespace -n interlink virtual-node \\\n  oci://ghcr.io/intertwin-eu/interlink-helm-chart/interlink \\\n  --values my-values.yaml\n</code></pre> my-values.yaml: my-values.yaml<pre><code>nodeName: interlink-socket-node\n\ninterlink:\n  enabled: true\n  socket: unix:///var/run/interlink.sock\n\nplugin:\n  socket: unix:///var/run/plugin.sock\n\nsshBastion:\n  enabled: true\n  clientKeys:\n    authorizedKeys: \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAI...\" # Previosly createt public key\n  port: 31022\n\nvirtualNode:\n  resources:\n    CPUs: 8\n    memGiB: 32\n    pods: 100\n</code></pre></p>"},{"location":"components/hpc/hcp-integration/#hpc-configuration","title":"HPC Configuration","text":"<ol> <li>Download interlink-slurm-plugin on your login node.</li> <li>Configure InterLink Slurm plugin to listen on a Unix socket instead of a TCP port: <pre><code>SidecarPort: \"\"\nSocket: \"unix:///var/run/plugin.sock\"\nSbatchPath: \"/usr/bin/sbatch\"\nScancelPath: \"/usr/bin/scancel\"\nSqueuePath: \"/usr/bin/squeue\"\nSinfoPath: \"/usr/bin/sinfo\"\nCommandPrefix: \"\"\nSingularityPrefix: \"\"\nSingularityPath: \"singularity\"\nExportPodData: true\nDataRootFolder: \".local/interlink/jobs/\"\nNamespace: \"vk\"\nTsocks: false\nTsocksPath: \"$WORK/tsocks-1.8beta5+ds1/libtsocks.so\"\nTsocksLoginNode: \"login01\"\nBashPath: /bin/bash\nVerboseLogging: true\nErrorsOnlyLogging: false\nContainerRuntime: singularity\nEnrootDefaultOptions: [\"--rw\"]\nEnrootPrefix: \"\"\nEnrootPath: enroot\n</code></pre></li> <li> <p>On the remote HCP login node, start your interLink plugin: <pre><code># Example: Start SLURM plugin on remote HPC system\ncd /path/to/plugin\nSLURMCONFIGPATH=/root/SlurmConfig.yaml SHARED_FS=true /path/to/plugin/slurm-sidecar\n</code></pre></p> </li> <li> <p>Forward slurm plugin Unix socket to ssh the bastion host: <pre><code>ssh -nNT -L /var/run/plugin.sock:/var/run/plugin.sock user@sshbastiononkubernetes\n</code></pre></p> </li> </ol>"},{"location":"components/hpc/hcp-integration/#post-installation","title":"Post-Installation","text":""},{"location":"components/hpc/hcp-integration/#verify-deployment","title":"Verify Deployment","text":"<pre><code># Check virtual node status\nkubectl get node &lt;nodeName&gt;\n\n# Check pod status\nkubectl get pods -n interlink\n\n# View virtual node details\nkubectl describe node &lt;nodeName&gt;\n\n# Check logs\nkubectl logs -n interlink deployment/&lt;nodeName&gt;-node -c vk\n</code></pre>"},{"location":"components/hpc/hcp-integration/#testing-the-virtual-node","title":"Testing the Virtual Node","text":"<pre><code># test-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-workload\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: &lt;nodeName&gt;\n  containers:\n  - name: test\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n</code></pre> <p>```bash kubectl apply -f test-pod.yaml kubectl get pod test-workload -o wide</p>"},{"location":"components/solr/basicauth/","title":"Configuring Basic Auth for Solr","text":"<p>In order to use Solr with an unprivileged with Core, Solr must be configured with the basicAuth plugin (already included in the chart).</p> <p>This documentation will provide an overview for the Values that you have to set and will help you finding them in the values.yaml file of the platform.</p>"},{"location":"components/solr/basicauth/#step-1-coresolrcollection","title":"Step 1: core.solr.collection","text":"<p>This section provides the necessary configuration for <code>core</code></p> <p>First, set <code>core.solr.collection.initialize.enabled</code> to true. When set to false, this value tells core to initialize everything in Solr by itself, removing the possibility of using BasicAuth. When set to true, instead, a script will initialize the Solr collection and the BasicAuth values will take effect.</p> <p>Next, set <code>core.solr.collection.initialize.securityJsonSecret</code> to the name of the secret that will contain the key security.json. Please keep in mind that the value of <code>core.solr.collection.initialize.securityJsonSecret</code> MUST be the same of <code>solr.solrOptions.security.bootstrapSecurityJson.name</code>.</p>"},{"location":"components/solr/basicauth/#step-2-solrsolroptionssecurity","title":"Step 2: solr.solrOptions.security","text":"<p>This section provides the necessary configuration for <code>solr</code></p> <p>First, choose the name for the BasicAuth secret that will be used by the operator for performing actions between Solr and your K8S environment at <code>solr.solrOptions.security.basicAuthSecret</code>.</p> <p>Then, set the name of the secret containing the security.json key and corresponding data at <code>solr.solrOptions.security.bootstrapSecurityJson.name</code>.</p> <p>Once again, <code>solr.solrOptions.security.bootstrapSecurityJson.name</code> value must match the one in <code>core.solr.collection.initialize.securityJsonSecret</code>. Furthermore, do not change the value of <code>solr.solrOptions.security.bootstrapSecurityJson.name</code> unless you are using an already existing secret with the required format of the Solr BasicAuth Plugin (you can set this at <code>solr.useExistingSecurityJson</code>) with a different key name.</p>"},{"location":"components/solr/basicauth/#step-3-solrcreds","title":"Step 3: solr.creds","text":"<p>The final step for the configuration of the BasicAuth Plugin is the configuration of the users and the credentials that will be used by Solr.</p> <p>Please keep in mind that the four users (admin, k8sOper, solr, user) should be left as they are, but you should still change the passwords.</p> <p>There are two values for the password configuration: <code>password</code> and <code>passwordSha</code>. The reason is that Solr requires crypted passwords in the format <code>sha256(sha256(salt || password))</code>. While the first password will be used (as data) in the security json secret in pair with the corresponding user (as key), the corresponding encrypted password will be used directly in the security.json data as required by Solr standard. You can either choose to encrypt it yourself or use an online tool to do it, just be sure to encrypt the correct one for every user.</p>"}]}