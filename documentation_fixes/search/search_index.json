{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Digital Hub is an Open-Source platform for building and managing Data and AI applications and services. Bringing the principles of DataOps, MLOps, DevOps and GitOps, integrating the state-of-art open source technologies and modern standards, DigitalHub extends your development process and operations to the whole application life-cycle, from exploration to deployment, monitoring, and evolution.</p> <p></p> <ul> <li>Explore. Use on-demand interactive scalable workspaces of your choice to code, explore, experiment and analyze the data and ML. Workspace provide a secure and isolated execution environments natively connected to the platform, such as Jupyter Notebooks or VS Code, Dremio distributed query engine, etc. Use extensibility mechanisms provided by the underlying workspace manager to bring your own workspace templates into the platform.</li> <li>Process. Use persistent storages (Datalake and Relational DBs) to manage structured and non-structured data on top of the data abstraction layer. Elaborate  data, perform data analysis activites and train AI models using frameworks and libraries of your choice (e.g., from python-based to DBT, to arbitrary containers). Manage the supporting computational and storage resources in a declarative and transparent manner.</li> <li>Execute. Delegate the code execution, image preparation, run-time operations and services to the underlying Kubernetes-based execution infrastructure, Serverless platform, and pipeline execution automation environment.</li> <li>Integrate. Build new AI services and expose your data in a standard and interoperable manner, to facilitate the integration within different applications, systems, business intelligence tools and visualizations.</li> </ul> <p>To support this functionality, the platform relies on scalable Kubernetes platform and its extensions (operators) as well as on the modular architecture and functionality model that allows for dealing with arbitrary jobs, functions, frameworks and solutions without affecting your development workflow. The underlying methodology and management approach aim at facilitating the re-use, reproducibility, and portability of the solution among different contexts and settings.</p>"},{"location":"#interested","title":"Interested?","text":"<ul> <li>Quick Start. Bring the platform up and explore it in few minutes!</li> <li>Installation. Learn how to install, configure, and manage the platform in different settings.</li> <li>Overview. Deep dive into the platform functionality, architecture, components, and functionality.</li> </ul>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<ul> <li>Native support for the integration of new functionality</li> <li>Standard infrastructures and frameworks</li> <li>Common workflow and abstractions</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform relies on a built-in serverless platform, and supports Python functions as well as a set of ML model formats for serving.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with ML a range of frameworks, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (under development), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-on-minikube","title":"Installation on minikube","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>Minikube</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>1) Start minikube (change 192.168.49.0 if your network setup is different): <pre><code>    minikube start --insecure-registry \"192.168.49.0/24\" --memory 12288 --cpus 4\n</code></pre></p> <p>2) Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></p> <p>3) Get minikube external IP: <pre><code>    minikube ip\n</code></pre></p> <p>4) Install DigitalHub with Helm.</p> <p>Replace the two placeholders called <code>MINIKUBE_IP_ADDRESS</code> in the command below with the output of the previous command, <code>minikube ip</code> <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --set global.registry.url=\"MINIKUBE_IP_ADDRESS\" --set global.externalHostAddress=\"MINIKUBE_IP_ADDRESS\" --timeout 45m0s\n</code></pre></p> <p>5) Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></p> <p>Once installed, you should see the references (URLs) for the different tools of the platform, similar to the example below: <pre><code>##########################################################\n#   _____   _       _           _ _     _       _        #\n#  (____ \\ (_)     (_)_        | | |   | |     | |       #\n#   _   \\ \\ _  ____ _| |_  ____| | |__ | |_   _| | _     #\n#  | |   | | |/ _  | |  _)/ _  | |  __)| | | | | || \\    #\n#  | |__/ /| ( ( | | | |_( ( | | | |   | | |_| | |_) )   #\n#  |_____/ |_|\\_|| |_|\\___)_||_|_|_|   |_|\\____|____/    #\n#            (_____|                                     #\n#                                                        #\n##########################################################\n\nDigitalhub has been installed. Check its status by running:\n  kubectl --namespace digitalhub get pods\n\nDigitalhub componet URLs:\n  - Dashboard: http://192.168.76.2:30110\n  - Jupyter: http://192.168.76.2:30040 (Create jupyter workspace from template in the coder dashboard before use)\n  - Dremio: http://192.168.76.2:30120 (Create dremio workspace from template in the coder dashboard before use)\n  - Sqlpad: http://192.168.76.2:30140 (Create sqlpad workspace from template in the coder dashboard before use)\n  - Grafana: http://192.168.76.2:30130 (Create grafana workspace from template in the coder dashboard before use)\n  - Vscode: http://192.168.76.2:30190 (Create vscode workspace from template in the coder dashboard before use)\n  - Docker Registry: http://192.168.76.2:30150\n  - Minio API: http://192.168.76.2:30080 (Username: minio Password: minio123)\n  - Minio UI: http://192.168.76.2:30090 (Username: minio Password: minio123)\n  - KubeFlow: http://192.168.76.2:30100\n  - Coder: http://192.168.76.2:30170 (Username: test@digitalhub.test Password: Test12456@!)\n  - Core: http://192.168.76.2:30180\n  - Kubernetes Resource Manager: http://192.168.76.2:30160\n</code></pre></p> <p>A note for Windows, Darwin and WSL users</p> <p>As of now, due to the limitations of Minikube it is not possible to access your applications directly while using one of the OS mentioned above.</p> <p>You can still access your apps from browser, but you will have to use the <code>kubectl port-forward</code> command.</p> <p>For example, if you wish to expose the core service, you can use: <pre><code>kubectl -n digitalhub port-forward service/digitalhub-core 30180:8080\n</code></pre> This will allow you to access core by typing <code>localhost:30180</code> in your browser.</p> <p>The full list of services can be checked using this command: <pre><code>kubectl -n digitalhub get services\n</code></pre></p> <p>Please consult the official Kubernetes documentation for more details.</p>"},{"location":"installation/#installation-on-cluster","title":"Installation on cluster","text":"<p>To install DigitalHub on a production environment, please consult the admin section of the documentation, where you will find informations about the configuration options and the installation as well.</p>"},{"location":"installation/#installing-the-cli","title":"Installing the CLI","text":"<p>To install and use the command-line interface, please refer to this section</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>To start with DigitalHub, the first step is to install the platform and all its components. For its functionality, DigitalHub relies on Kubernetes, a state-of-art Open-Source containerized application deployment, orchestration and execution platform. While it is possible to run DigitalHub on any Kubernetes installation, the quickest way is to deploy it on Minikube, a local Kubernetes environment with minimal settings. See here instruction on how to set up DigitalHub on Minikube.</p> <p>Once installed, you can access different platform components and perform different operations, ranging from explorative data science with Jupyter Notebooks, creating projects for data processing or ML tasks, managing necessary resources (e.g., databases or datalake buckets), creating and running different functions, etc.</p>"},{"location":"quickstart/#platform-components-and-functionality","title":"Platform Components and Functionality","text":"<p>To access the different components of the platform start from the landing page, where the components are linked:</p> <ul> <li>Use Coder to create interactive workspaces, such as Jupyter Notebooks, to perform explorative tasks, access and manage the data. See how to use Workspaces for these type of activities.</li> <li>Use DH Core UI to manage your data science and ML project and start with management activities, such as creating data items, defining and executing different functions and operations. Please note that these tasks may be done directly with the DH Core Python SDK from your interactive environment. See how to use DH Console for the management operations.</li> <li>To see and manage the relevant Kubernetes resources (e.g., services, jobs, secrets), as well as custom resources of the platform (e.g., databases, S3 buckets, data services), use Kubernetes Resource Manager. The operations and the functionality of the tool are described in the Resource Management with KRM section of the documentation.</li> <li>Use Minio browser to navigate your datalake, upload and manage the files. The datalake is based on S3 protocol and can be used also programmatically. See the Data and Transformations section on how the data abstraction layer is defined and implemented.</li> <li>If you perform ML task with the Python runtime, you can prepare data, create and log ML Models using DH Core (see, e.g., Python Runtime if you want to use python operations through DHCore). </li> <li>Use Core Serverless platform to deploy and expose Python functions or ML Models in different formats as services within the platform.</li> <li>It is possible to organize the data and ML operations in complex pipelines. Currently the platform relies on Kubeflow Pipelines specification for this purpose, orchestrating the activities as single Kubernetes Jobs. See more on this in the corresponding Pipelines section.</li> </ul> <p>You can also use the CLI to perform operations on the platform externally. See how to install and use it in this section.</p>"},{"location":"quickstart/#tutorials","title":"Tutorials","text":"<p>Start exploring the platform through a series of tutorials aiming at explaining the key usage scenarios for DigitalHub platform. Specifically</p> <ul> <li>Create your first data management pipeline, from data exploration to automated data ETL procedure running on the platform.</li> <li>Perform DBT data transformation and store the data in a database.</li> <li>Train a scikit-learn ML Model and deploy it as an inference server.</li> <li>Train a MLFLow-compatible Model and deploy it as an inference server.</li> <li>Train a custom ML Model and deploy it as a service with the serverless platform.</li> <li>Work with LLM Model and deploy it as a service with the serverless platform.</li> <li>Use Dremio distributed query engine to organize data and visualize with Grafana.</li> <li>Store data in DB to perform efficient and complex queries and expose the data as REST API.</li> </ul>"},{"location":"cli/commands/","title":"CLI commands","text":"<p>Available CLI commands and their parameters are listed here. In these examples, the executable is named <code>dhcli</code>.</p> <p>If you need to install the CLI, refer to this section.</p> <p>Run commands</p> <p>Depending on the shell you are using, you may have to run the CLI with <code>./dhcli</code>.</p>"},{"location":"cli/commands/#register","title":"<code>register</code>","text":"<p><code>register</code> takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional. Name of the environment to register.</li> <li><code>core_endpoint</code> Mandatory</li> </ul> <p><pre><code>dhcli register -e example http://localhost:8080\n</code></pre> It will create a <code>.dhcore.ini</code> file (if it doesn't already exist) in the user's home directory, or, if not possible, in the current one. A section will be appended, using the provided environment name (or, if missing, the one returned by the endpoint), containing the environment's configuration. This environment will be set as default, unless one is already set.</p>"},{"location":"cli/commands/#list-env","title":"<code>list-env</code>","text":"<p><code>list-env</code> lists available environments. It takes no parameters.</p> <pre><code>dhcli list-env\n</code></pre>"},{"location":"cli/commands/#use","title":"<code>use</code>","text":"<p><code>use</code> takes the following parameters:</p> <ul> <li><code>environment</code> Mandatory</li> </ul> <p><pre><code>dhcli use example\n</code></pre> This will set the default environment.</p>"},{"location":"cli/commands/#login","title":"<code>login</code>","text":"<p><code>login</code> is to be used after registering an environment with the <code>register</code> command. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional.</li> </ul> <p><pre><code>dhcli login -e example\n</code></pre> It will read the corresponding section from the configuration file and start the log in procedure. It will update this section with the access token obtained. If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#refresh","title":"<code>refresh</code>","text":"<p><code>refresh</code> is to be used after the <code>login</code> command, to update <code>access_token</code> and <code>refresh_token</code>. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> </ul> <p><pre><code>dhcli refresh example\n</code></pre> If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#remove","title":"<code>remove</code>","text":"<p><code>remove</code> takes the following parameters:</p> <ul> <li><code>environment</code> Mandatory</li> </ul> <p><pre><code>dhcli remove example\n</code></pre> It will remove the section from the configuration file.</p>"},{"location":"cli/commands/#init","title":"<code>init</code>","text":"<p><code>init</code> is used to install the platform's Python packages; therefore, Python must be installed. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> </ul> <p><pre><code>dhcli init example\n</code></pre> It will match core's minor version as indicated in the specified environment. If no environment is specified, it will use the default one.</p>"},{"location":"cli/commands/#create","title":"<code>create</code>","text":"<p><code>create</code> will create an instance of the indicated resource on the platform. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) when creating projects, mandatory otherwise.</li> <li><code>-f yaml_file_path</code> Mandatory when creating resources other than projects, alternative to <code>name</code> for projects.</li> <li><code>-n name</code> Optional (ignored) when creating resources other than projects, alternative to <code>yaml_file_path</code> for projects.</li> <li><code>-reset-id</code> Optional. Boolean. If set, the <code>id</code> specified in the file is ignored.</li> <li><code>resource</code> Mandatory</li> </ul> <p>The type of resource to create is mandatory. The project flag <code>-p</code> is only mandatory when creating resources other than projects (artifacts, models, etc.). For projects, you may omit the file path and just use the <code>-n</code> flag to specify the name. The <code>-reset-id</code> flag, when set, ensures the created object has a randomly-generated ID, ignoring the <code>id</code> field if present in the input file (this is not relevant to projects).</p> <p>Create a project: <pre><code>dhcli create -f samples/project.yaml projects\n</code></pre></p> <p>Create an artifact, while resetting its ID: <pre><code>dhcli create -p my-project -f samples/artifact.yaml -reset-id artifacts\n</code></pre></p>"},{"location":"cli/commands/#list","title":"<code>list</code>","text":"<p><code>list</code> returns a list of resources of the specified type. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-o output_format</code> Optional. Accepts <code>short</code>, <code>json</code>, <code>yaml</code>. Defaults to <code>short</code>.</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Optional. If present, will return all versions of specified resource. If missing, will return the latest version of all matching resources.</li> <li><code>-k kind</code> Optional</li> <li><code>-s state</code> Optional</li> <li><code>resource</code> Mandatory</li> </ul> <p><code>output_format</code> determines how the output will be formatted. The default value, <code>short</code>, is meant to be used to quickly check resources in the terminal, while <code>json</code> and <code>yaml</code> will format the output accordingly, making it ideal to write to file.</p> <p>List all projects:</p> <pre><code>dhcli list projects\n</code></pre> <p>List all artifacts in a project:</p> <pre><code>dhcli list -p my-project artifacts\n</code></pre> <p>Note that you can easily write the results to file by redirecting standard output: <pre><code>dhcli list -o yaml -p my-project artifacts &gt; output.yaml\n</code></pre></p>"},{"location":"cli/commands/#get","title":"<code>get</code>","text":"<p><code>get</code> returns the details of a single resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-o output_format</code> Optional. Accepts <code>short</code>, <code>json</code>, <code>yaml</code>. Defaults to <code>short</code>.</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Ignored if <code>id</code> is present, otherwise mandatory and will return the latest version of the specified resource.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>-n name</code>.</li> </ul> <p>Similarly to the <code>list</code> command, <code>output_format</code> determines how the output will be formatted. The default value, <code>short</code>, is meant to be used to quickly check resources in the terminal, while <code>json</code> and <code>yaml</code> will format the output accordingly, making it ideal to write to file.</p> <p>Get project:</p> <pre><code>dhcli get projects my-project\n</code></pre> <p>Get artifact: <pre><code>dhcli get -p my-project artifacts my-artifact-id\n</code></pre></p> <p>Get artifact and write to file: <pre><code>dhcli get -o yaml -p my-project artifacts my-artifact-id &gt; output.yaml\n</code></pre></p>"},{"location":"cli/commands/#update","title":"<code>update</code>","text":"<p><code>update</code> will update a resource with a new definition. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-f yaml_file_path</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Update a project: <pre><code>dhcli update -f samples/project.yaml projects my-project\n</code></pre></p> <p>Update an artifact: <pre><code>dhcli update -p my-project -f samples/artifact.yaml artifacts my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#delete","title":"<code>delete</code>","text":"<p><code>delete</code> will delete a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Optional (ignored) for projects, mandatory otherwise.</li> <li><code>-n name</code> Alternative to <code>id</code>, will delete all versions of a resource.</li> <li><code>-y</code> Optional. Boolean. If omitted, confirmation will be asked.</li> <li><code>-c</code> Optional. Boolean, only applies to projects. When set, all resource belonging to the project will also be deleted.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>name</code>, will delete a specific version. For projects, since versions do not apply, this is synonym with <code>id</code>.</li> </ul> <p>Delete a project and all of its resources: <pre><code>dhcli delete -c projects my-project\n</code></pre></p> <p>Delete an artifact, skip confirmation: <pre><code>dhcli delete -p my-project -y artifacts my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#run","title":"<code>run</code>","text":"<p>Creates a run of the specified function. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name_of_function</code> Ignored if <code>id</code> is specified, otherwise mandatory and will run the latest version of the function.</li> <li><code>-i id</code> Alternative to <code>-n name_of_function</code>.</li> <li><code>-f yaml_file_path</code> Optional, can contain additional parameters for the run.</li> <li><code>task</code> Mandatory. Must contain a valid task, such as <code>python+build</code>.</li> </ul> <p>Create a <code>python+build</code> run of latest version of <code>my-function</code>: <pre><code>dhcli run -p my-project -n my-function python+build\n</code></pre></p>"},{"location":"cli/commands/#log","title":"<code>log</code>","text":"<p>Returns the logs of the specified resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-c container</code> Optional, ID of the container to read logs from. If not specified, the main container will be picked.</li> <li><code>-f</code> Optional, will update the printed logs periodically if set.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Retrieve and follow logs from the main container of a run: <pre><code>dhcli log -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#metrics","title":"<code>metrics</code>","text":"<p>Returns metrics for the specified resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-c container</code> Optional, ID of the container to read metrics from. If not specified, the main container will be picked.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Retrieve metrics from the main container of a run: <pre><code>dhcli metrics -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#stop","title":"<code>stop</code>","text":"<p>Stops a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Stop a run: <pre><code>dhcli stop -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#resume","title":"<code>resume</code>","text":"<p>Resumes a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Mandatory</li> </ul> <p>Resume a run: <pre><code>dhcli resume -p my-project run my-run-id\n</code></pre></p>"},{"location":"cli/commands/#download","title":"<code>download</code>","text":"<p>Downloads a resource. It takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name</code> Alternative to <code>id</code>, will download latest version.</li> <li><code>-o output_filename_or_dir</code> Optional, base directory for downloaded resources, will be created if missing.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Alternative to <code>-n name</code>.</li> </ul> <p>Download an artifact: <pre><code>dhcli download -p my-project -o downloaded_artifacts artifact my-artifact-id\n</code></pre></p>"},{"location":"cli/commands/#upload","title":"<code>upload</code>","text":"<p>Uploads a resource. takes the following parameters:</p> <ul> <li><code>-e environment</code> Optional</li> <li><code>-p project</code> Mandatory</li> <li><code>-n name</code> Must be specified when creating a new artifact.</li> <li><code>-f input_filename_or_dir</code> Mandatory, path to input file or directory.</li> <li><code>resource</code> Mandatory</li> <li><code>id</code> Must be omitted for new artifacts; used to update an existing artifact.</li> </ul> <p>Upload an artifact: <pre><code>dhcli upload -p my-project -f artifacts/artifact.csv artifact -n my-artifact\n</code></pre></p>"},{"location":"components/cli/","title":"CLI","text":"<p>A command-line interface (CLI) is available, allowing access to certain functionalities of the platform remotely.</p>"},{"location":"components/cli/#installation","title":"Installation","text":""},{"location":"components/cli/#linux","title":"Linux","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli</code> file, open a shell and access this path. Run <code>dhcli -h</code> for a list of available commands.</p>"},{"location":"components/cli/#macos","title":"MacOS","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli</code> file, open a terminal and access this path. Run <code>./dhcli -h</code> for a list of available commands.</p> <p>Alternatively, the CLI can be installed using Homebrew Tap:</p> <pre><code>brew tap scc-digitalhub/digitalhub-cli https://github.com/scc-digitalhub/digitalhub-cli\nbrew install dhcli\n</code></pre>"},{"location":"components/cli/#windows","title":"Windows","text":"<p>Download the CLI for your OS and architecture from the releases page. Extract the archive to find the <code>dhcli.exe</code> file, open the command prompt and access this path. Run <code>./dhcli -h</code> for a list of available commands.</p>"},{"location":"components/cli/#cli-usage","title":"CLI usage","text":"<p>Run commands</p> <p>Depending on the shell you are using, you may have to run the CLI with <code>./dhcli</code>.</p> <p>The standard use flow of the CLI is as follows:</p> <ol> <li>Register your instance's configuration. This creates a <code>.dhcore.ini</code> file in your home directory (or, if not possible, in the current one), where the configuration will be stored, to be used and updated by subsequent commands.</li> </ol> <pre><code>dhcli register http://localhost:8080\n</code></pre> <ol> <li>Log in. This will open a tab in your Internet browser, where you will have to carry out the log in procedure.</li> </ol> <pre><code>dhcli login\n</code></pre> <ol> <li>If you wish to install the python packages, <code>init</code> will do so, matching the platform's version.</li> </ol> <pre><code>dhcli init\n</code></pre> <p>In-detail descriptions of available commands can be found in this dedicated section.</p>"},{"location":"components/dashboard/","title":"Landing Page","text":"<p>The landing page is a central access point to reach a number of tools that are automatically run when the platform is installed. It provides access to the platform components and to the monitoring subsystem of the platform.</p> <p></p> <p>Components</p> <ul> <li>Coder, Tool for managing interactive workspaces</li> <li>DH Core Console, UI for the platform management</li> <li>KRM, or Kubernetes Resource Manager, is the tool for organizing and managing standard and custom Kubernetes resources</li> <li>Kubeflow, a tool for ML pipelines on Kubernetes</li> <li>MinIO, an S3-compatible object datalake UI</li> </ul>"},{"location":"components/dh_console/","title":"Core UI","text":"<p>The Core console is a front-end application backed by the Core API. It provides a management interface  for the organization and operations over the Data Science Projects and the associated entities, such as:</p> <ul> <li>functions of various runtimes (see the Functions and Runtimes section for details), as well as their executions (runs) grouped by the corresponding operations (tasks)</li> <li>workflows - composite pipelines combining executions of different functions</li> <li>dataitems - structured Data Items managed by the project</li> <li>artifacts - unstructured files related and maanged by the project</li> <li>models - versioned ML Model artifacts with their metrics and metadata (see ML Models section for details)</li> </ul> <p>When you access the console, you land to the project management page, where you can create or delete projects.</p> <p>Note that all the functionality that is performed via UI console through the Core API can be also performed using the platform management Python SDK reflecting management of the same platform entities.</p>"},{"location":"components/dh_console/#create-a-project","title":"Create a Project","text":"<p>Start by clicking the <code>CREATE A NEW PROJECT</code> button.</p> <p></p> <p>Fill the form's properties. </p> <p>Following the selection of a project, you can get an overview of the associated objects on its dashboard and manage them on their dedicated pages.</p>"},{"location":"components/dh_console/#dashboard","title":"Dashboard","text":"<p>The console dashboard shows the resources that have been created with a series of cards and allows you to quickly access them. You can see the runs performed and their respective status, as well as artifacts, data items and functions. </p>"},{"location":"components/dh_console/#objects","title":"Objects","text":"<p>Through the console it is also possible to manage directly the entities related to the project and perform different operations over those. This amounts not only to CRUD (create, update, delete, and read) operations, but also track relations, view detailed metadata and versions, execute functions and pipelines, etc. </p>"},{"location":"components/dh_console/#functions","title":"Functions","text":"<p>Functions define the executable procedures implemented in various ways that can be run by the platform. In console it is possible to create new functions selecting the corresponding runtime and, based on that, providing its specification, e.g., source code. For each function the console lists the different versions of the function, the specification and the code (if available) of the function, as well as different tasks that can be performed over the function in the corresponding runtime. For example, in case of Python runtime, it is possible to <code>build</code> function (generating the corresponding Docker image and caching in the regsitry), to run the function as <code>job</code> (to be executed on the Kubernetes), or to expose the function as a service, that is to <code>serve</code> the function. </p> <p></p> <p>Within the tab corresponding to the specific task, it is possible to access the list of runs executed over the function, the status of execution, the execution log. It is also possible to create new run of the task, defining the specific parameters and configurations for the run. </p>"},{"location":"components/dh_console/#workflows","title":"Workflows","text":"<p>Workflows represent a composition of function executions that is run over the platform, specifying their dependencies (in terms of data and order). This allows for creating complex pipelines for AI/ML and data operations. Currently, the implementation of the workflow relies on the Kubeflow Pipelines framework, that in turn relies on Kubernetes Argo Workflows so that each step of the workflow is executed as a single Kubernetes Job. </p> <p>From the console it is possible to define a new workflow providing the code of the pipeline and run the <code>pipeline</code> task. As in case of the function runs, the execution of the pipeline is being tracked, as well as the progress of single steps, and the corresponding log. </p>"},{"location":"components/dh_console/#dataitems","title":"Dataitems","text":"<p>Through Dataitems the project may define the relevant structured and semi-structured datasets. Dataset may created manually, starting from a reference to an URL of the file or DB table, or may be produced as a result of some data transformation function execution. As in case of the functions, the dataitems are equipped with the relevant metadata (e.g., creation and changes, tags, ownership, etc). Furthermore, the datasets structured as tables (i.e., <code>table</code> kind datasets) are equipped with the derived schemas and profiling and data preview.</p> <p>Through the console it is possible to manage the datasets, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#artifacts","title":"Artifacts","text":"<p>Similar to dataitems, Artifacts it is possible to explicitly capture the relevant unstructured objects and files. Artifacts may be of arbitrary type, and equipped with a generic metadata properties.  </p> <p>Through the console it is possible to manage the artifacts, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#ml-models","title":"ML Models","text":"<p>ML Models represent a specific type of artifacts, which are produced by the AI tranining activites and represent the datasets used for inference operations. While managed in the same manner as other types of entities, ML Models may have a specific set of metadata and specification attributes, such model kind, metrics, algorithm and framework specification, etc. </p> <p>ML Models are further used by the inference services.</p>"},{"location":"components/dh_console/#secrets","title":"Secrets","text":"<p>When executing operations with the platform, the execution might need access to some sensitive values, for example to access data residing on a data-store that requires credentials (such as a private S3 bucket), access a private repository, or many other similar needs.</p> <p>The platform provides the functionality to manage these values, reffered to as Secrets, both through UI and SDK, where it is possible to associate the key-value pair to the project. The data is managed as Kubernetes secrets and is embedded in the execution of a run that relies on that.</p> <p>The management of secrets allows through the console to create, delete, and read the secret values.</p>"},{"location":"components/dh_console/#versioning","title":"Versioning","text":"<p>All entities operated by Core are versioned. When you view the details of an object, all of its versions are listed and browsable. Moreover, when you view a dataitem, its schema and data preview are available.</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with your Coder username and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from [Coder, using its template]../tasks/workspaces.md). After launching it from Coder you can access Grafana on Grafana UI (http://nodeipaddress:30110).</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> </li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> </li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/kubeai/","title":"KubeAI","text":"<p>KubeAI is an AI Inference Operator for deploy and scale machine learning models on Kubernetes currently built for LLMs, embeddings, and speech-to-text.</p> <p>KubeAI does not depend on other systems like Istio &amp; Knative (for scale-from-zero), or the Prometheus metrics adapter (for autoscaling). This allows KubeAI to work out of the box in almost any Kubernetes cluster. Day-two operations is greatly simplified as well - don't worry about inter-project version and configuration mismatches.</p> <p>To accomplish this, KubeAI as a part of the platform is offered with these primary sub-components:</p> <ol> <li> <p>The model proxy: the KubeAI proxy provides an OpenAI-compatible API. Behind this API, the proxy implements a prefix-aware load balancing strategy that optimizes for KV the cache utilization of the backend serving engines (i.e. vLLM). The proxy also implements request queueing (while the system scales from zero replicas) and request retries (to seamlessly handle bad backends).</p> </li> <li> <p>The model operator: the KubeAI model operator manages backend server Pods directly. It automates common operations such as downloading models, mounting volumes, and loading dynamic LoRA adapters via the KubeAI Model CRD. The KubeAI operator abstract the concepts of specific implementations and tasks providing a common specification model for defining models under different engines (OLlama, vLLM, Infinity, FasterWhisper).</p> </li> <li> <p>Open WebUI component - an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution. Its interface is integrated with the SSO authentication adopted by the platform and provides the management tools to expose and test the AI models. </p> </li> </ol>"},{"location":"components/kubeai/#exposing-ai-models-with-kubeai","title":"Exposing AI models with KubeAI","text":"<p>Cirrently, KubeAI allows for serving the LLM models for text generation and text embedding and speech-to-text processing, in line with the OpenAI specification. To expose a model, it is possible to use Python SDK or directly the Core UI. More specifically,</p> <ul> <li>it is necessary to define the corresponding KubeAI function (of <code>kubeai-text</code> or <code>kubeai-speech</code> kinds), define a set of parameters, like the task (feature) to perform, reference to an engine (e.g., vLLM or OLlama for text generation), reference to a model to expose (HuggingFace id, OLlama model, or S3 resource reference) and model name.</li> <li>activate serving operation on the function passing extra parameters (like engine arguments, HW resource profile and number of GPUs, secrets and/or envirnoment variables).</li> </ul> <p>Once deployed, the serving run provides the information about the model name (which is randomized to avoid naming clashes), exposed endpoints and status. Depending on your environment, the deployment may require some time for the model to be operative.</p> <p></p> <p>It is also possible to access the exposed model API through the KubeAI model proxy. The proxy exposes the OpenAI-compatible endpoints in function of the specified task. See the KubeAI documentation on how to use and access the models, to integrate this with the client libraries and applications. </p>"},{"location":"components/kubeai/#testing-and-using-the-models-with-open-webui","title":"Testing and using the models with Open WebUI","text":"<p>To simplify the testing and usage of the model, we provide Open WebUI instance integrated with the platform and configured to access the KubeAI models. </p> <p></p> <p>The possibility to access and test the models with Open WebUI depends, however, on the configuration of the latter. By default, the installation foresees two roles - admin and user. The former can configure the tool functionality, users, groups, and the model visibility (public, private or group-scoped). The deployed model are not available to the users unless the admin changes their visibility.</p>"},{"location":"components/kubeai/#see-also","title":"See Also","text":"<ul> <li>How to manage LLM Models with KubeAI Runtime</li> <li>How to manage speech-to-text models with KubeAI Runtime</li> <li>Model Serving Runtime Reference</li> </ul>"},{"location":"components/kubeflow/","title":"KubeFlow Pipelines","text":"<p>Kubeflow Pipelines makes part of the Kubeflow platform and allows for organizing workflows out of single tasks performed as Kubernetes Jobs via Argo Workflows. Kubeflow Pipelines comes with its own DSL specification on top of Python, which is compiled into a workflow definition ready for execution in Kubernetes. In this way wach task, its resources, dependencies, etc may be configured indipendently; the management and tracking is performed by the Kubeflow Pipelines component, equipped also with the Web-based UI for monitoring.  </p> <p>The platform used Kubeflow pipelines to implement the composite pipelines through its Core orchestrator component and UI.</p> <p>Currently, version v1 of the Kubeflow Pipelines is used for the compatibility purposes. The definition of the KFP workflows is provided in the corresponding KFP Runtime section.</p> <p>How to access</p> <p>Kubeflow Pipelines UI may be accessed from the dashboard. From its interface, you will be able to monitor the deployed workflows and their executions.</p>"},{"location":"components/kubeflow/#resources","title":"Resources","text":""},{"location":"components/kubeflow/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/resourcemanager/","title":"Kubernetes Resource Manager","text":"<p>Kubernetes Resource Manager (KRM) is an application to manage several types of Kubernetes resources:</p> <ul> <li>Custom resources</li> <li>Services</li> <li>Deployments</li> <li>Volumes</li> <li>Jobs</li> </ul> <p>It consists in a back-end, written in Java, which connects to the Kubernetes API to perform actions on resources, and a front-end, written in React and based on React-admin.</p> <p>Instructions on how to install and start an instance can be found on the repository.</p>"},{"location":"components/resourcemanager/#standard-kubernetes-resources","title":"Standard Kubernetes Resources","text":"<p>With KRM you can control the main Kubernetes resources (e.g., services, deployments), manage Persistent Volume Claims, and access secrets. Click the corresponding button in the left menu, and view the details of one item by clicking its Show button.</p>"},{"location":"components/resourcemanager/#custom-resources","title":"Custom resources","text":"<p>Custom resources can be viewed, created, edited and deleted through the use of the interface. </p> <p>If you don't see a specific kind of custom resource listed to the left, it means neither Kubernetes nor KRM contain a schema for it. A schema is required so that the application may understand and describe the related resources.</p> <p>If some resources already exist, they will immediately be visible.</p>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"runtimes/container/","title":"Container runtime","text":"<p>The Container runtime allows you to create deployments, jobs and services on Kubernetes.</p>"},{"location":"runtimes/container/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK container runtime documentation for more information.</p>"},{"location":"runtimes/dbt/","title":"DBT runtime","text":"<p>The DBT runtime allows you to run DBT transformations on your data. It is a wrapper around the DBT CLI tool.</p>"},{"location":"runtimes/dbt/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK DBT runtime documentation for more information.</p>"},{"location":"runtimes/hera/","title":"Hera Pipelines Runtime","text":"<p>The Hera runtime allows you to run workflows within the platform.</p>"},{"location":"runtimes/hera/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK Hera runtime documentation for more information.</p>"},{"location":"runtimes/kfp_pipelines/","title":"KFP Pipelines Runtime","text":"<p>The kfp runtime allows you to run workflows within the platform.</p>"},{"location":"runtimes/kfp_pipelines/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK KFP runtime documentation for more information.</p>"},{"location":"runtimes/modelserve/","title":"Modelserve runtime","text":"<p>The Modelserve runtime allows you to deploy ML models on the platform.</p>"},{"location":"runtimes/modelserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK modelserve runtime documentation for more information.</p>"},{"location":"runtimes/python/","title":"Python","text":"<p>The python runtime allows you to run generic python function within the platform.</p>"},{"location":"runtimes/python/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK python runtime documentation for more information.</p>"},{"location":"scenarios/dremio_grafana/scenario/","title":"Data transformation and usage with Dremio and Grafana","text":"<p>In this scenario we will learn how to use Dremio to transform data and create some virtual datasets on top of it. Then, we will visualize the transformed data in a dashboard created with Grafana, by importing a template. For this template to work with minimal changes, make sure you match the naming of entities indicated throughout the tutorial.</p> <p>In order to collect the initial data and make it accessible to Dremio, we will follow the first step of the ETL scenario, in which we download some traffic data and store it in the DigitalHub datalake.</p>"},{"location":"scenarios/dremio_grafana/scenario/#collect-the-data","title":"Collect the data","text":"<p>Collect the data</p> <p>The process of collecting data is the same as described in the ETL scenario introduction and Collect the data pages.</p> <ul> <li>Access Jupyter from your Coder instance and create a new notebook using the <code>Python 3 (ipykernel)</code> kernel</li> <li> <p>Set up the environment and create a project named <code>demo-etl</code> <pre><code>import digitalhub as dh\nimport os\n</code></pre> <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p> </li> <li> <p>Create the <code>src</code> folder, define the download function and register it <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre> <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef downloader(url):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(file_format='csv',sep=\";\")\n    return df\n</code></pre> <pre><code>func = project.new_function(\n                         name=\"download-data\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_10\",\n                         code_src=\"src/download-data.py\",\n                         handler=\"downloader\")\n</code></pre></p> </li> <li> <p>Set the URL and execute the function: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi= project.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n</code></pre> <pre><code>run = func.run(action=\"job\", inputs={'url':di.key}, outputs={\"dataset\": \"dataset\"}, local_execution=True)\n</code></pre></p> </li> </ul>"},{"location":"scenarios/dremio_grafana/scenario/#access-the-data-from-dremio","title":"Access the data from Dremio","text":"<p>Access Dremio from your Coder instance or create a new Dremio workspace. You should see S3 already configured as an object storage and you should find the downloaded data in a .parquet file at the path <code>S3/datalake/demo-etl/dataitem/dataset/0eed9ced-5f04-4f12-8494-763926070835/dataset.parquet</code>. The auto-generated <code>id</code> before <code>/dataset.parquet</code> will be different for you.</p> <p>Click on the file to open its Dataset Settings, verify that the selected format is <code>Parquet</code> and click Save. It will be saved as a Dremio dataset, so that it can be queried.</p> <p>Now you can run SQL queries against the dataset. Try the following (update the <code>id</code> to match your own):</p> <pre><code>SELECT *\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\nORDER BY data, \"codice spira\"\n</code></pre> <p>Create a new Dremio space named <code>demo_etl</code>. We will create three virtual datasets and save them here.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-measurement-data","title":"Extract measurement data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic measurements to save them as a separate dataset:</p> <pre><code>SELECT \"dataset.parquet\".data, \"dataset.parquet\".\"codice spira\", \"00:00-01:00\", \"01:00-02:00\", \"02:00-03:00\", \"03:00-04:00\", \"04:00-05:00\", \"05:00-06:00\", \"06:00-07:00\", \"07:00-08:00\", \"08:00-09:00\", \"09:00-10:00\", \"10:00-11:00\", \"11:00-12:00\", \"12:00-13:00\", \"13:00-14:00\", \"14:00-15:00\", \"15:00-16:00\", \"16:00-17:00\", \"17:00-18:00\", \"18:00-19:00\", \"19:00-20:00\", \"20:00-21:00\", \"21:00-22:00\", \"22:00-23:00\", \"23:00-24:00\"\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as (top right), select Save View as..., name the new dataset <code>misurazioni</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-traffic-sensors-data","title":"Extract traffic sensors data","text":"<p>From the SQL runner, execute the following query, which will extract the traffic sensors data (e.g. their geographical position) as a separate dataset:</p> <pre><code>SELECT DISTINCT \"dataset.parquet\".\"codice spira\", \"dataset.parquet\".tipologia, \"dataset.parquet\".id_uni, \"dataset.parquet\".codice, \"dataset.parquet\".Livello, \"dataset.parquet\".\"codice arco\", \"dataset.parquet\".\"codice via\", \"dataset.parquet\".\"Nome via\", \"dataset.parquet\".\"Nodo da\", \"dataset.parquet\".\"Nodo a\", \"dataset.parquet\".stato, \"dataset.parquet\".direzione, \"dataset.parquet\".angolo, \"dataset.parquet\".longitudine, \"dataset.parquet\".latitudine, \"dataset.parquet\".geopoint\nFROM S3.datalake.\"demo-etl\".dataitem.dataset.\"0eed9ced-5f04-4f12-8494-763926070835\".\"dataset.parquet\"\n</code></pre> <p>Select Save View as... again (do not overwrite the previous one), name the new dataset <code>spire</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#transform-hourly-measurements-into-daily-measurements","title":"Transform hourly measurements into daily measurements","text":"<p>From the SQL runner, execute the following query, which will sum the measurement columns, each corresponding to an hour, to obtain the daily value and save it as a new dataset:</p> <pre><code>SELECT data, \"codice spira\", \"00:00-01:00\"+\"01:00-02:00\"+\"02:00-03:00\"+\"03:00-04:00\"+\"04:00-05:00\"+\"05:00-06:00\"+\"06:00-07:00\"+\"07:00-08:00\"+\"08:00-09:00\"+\"09:00-10:00\"+\"10:00-11:00\"+\"11:00-12:00\"\n+\"12:00-13:00\"+\"13:00-14:00\"+\"14:00-15:00\"+\"15:00-16:00\"+\"16:00-17:00\"+\"17:00-18:00\"+\"18:00-19:00\"+\"19:00-20:00\"+\"20:00-21:00\"+\"21:00-22:00\"+\"22:00-23:00\"+\"23:00-24:00\" AS totale_giornaliero\nFROM (\n  SELECT * FROM \"demo_etl\".misurazioni\n) nested_0;\n</code></pre> <p>Select Save View as... again (do not overwrite the previous one), name the new dataset <code>misurazioni_giornaliere</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#connect-grafana-to-dremio","title":"Connect Grafana to Dremio","text":"<p>Access Grafana from your Coder instance or create a new Grafana workspace. Open the left menu and navigate to Connections &gt; Data Sources. Add a new <code>Dremio</code> data source configured as follows:</p> <ul> <li>Name: <code>Dremio</code></li> <li>URL: the Internal Endpoint you see on Coder for your Dremio workspace</li> <li>User: <code>admin</code></li> <li>Password: <code>&lt;dremio_password_set_on_coder&gt;</code></li> </ul> <p>Now you can create a dashboard to visualize Dremio data. An example dashboard is available as a JSON file at the <code>user/examples/dremio_grafana</code> path within the repository of this documentation.</p> <p>Navigate to Dashboards from the left menu, expand the New button on the top right and select Import. Once imported, you will need to update the <code>datasource.uid</code> field, which holds a reference to the Dremio data source in your Grafana instance, throughout the JSON model.</p> <p>To obtain your ID easily, navigate to Connections &gt; Data Sources, select the Dremio source, and copy the ID from the page's URL:</p> <pre><code>https://&lt;grafana_host&gt;/connections/datasources/edit/&lt;YOUR_DATASOURCE_ID&gt;\n</code></pre> <p>Then, go back to Dashboards, open your dashboard, open the Dashboard settings (cog icon in the top toolbar) and select JSON Model from the left. There will be a number of instances where you have to replace the ID, referenced by <code>datasource.uid</code>. When done, click Save changes and return to your dashboard.</p> <p>The dashboard includes three panels: a map of the traffic sensors, a table with the daily number of vehicles registered by each sensor and a graph of the vehicles registered monthly.</p> <p></p> <p>We can now use the dashboard to explore the data. We can either interact with the map to get the information related to each sensor, or use the dashboard filters to select different time ranges and analyze traffic evolution over time.</p>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Define a function for downloading data as-is and persisting it in the data-lake:</p> <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef downloader(url):\n    return url.as_df(file_format='csv',sep=\";\")\n</code></pre> <p>Register the function in Core:</p> <pre><code>func = project.new_function(name=\"download-data\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\",\n                            code_src=\"src/download-data.py\",\n                            handler=\"downloader\")\n</code></pre> <p>This code creates a new function definition that uses Python runtime (versione 3.9) pointing to the created file and the handler method that should be called.</p> <p>For the function to be executed, we need to pass it a reference to the data item. Let us create and register the corresponding data item:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi = project.new_dataitem(name=\"url_data_item\",\n                          kind=\"table\",\n                          path=URL)\n</code></pre> <p>It is also possible to see the data item directly in the Core UI.</p> <p>Then, execute the function (locally) as a single job. Note that it may take a few minutes.</p> <pre><code>run = func.run(\"job\",\n               inputs={'url': di.key},\n               wait=True)\n</code></pre> <p>Note that the <code>inputs</code> map should contain the references to the project entities (e.g., artifacts, dataitems, etc), while in order to pass literal values to the function execution it is necessary to use <code>parameters</code> map.</p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. The name of the artifact will be defined according to the mapping specified in <code>@handler</code> annotation. To get the value of the artifact we can refer to it by the output name:</p> <pre><code>dataset_di = project.get_dataitem('dataset')\n</code></pre> <p>Load the data item and then into a data frame:</p> <pre><code>dataset_df = dataset_di.as_df()\n</code></pre> <p>Run <code>dataset_df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API:</p> <pre><code>%%writefile 'src/api.py'\n\ndef init_context(context, dataitem):\n    di = context.project.get_dataitem(dataitem)\n    df = di.as_df()\n    setattr(context, \"df\", df)\n\ndef serve(context, event):\n    df = context.df\n\n    if df is None:\n        return \"\"\n\n    # mock REST api\n    fields = event.fields\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields[\"page\"])\n\n    if \"size\" in fields:\n        pageSize = int(fields[\"size\"])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    return {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n</code></pre> <p>Register the function:</p> <pre><code>api_func = project.new_function(name=\"api\",\n                                kind=\"python\",\n                                python_version=\"PYTHON3_10\",\n                                code_src=\"src/api.py\",\n                                handler=\"serve\",\n                                init_function=\"init_context\")\n</code></pre> <p>Please note that other than defining the handler method, it is possible to define the <code>init_function</code> to define the preparatory steps.</p> <p>Deploy the function (perform <code>serve</code> action):</p> <pre><code>run_serve_model = api_func.run(\"serve\", init_parameters={\"dataitem\": \"dataset-measures\"}, wait=True)\n</code></pre> <p>Wait till the deployment is complete and the necessary pods and services are up and running.</p> <pre><code>run_serve_model.refresh()\n</code></pre> <p>When done, the status of the run contains the <code>service</code> element with the internal service URL to be used.</p> <pre><code>svc_url = f\"http://{run_serve_model.status.service['url']}/?page=5&amp;size=10\"\n</code></pre> <p>Invoke the API and print its results:</p> <pre><code>res = run_serve_model.invoke(url=svc_url).json()\nprint(res)\n</code></pre> <p>You can also use pandas to load the result in a data frame:</p> <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a simple yet realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell. Alternatively, the final notebook for this scenario can be found in the tutorial repository.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries:</p> <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre> <p>Create a project:</p> <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> <p>Download the file and save it locally:</p> <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre> <p>Use pandas to read the file into a dataframe:</p> <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together:</p> <pre><code>%%writefile \"src/pipeline.py\"\n\nfrom digitalhub_runtime_hera.dsl import step\nfrom hera.workflows import DAG, Parameter, Workflow\n\n\ndef pipeline():\n    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"url\")) as w:\n        with DAG(name=\"dag\"):\n            A = step(\n                template={\"action\": \"job\", \"inputs\": {\"url\": \"{{workflow.parameters.url}}\"}},\n                function=\"download-data\",\n                outputs=[\"dataset\"],\n            )\n            B = step(\n                template={\"action\": \"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n                function=\"process-spire\",\n                inputs={\"di\": A.get_parameter(\"dataset\")},\n            )\n            C = step(\n                template={\"action\": \"job\", \"inputs\": {\"di\": \"{{inputs.parameters.di}}\"}},\n                function=\"process-measures\",\n                inputs={\"di\": A.get_parameter(\"dataset\")},\n                outputs=[\"dataset-measures\"],\n            )\n            A &gt;&gt; [B, C]\n    return w\n</code></pre> <p>Here we use the Hera-based DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method maps to Hera templates and produces an Argo workflow descriptor which performs the remote execution of the corresponding job. Note that the syntax for <code>step</code> is similar to that of function execution.</p> <p>Register the workflow:</p> <pre><code>workflow = project.new_workflow(name=\"pipeline\",\n                                kind=\"hera\",\n                                code_src=\"src/pipeline.py\",\n                                handler=\"pipeline\")\n</code></pre> <p>You MUST build the workflow before running it. This is necessary to compose the Argo descriptor which will be used to execute the workflow:</p> <pre><code>run_build = workflow.run(\"build\", wait=True)\n</code></pre> <p>The Argo descriptor is saved as encoded base64 string into the workflow spec under the build attribute. Once the workflow is built, you can run it, passing the URL key as a parameter:</p> <pre><code>workflow.run(\"pipeline\", parameters={\"url\": di.key}, wait=True)\n</code></pre> <p>It is possible to monitor the execution in the Core console:</p> <p></p> <p>The next section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store:</p> <pre><code>%%writefile \"src/process-spire.py\"\n\nfrom digitalhub_runtime_python import handler\n\nKEYS=['codice spira','longitudine','latitudine',\n      'Livello','tipologia','codice','codice arco',\n      'codice via','Nome via', 'stato','direzione',\n      'angolo','geopoint']\n\n@handler(outputs=[\"dataset-spire\"])\ndef process(project, di):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n    return sdf\n</code></pre> <p>Register the function in Core:</p> <pre><code>process_func = project.new_function(name=\"process-spire\",\n                                    kind=\"python\",\n                                    python_version=\"PYTHON3_10\",\n                                    code_src=\"src/process-spire.py\",\n                                    handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_run = process_func.run(\"job\",\n                               inputs={'di':dataset_di.key},\n                               wait=True)\n</code></pre> <p>The results has been saved as an artifact in the data store:</p> <pre><code>spire_di = project.get_dataitem('dataset-spire')\nspire_df = spire_di.as_df()\n</code></pre> <p>Now you can view the results with <code>spire_df.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot:</p> <pre><code>keys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = dataset_df[columns]\n</code></pre> <p>Derive dataset for recorded traffic within each time slot for each spire:</p> <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function:</p> <pre><code>%%writefile \"src/process-measures.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00',\n        '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00',\n        '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00',\n        '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00',\n        '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00',\n        '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@handler(outputs=[\"dataset-measures\"])\ndef process(project, di):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre> <p>Register it:</p> <pre><code>process_measures_func = project.new_function(name=\"process-measures\",\n                                             kind=\"python\",\n                                             python_version=\"PYTHON3_10\",\n                                             code_src=\"src/process-measures.py\",\n                                             handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_measures_run = process_measures_func.run(\"job\",\n                                                 inputs={'di':dataset_di.key},\n                                                 wait=True)\n</code></pre> <p>Inspect the resulting data artifact:</p> <pre><code>measures_di = project.get_dataitem('dataset-measures')\nmeasures_df = measures_di.as_df()\nmeasures_df.head()\n</code></pre> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/etl/streamlit/","title":"Visualize data with Streamlit","text":"<p>We can take this one step further and visualize our data in a graph using Streamlit, a library to create web apps and visualize data by writing simple scripts. Let's get familiar with it.</p>"},{"location":"scenarios/etl/streamlit/#setup","title":"Setup","text":"<p>From the Jupyter notebook you've been using, write the result of the API call to a file:</p> <pre><code>with open(\"result.json\", \"w\") as file:\n    file.write(res['data'])\n</code></pre> <p>Create the script that Streamlit will run:</p> <pre><code>%%writefile 'streamlit-app.py'\n\nimport pandas as pd\nimport streamlit as st\n\nrdf = pd.read_json(\"result.json\", orient=\"records\")\n\n# Replace colons in column names as they can cause issues with Streamlit\nrdf.columns = rdf.columns.str.replace(\":\", \"\")\n\nst.write(\"\"\"My data\"\"\")\nst.line_chart(rdf, x=\"codice spira\", y=\"value\")\n</code></pre>"},{"location":"scenarios/etl/streamlit/#launch-app","title":"Launch app","text":"<p>In a new code cell, run the following to install Streamlit in the workspace. Note that if you want to run a shell command from Jupyter cell, prepone it with <code>!</code>. If you want to install a package in the workspace, prepone <code>pip install</code> with <code>%</code>.</p> <pre><code>%pip install streamlit\n</code></pre> <p>Similarly, run the following command. This will start hosting the Streamlit web app, so the cell will remain running. The <code>browser.gatherUsageStats</code> flag is set to <code>false</code> because, otherwise, Streamlit will automatically gather usage stats and print a warning about it.</p> <pre><code>!streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Next, go to your Coder instance and access the Jupyter workspace you've been using.</p> <p></p> <p></p> <p>Click on Ports, type <code>8501</code> (Streamlit's default port), then click the button next to it. It will open a tab to the Streamlit app, where you can visualize data!</p> <p></p> <p>The graph we displayed is very simple, but you are welcome to experiment with more Streamlit features. Don't forget to stop the above code cell, to stop the app.</p> <p>Connect to workspace remotely</p> <p>Alternatively to running shell commands from Jupyter and port-forwarding through the Coder interface, you could connect your local shell to the workspace remotely. You do not need to do this if you already used the method above.</p> <p>Login to Coder with the following command. A tab will open in your browser, containing a token you must copy and paste to the shell (it may ask for your credentials, if your browser isn't already logged in).</p> <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> <p>With this, your shell is authenticated to the Coder instance, and the following command will be able to connect your shell to the workspace remotely, while tunneling port 8501:</p> <pre><code>ssh -L 8501:localhost:8501 coder.my-jupyter-workspace\n</code></pre> <p>Install streamlit:</p> <pre><code>pip install streamlit\n</code></pre> <p>Run the app:</p> <pre><code>streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Access <code>localhost:8501</code> on your browser to view the app!</p>"},{"location":"scenarios/etl/streamlit/#as-docker-container","title":"As Docker container","text":"<p>Streamlit apps can be run as Docker containers. For this section, we will run the same application locally as a container, so you will need either Docker or Podman installed on your machine. Instructions refer to Docker, but if you prefer to use Podman, commands are equivalent: simply replace instances of <code>docker</code> with <code>podman</code>.</p> <p>Download the <code>result.json</code> file obtained previously on your machine, as we will need its data for the app. Also download the <code>streamlit-app.py</code> file.</p> <p>Create a file named <code>Dockerfile</code> and paste the following contents in it:</p> <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY streamlit-app.py streamlit-app.py\nCOPY result.json result.json\n\nRUN pip3 install altair pandas streamlit\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit-app.py\", \"--browser.gatherUsageStats=false\"]\n</code></pre> <p>The Dockerfile describes how the image for the container will be built. In short, it installs the required libraries, copies the files you downloaded, then launches the Streamlit script.</p> <p>Make sure the three files are in the same directory, then open a shell in it and run the following, which builds the Docker image:</p> <pre><code>docker build -t streamlit .\n</code></pre> <p>Once it's finished, you can verify the image exists with:</p> <pre><code>docker images\n</code></pre> <p>Now, run a container:</p> <pre><code>docker run -p 8501:8501 --name streamlit-app streamlit\n</code></pre> <p>Port already in use</p> <p>If you run into an error, it's likely that you didn't quit the remote session you opened while following the previous section, meaning port 8501 is already in use.</p> <p>Open your browser and visit <code>localhost:8501</code> to view the data!</p> <p>To stop the container, simply press Ctrl+C, then run the following to remove the container:</p> <pre><code>docker rm -f streamlit-app\n</code></pre>"},{"location":"scenarios/etl-core/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute the ETL step:</p> <pre><code>%%writefile \"src/dbt_pipeline.py\"\n\nfrom digitalhub_runtime_hera.dsl import step\nfrom hera.workflows import DAG, Parameter, Workflow\n\n\ndef pipeline():\n    with Workflow(entrypoint=\"dag\", arguments=Parameter(name=\"employees\")) as w:\n        with DAG(name=\"dag\"):\n            A = step(\n                template={\n                    \"action\": \"transform\",\n                    \"inputs\": {\"employees\": \"{{workflow.parameters.employees}}\"},\n                    \"outputs\": {\"output_table\": \"department-50\"},\n                },\n                function=\"transform-employees\",\n            )\n    return w\n</code></pre> <p>Here we use the Hera-based DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method maps to Hera templates and produces an Argo workflow descriptor which performs the remote execution of the corresponding job. Note that the syntax for <code>step</code> is similar to that of function execution.</p> <p>Register the workflow:</p> <pre><code>workflow = project.new_workflow(name=\"pipeline_dbt\",\n                                kind=\"hera\",\n                                code_src=\"src/dbt_pipeline.py\",\n                                handler=\"pipeline\")\n</code></pre> <p>You MUST build the workflow before running it. This is necessary to compose the Argo descriptor which will be used to execute the workflow:</p> <pre><code>run_build = workflow.run(\"build\", wait=True)\n</code></pre> <p>The Argo descriptor is saved as encoded base64 string into the workflow spec under the build attribute. Once the workflow is built, you can run it, passing the URL key as a parameter:</p> <pre><code>workflow.run(\"pipeline\", parameters={\"employees\": di.key}, wait=True)\n</code></pre> <p>It is possible to monitor the execution in the Core console.</p>"},{"location":"scenarios/etl-core/scenario/","title":"ETL with digitalhub-core and DBT scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding organizations, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. Alternatively, the final notebook for this scenario can be found in the tutorial repository.</p>"},{"location":"scenarios/etl-core/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-dbt-ci\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl-core/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The DBT runtime will use the dataitem specifications to fetch the data and perform the <code>transform</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees-dbt\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/etl-core/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a tranformation on data with DBT. Our function will be an SQL query that selects all the employees of department 60.</p> <pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '50'\n\"\"\"\n</code></pre> <p>We create the function from the project object:</p> <pre><code>function = project.new_function(name=\"transform-employees\",\n                                kind=\"dbt\",\n                                code=sql)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>dbt</code>.</li> <li><code>code</code> contains the code that is the SQL we'll execute in the function.</li> </ul>"},{"location":"scenarios/etl-core/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>transform</code>)</li> <li>the inputs map the refereced table in the DBT query (<code>{{ ref('employees') }}</code>) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be <code>department-50</code> and will be the sql query table name result and the output dataitem name.</li> </ul> <pre><code>run = function.run(\"transform\",\n                   inputs={\"employees\": di.key},\n                   outputs={\"output_table\": \"department-50\"},\n                   wait=True)\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling <code>run.refresh()</code> will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/etl-core/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. We can fetch the output table and explore it with <code>pandas</code>.</p> <pre><code>df = run.output('department-50').as_df()\ndf.head()\n</code></pre> <p>In the next section, we will see how to convert this example in a workflow.</p>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function: we should define the inference operation and the initialization operation where the model is loaded.</p> <p>Create a model serving function and provide the model:</p> <pre><code>%%writefile \"src/serve_darts_model.py\"\nimport json\nfrom zipfile import ZipFile\n\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nfrom darts.metrics import mae, mape, smape\nfrom darts.models import NBEATSModel\nfrom digitalhub_runtime_python import handler\n\ndef init_context(context, model_key):\n    \"\"\"\n    Initialize serving context by loading the trained model\n    \"\"\"\n    model = context.project.get_model(model_key)\n    path = model.download()\n    local_path_model = \"extracted_model/\"\n\n    # Extract model from zip file\n    with ZipFile(path, \"r\") as zip_ref:\n        zip_ref.extractall(local_path_model)\n\n    # Load the NBEATS model\n    input_chunk_length = 24\n    output_chunk_length = 12\n    name_model_local = local_path_model + \"predictor_model.pt\"\n    mm = NBEATSModel(input_chunk_length, output_chunk_length).load(name_model_local)\n\n    setattr(context, \"model\", mm)\n\n\ndef serve_predictions(context, event):\n    \"\"\"\n    Serve time series predictions via REST API\n    \"\"\"\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n\n    context.logger.info(f\"Received event: {body}\")\n    inference_input = body[\"inference_input\"]\n\n    # Convert input to Darts TimeSeries format\n    pdf = pd.DataFrame(inference_input)\n    pdf[\"date\"] = pd.to_datetime(pdf[\"date\"], unit=\"ms\")\n\n    ts = TimeSeries.from_dataframe(pdf, time_col=\"date\", value_cols=\"value\")\n\n    # Make predictions\n    output_chunk_length = 12\n    result = context.model.predict(n=output_chunk_length * 2, series=ts)\n\n    # Convert result to JSON format\n    jsonstr = result.pd_dataframe().reset_index().to_json(orient=\"records\")\n    return json.loads(jsonstr)\n</code></pre> <p>Register it:</p> <pre><code>func = project.new_function(name=\"serve-darts-model\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_10\",\n                            code_src=\"src/serve_darts_model.py\",\n                            handler=\"serve_predictions\",\n                            init_function=\"init_context\")\n</code></pre> <p>Given the dependencies, it is better to have the image ready, using <code>build</code> action of the function:</p> <pre><code>run_build_model_serve = func.run(\"build\",\n                                 instructions=[\"pip3 install torch'&lt;2.6.0' darts==0.30.0 patsy\"],\n                                 wait=True)\n</code></pre> <p>Now we can deploy the function:</p> <pre><code>serve_run = serve_func.run(\"serve\", init_parameters={\"model_key\": model.key}, labels=[\"time-series-service\"], wait=True)\n</code></pre> <p>Install locally the dependencies:</p> <pre><code># Install darts locally for testing (if not already installed)\n%pip install darts==0.30.0 torch'&lt;2.6.0' --quiet\n</code></pre> <p>Create a test input:</p> <pre><code>import json\nfrom datetime import datetime\nfrom darts.datasets import AirPassengersDataset\n\n# Load test data\nseries = AirPassengersDataset().load()\nval = series[-24:]  # Last 24 points for prediction\njson_value = json.loads(val.to_json())\n\n# Prepare input data in the expected format\ndata = map(\n    lambda x, y: {\"value\": x[0], \"date\": datetime.timestamp(datetime.strptime(y, \"%Y-%m-%dT%H:%M:%S.%f\")) * 1000},\n    json_value[\"data\"],\n    json_value[\"index\"],\n)\ninputs = {\"inference_input\": list(data)}\n</code></pre> <p>And finally test the endpoint:</p> <pre><code>serve_run.invoke(json=inputs).json()\n</code></pre>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"Custom ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying generic machine learning applications using the functionalities of the platform. For this purpose, we use ML algorithms for the time series management provided by the Darts framework.</p> <p>We will train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/ml/intro/#set-up","title":"Set-up","text":"<p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-cml-darts-ci\")\n</code></pre>"},{"location":"scenarios/ml/intro/#create-dir-for-the-code","title":"Create dir for the code","text":"<p>Create a directory for the code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre>"},{"location":"scenarios/ml/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function. For the sake of simplicity, we use predefined \"Air Passengers\" dataset of Darts.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport json\nfrom zipfile import ZipFile\n\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nfrom darts.metrics import mae, mape, smape\nfrom darts.models import NBEATSModel\nfrom digitalhub_runtime_python import handler\n\n\n@handler(outputs=[\"model\"])\ndef train_model(project):\n    \"\"\"\n    Train a NBEATS model on the Air Passengers dataset\n    \"\"\"\n    # Load Air Passengers dataset\n    series = AirPassengersDataset().load()\n    train, test = series[:-36], series[-36:]\n\n    # Configure and train NBEATS model\n    model = NBEATSModel(input_chunk_length=24, output_chunk_length=12, n_epochs=200, random_state=0)\n    model.fit(train)\n\n    # Make predictions for evaluation\n    pred = model.predict(n=36)\n\n    # Save model artifacts\n    model.save(\"predictor_model.pt\")\n    with ZipFile(\"predictor_model.pt.zip\", \"w\") as z:\n        z.write(\"predictor_model.pt\")\n        z.write(\"predictor_model.pt.ckpt\")\n\n    # Calculate metrics\n    metrics = {\"mape\": mape(test, pred), \"smape\": smape(test, pred), \"mae\": mae(test, pred)}\n\n    # Register model in DigitalHub\n    model_artifact = project.log_model(\n        name=\"air-passengers-forecaster\",\n        kind=\"model\",\n        source=\"predictor_model.pt.zip\",\n        algorithm=\"darts.models.NBEATSModel\",\n        framework=\"darts\",\n    )\n    model_artifact.log_metrics(metrics)\n    return model_artifact\n</code></pre> <p>In this code we create a NBEATS DL model, store it locally zipping the content, extract some metrics, and log the model to the platform with a generic <code>model</code> kind.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-time-series-model\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n)\n</code></pre> <p>and run it with build instruction:</p> <pre><code>train_build = train_fn.run(\"build\",\n                           instructions=[\"pip3 install torch'&lt;2.6.0' darts==0.30.0 patsy\"],\n                           wait=True)\n</code></pre> <p>Once the build is completed, launch the training.</p> <pre><code>train_run = train_fn.run(\"job\", wait=True)\n</code></pre> <p>As a result of train execution, a new model is registered in the Core and may be used by different inference operations.</p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlllm/llm/","title":"Managing LLM Models","text":"<p>With the platform it is possible to create and serve LLM HuggingFace-compatible-models. Specifically, it is possible to serve directly the LLM models from the HuggingFace catalogue provided the id of the model or to serve the fine-tuned model from the specified path, such as S3.</p> <p>LLM implementation relies on the KServe LLM runtime and therefore supports one of the corresponding LLM tasks:</p> <ul> <li>Text Generation</li> <li>Text2Text Generation</li> <li>Fill Mask</li> <li>Text (Sequence) Classification</li> <li>Token Classification</li> </ul> <p>Based on the type of the task the API of the exposed service may differ. Generative models (text generation and text2text generation) use OpenAI's Completion and Chat Completion API.</p> <p>The other types of tasks like token classification, sequence classification, fill mask are served using KServe's Open Inference Protocol v2 API.</p> <p>You can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-classification-models","title":"Exposing Predefined Text Classification Models","text":"<p>In case of predefined HuggingFace non-generative model it is possible to use <code>huggingfaceserve</code> runtime to expose the corresponding inference  API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english</code>.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-classification\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=\"huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", wait=True)\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>As in other scenarios, you need to wait a bit for the service to become available. Once the service becomes available, it is possible to make the calls:</p> <pre><code>model_name = \"mymodel\"\njson = {\n    \"inputs\": [\n        {\n            \"name\": \"input-0\",\n            \"shape\": [2],\n            \"datatype\": \"BYTES\",\n            \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"],\n        },\n    ]\n}\n\nllm_run.invoke(model_name=model_name, json=json).json()\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    \"model_name\": \"mymodel\",\n    \"model_version\": None,\n    \"id\": \"cab30aa5-c10f-4233-94e2-14e4bc8fbf6f\",\n    \"parameters\": None,\n    \"outputs\": [\n        {\n            \"name\": \"output-0\",\n            \"shape\": [2],\n            \"datatype\": \"INT64\",\n            \"parameters\": None,\n            \"data\": [1, 0],\n        },\n    ],\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-generation-models","title":"Exposing Predefined Text Generation Models","text":"<p>In case of predefined HuggingFace ngenerative model it is possible to use <code>huggingfaceserve</code> runtime to expose the OpenAI compatible API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://meta-llama/meta-llama-3-8b-instruct</code>.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-generation\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=\"huggingface://meta-llama/meta-llama-3-8b-instruct\")\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", wait=True)\n</code></pre> <p>Please note that in case of protected models (like, e.g., llama models) it is necessary to path the HuggingFace token. For example,</p> <pre><code>hf_token = \"&lt;HUGGINGFACE TOKEN&gt;\"\nllm_run = llm_function.run(action=\"serve\",\n                           profile=\"1xa100\",\n                           envs = [{\"name\": \"HF_TOKEN\", \"value\": hf_token}],\n                           wait=True)\n</code></pre> <p>Deployment time</p> <p>Mind that when requesting a GPU node for the service, it may take some time for the service to start, in some cases up to 10 minutes.</p> <p>As in case of classification models, the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls. For example, for the completion requests:</p> <pre><code>service_url = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nurl = f\"http://{service_url}/openai/v1/completions\"\nmodel_name = \"mymodel\"\njson = {\n    \"model\": model_name,\n    \"prompt\": \"Hello! How are you?\",\n    \"stream\": False,\n    \"max_tokens\": 30\n}\n\nllm_run.invoke(url=url, json=json).json()\n</code></pre> <p>Here the expected output should have the following form:</p> <pre><code>{\n  \"id\": \"cmpl-625a9240f25e463487a9b6c53cbed080\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" and how they make you feel\\nColors, oh colors, so vibrant and bright\\nA world of emotions, a kaleidoscope in sight\\nRed\"\n    }\n  ],\n  \"created\": 1718620153,\n  \"model\": \"mymodel\",\n  \"system_fingerprint\": null,\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 30,\n    \"prompt_tokens\": 6,\n    \"total_tokens\": 36\n  }\n}\n</code></pre> <p>In case of chat requests:</p> <pre><code>service_url = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nurl = f'http://{service_url}/openai/v1/chat/completions'\n\nmodel_name = \"mymodel\"\n\njson = {\n    \"model\": model_name,\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are an assistant that speaks like Shakespeare.\"},\n        {\"role\": \"user\", \"content\": \"Write a poem about colors\"}\n    ],\n    \"max_tokens\": 30,\n    \"stream\": False\n}\n\nllm_run.invoke(url=url, json=json).json()\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"id\": \"cmpl-9aad539128294069bf1e406a5cba03d3\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"  O, fair and vibrant colors, how ye doth delight\\nIn the world around us, with thy hues so bright!\\n\",\n        \"tool_calls\": null,\n        \"role\": \"assistant\",\n        \"function_call\": null\n      },\n      \"logprobs\": null\n    }\n  ],\n  \"created\": 1718638005,\n  \"model\": \"mymodel\",\n  \"system_fingerprint\": null,\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 30,\n    \"prompt_tokens\": 37,\n    \"total_tokens\": 67\n  }\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llm/#fine-tuned-llm-model","title":"Fine-tuned LLM model","text":"<p>when it comes to custom LLM model, it is possible to create HuggingFace-based fine tuned model, log it and then serve it from the model path.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries</p> <p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"llm\")\n</code></pre> <p>Create the training procedure that logs model to the platform:</p> <pre><code>%%writefile \"src/train_model.py\"\n\nimport os\n\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom digitalhub_runtime_python import handler\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\n@handler()\ndef train(project):\n    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n    metric = evaluate.load(\"accuracy\")\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    dataset = load_dataset(\"yelp_review_full\")\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\")\n\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\n    save_model = \"model\"\n    if not os.path.exists(save_model):\n        os.makedirs(save_model)\n\n    save_dir = \"model\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    trainer.save_model(save_dir)\n    tokenizer.save_pretrained(save_dir)\n\n    project.log_model(\n        name=\"test_llm_model\",\n        kind=\"huggingface\",\n        base_model=\"google-bert/bert-base-cased\",\n        source=save_dir,\n    )\n</code></pre> <p>Register the function and execute it:</p> <pre><code>train_func = project.new_function(name=\"train_model\",\n                                  kind=\"python\",\n                                  python_version=\"PYTHON3_10\",\n                                  code_src=\"src/train_model.py\",\n                                  handler=\"train\",\n                                  requirements=[\"evaluate\", \"transformers[torch]\", \"torch\", \"torchvision\", \"accelerate\"])\n\ntrain_run=train_func.run(action=\"job\", profile=\"1xa100\", wait=True)\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm-classification\",\n                                    kind=\"huggingfaceserve\",\n                                    model_name=\"mymodel\",\n                                    path=\"s3://datalake/llm/model/test_llm_model/f8026820-2471-4497-97f5-8e6d49baac5f/\")\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"1xa100\", wait=True)\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls:</p> <pre><code>model_name = \"mymodel\"\njson = {\n    \"inputs\": [\n        {\n            \"name\": \"input-0\",\n            \"shape\": [2],\n            \"datatype\": \"BYTES\",\n            \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"],\n        }\n    ]\n}\n\nllm_run.invoke(model_name=model_name, json=json).json()\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    \"model_name\": \"mymodel\",\n    \"model_version\": None,\n    \"id\": \"cab30aa5-c10f-4233-94e2-14e4bc8fbf6f\",\n    \"parameters\": None,\n    \"outputs\": [\n        {\n            \"name\": \"output-0\",\n            \"shape\": [2],\n            \"datatype\": \"INT64\",\n            \"parameters\": None,\n            \"data\": [4, 0],\n        }\n    ],\n}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality.</p>"},{"location":"scenarios/mlllm/llmkubeai/","title":"Managing LLM Models with KubeAI Runtime","text":"<p>To support the set of LLM scenarios within the platform it is possible to use the KubeAI runtime when the KubeAI operator is enabled. </p> <p>For what concerns LLM tasks, currently KubeAI runtime allows for deploying the models for the following tasks:</p> <ul> <li><code>TextGeneration</code>: text generation tasks with  the OpenAI-compatible API</li> <li><code>TextEmbedding</code>: creating embeddings from the text following the OpenAI-compatible API</li> </ul> <p>To accomplish this, it is possible to use one of the KubeAI-supported runtimes, namely vLLM, OLlama, and Infinity. in case of vLLM also adapters are supported.</p> <p>For details about the specification, see the corresponding section of Modelserve reference.</p>"},{"location":"scenarios/mlllm/llmkubeai/#exposing-text-generation-models","title":"Exposing Text Generation Models","text":"<p>To expose the text generation model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>inference engine (one of <code>VLLM</code>, <code>OLlama</code>, <code>Infinity</code>) to use</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix), from S3 storage of the platform (<code>s3://</code>), or OLlama compatible model (<code>ollama://</code> prefix in case of OLlama engine).</li> <li>feature should be set to <code>TextGeneration</code>.</li> <li>in case of vLLM engine it is also possible to add list of adapters for the main model. Each adapter is specified with its own name and URL of the corresponding adapter.</li> </ul> <p>To serve the text generation model, the function should be run with the <code>serve</code> action, specifying additional parameters. In particular, it may be necessary to specify the HW profile to use with number of processors (since GPU may be required) or resource specification, and further parameters and arguments accepted by the KubeAI model specification:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values. For example, in case HuggingFace token is needed, create <code>HF_TOKEN</code> secret at the project with the token value to use. </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model with an adapter from HuggingFace, the following procedure may be used:</p> <pre><code>llm_function = project.new_function(\"llm\",\n                                    kind=\"kubeai-text\",\n                                    model_name=\"tinyllama-chat\",\n                                    url=\"hf://TinyLlama/TinyLlama-1.1B-Chat-v0.3\",\n                                    engine=\"VLLM\",\n                                    features=[\"TextGeneration\"],\n                                    adapters=[{\"name\": \"colorist\", \"url\": \"hf://jashing/tinyllama-colorist-lora\"}])\n\n\nllm_run = llm_function.run(action=\"serve\",\n                           profile=\"1xa100\",\n                           args=[\"--enable-prefix-caching\", \"--max-model-len=8192\"])                                    \n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform. The run status (see <code>openai</code> and <code>service</code> section) contains the information about the name of the model and the endpoints of the KubeAI API exposed </p> <pre><code>import requests \n\nmodel_name = f\"tinyllama-chat-123xyz_colorist\"\n\ninput = {\"prompt\": \"Hi\", \"model\": model_name}\n\nres = requests.post(f\"http://{KUBEAI_ENDPOINT}/openai/v1/completions\", json=input)\nprint(res.json())\n</code></pre> <p>By default, the <code>KUBEAI_ENDPOINT</code> is <code>kubeai:80</code>.</p> <p>Model name</p> <p>Please note how the model name is defined: it is composed of the name of the model as specified in the function and the random value. In case of adapter the name of adapter should be added: <code>&lt;model_name&gt;-&lt;random&gt;_&lt;adapter-name&gt;</code>.</p> <p>It is also possible to use OpenAI client for interacting with the model. </p>"},{"location":"scenarios/mlllm/llmkubeai/#exposing-text-embedding-models","title":"Exposing Text Embedding Models","text":"<p>To expose the text embedding model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>inference engine (one of <code>VLLM</code>, or <code>Infinity</code>) to use</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix), from S3 storage of the platform (<code>s3://</code>).</li> <li>feature should be set to <code>TextEmbedding</code>.</li> </ul> <p>To serve the text emvedding model, the function should be run with the <code>serve</code> action, specifying additional parameters.  Normally embedding models do not require extra resources. However,  further parameters and arguments accepted by the KubeAI model specification may be added:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model from HuggingFace, the following procedure may be used:</p> <pre><code>llm_function = project.new_function(\"llm\",\n                                    kind=\"kubeai-text\",\n                                    model_name=\"embedding\",\n                                    url=\"hf://BAAI/bge-small-en-v1.5\",\n                                    engine=\"Infinity\",\n                                    features=[\"TextEmbedding\"])\n\n\nllm_run = llm_function.run(action=\"serve\")                                    \n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform or OpenAI client:</p> <pre><code>from openai import OpenAI\n\nmodel_name = f\"embedding-123qwe\"\n\nclient = OpenAI(api_key=\"ignored\", base_url=f\"http://{KUBEAI_ENDPOINT}/openai/v1\")\nresponse = client.embeddings.create(\n    input=\"Your text goes here.\",\n    model=model_name\n)\n</code></pre> <p>By default, the <code>KUBEAI_ENDPOINT</code> is <code>kubeai:80</code>.</p>"},{"location":"scenarios/mlmlflow/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a MLFLow model is easy: <code>mlflowserve</code> runtime supports this functionality out of the box. It is sufficient to specify the path to the model artifact and optionally the name of the model to expose.</p> <p>It is important to note that the path should point to the folder, where the MLFlow <code>MLModel</code> artifact is placed. If the model is created from MLFlow run artifact path, besides the <code>model</code> folder it may contain additional artifacts.</p> <p>Register it and deploy:</p> <pre><code>serve_func = project.new_function(\n    name=\"serve-mlflow-model\",\n    kind=\"mlflowserve\",\n    model_name=model.name,\n    path=model.spec.path + \"model/\",\n)\n\nserve_run = func.run(\"serve\", wait=True)\n</code></pre> <p>You can now create a dataset for testing the endpoint:</p> <pre><code>from sklearn import datasets\n\niris = datasets.load_iris()\ndata = iris.data[0:2].tolist()\njson={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [-1, 4],\n        \"datatype\": \"FP64\",\n        \"data\": data\n        }\n    ]\n}\n</code></pre> <p>Finally, you can test the endpoint. When the model is ready to be used, invoke the endpoint:</p> <pre><code>serve_run.invoke(model_name=model.name, json=json).json()\n</code></pre> <p>If it does not work, wait for sometimes, because it takes a while to load the model.</p> <p>Please note that the MLFLow model serving exposes also the Open API specification under <code>/v2/docs</code> path.</p>"},{"location":"scenarios/mlmlflow/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlmlflow/intro/","title":"MLFLow ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a machine learning application based on model tracked with MLFlow framework using the functionalities of the platform.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlmlflow/intro/#set-up","title":"Set-up","text":"<p>Create folder for source code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre> <p>Then, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-mlflow-model-ci\")\n</code></pre>"},{"location":"scenarios/mlmlflow/intro/#generate-data","title":"Generate data","text":"<p>For the sake of simplicity, we use the predefined IRIS dataset.</p>"},{"location":"scenarios/mlmlflow/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport mlflow\nfrom digitalhub import from_mlflow_run, get_mlflow_model_metrics\nfrom digitalhub_runtime_python import handler\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n\n@handler(outputs=[\"model\"])\ndef train_model(project):\n    \"\"\"\n    Train an SVM classifier on the Iris dataset with hyperparameter tuning using MLflow\n    \"\"\"\n    # Enable MLflow autologging for sklearn\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    # Load Iris dataset\n    iris = datasets.load_iris()\n\n    # Define hyperparameter search space\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    # Train model with grid search\n    clf.fit(iris.data, iris.target)\n\n    # Get MLflow run information\n    run_id = mlflow.last_active_run().info.run_id\n\n    # Extract MLflow run artifacts and metadata for DigitalHub integration\n    model_params = from_mlflow_run(run_id)\n    metrics = get_mlflow_model_metrics(run_id)\n\n    # Register model in DigitalHub with MLflow metadata\n    model = project.log_model(name=\"iris-classifier\", kind=\"mlflow\", **model_params)\n    model.log_metrics(metrics)\n    return model\n</code></pre> <p>The function creates an SVC model with the scikit-learn framework. Note that here we use the autologging functionality of MLFlow and then construct the necessary model metadata out of the tracked MLFlow model. Specifically, MLFlow creates a series of artifacts that describe the model and the corresponding model files, as well as additional files representing the model properties and metrics.</p> <p>We then log the model of <code>mlflow</code> kind using the extract metadata as kwargs.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-mlflow-model\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n    requirements=[\"numpy&lt;2\", \"mlflow&lt;3\"],\n)\n</code></pre> <p>and run it locally:</p> <pre><code>train_model_run = train_fn.run(action=\"job\", wait=True)\n</code></pre> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = train_model_run.output(\"model\")\nmodel.spec.path\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlsklearn/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a scikit-learn model is easy: <code>sklearnserve</code> runtime supports this functionality out of the box. It is sufficient to specify the path to the model artifact and optionally the name of the model to expose.</p> <p>Register it and deploy:</p> <pre><code>serve_func = project.new_function(\n    name=\"serve-classifier\",\n    kind=\"sklearnserve\",\n    path=model.spec.path + \"breast_cancer_classifier.pkl\",\n)\n\nserve_run = serve_func.run(\"serve\", wait=True)\n</code></pre> <p>You can now create a dataset for testing the endpoint:</p> <pre><code>import numpy as np\n\n# Generate sample data for prediction\ndata = np.random.rand(2, 30).tolist()\njson_payload = {\"inputs\": [{\"name\": \"input-0\", \"shape\": [2, 30], \"datatype\": \"FP32\", \"data\": data}]}\n</code></pre> <p>Finally, you can test the endpoint. To do so, you need to refresh the serve run. This is needed because the backend monitors the deployment every minute and the model status, where the endpoint is exposed, is updated after a minute. When the model is ready, invoke the endpoint:</p> <pre><code>result = serve_run.refresh().invoke(json=json_payload).json()\nprint(\"Prediction result:\")\nprint(result)\n</code></pre> <p>Please note that the scikit-learn model serving exposes also the Open API specification under <code>/docs</code> path.</p>"},{"location":"scenarios/mlsklearn/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlsklearn/intro/","title":"Scikit-learn ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a scikit-learn machine learning application using the functionalities of the platform.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook. Alternatively, you can find the final notebook file for this scenario in the tutorial repository.</p>"},{"location":"scenarios/mlsklearn/intro/#set-up","title":"Set-up","text":"<p>First, import necessary libraries and create a project to host the functions and executions</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"project-ml-ci\")\n</code></pre> <p>Create folder for source code:</p> <pre><code>from pathlib import Path\nPath(\"src\").mkdir(exist_ok=True)\n</code></pre>"},{"location":"scenarios/mlsklearn/intro/#generate-data","title":"Generate data","text":"<p>Define the following function, which generates the dataset as required by the model:</p> <pre><code>%%writefile \"src/data-prep.py\"\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef data_generator():\n    \"\"\"\n    A function which generates the breast cancer dataset from scikit-learn\n    \"\"\"\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"target\"])\n    breast_cancer_dataset = pd.concat([breast_cancer_dataset, breast_cancer_labels], axis=1)\n    return breast_cancer_dataset\n</code></pre> <p>Register it:</p> <pre><code>data_gen_fn = project.new_function(name=\"data-prep\",\n                                   kind=\"python\",\n                                   python_version=\"PYTHON3_10\",\n                                   code_src=\"src/data-prep.py\",\n                                   handler=\"data_generator\")\n</code></pre> <p>Run it:</p> <pre><code>gen_data_run = data_gen_fn.run(\"job\",wait=True)\n</code></pre> <p>You can view the state of the execution with <code>gen_data_run.status</code> or its output with <code>gen_data_run.outputs()</code>. You can see a few records from the output artifact:</p> <pre><code>gen_data_run.output(\"dataset\").as_df().head()\n</code></pre> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/mlsklearn/training/","title":"Training the model","text":"<p>Let us define the training function.</p> <pre><code>%%writefile \"src/train-model.py\"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom pickle import dump\nimport sklearn.metrics\nfrom digitalhub_runtime_python import handler\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n@handler(outputs=[\"model\"])\ndef train_model(project, di):\n    \"\"\"\n    Train an SVM classifier on the breast cancer dataset and log metrics\n    \"\"\"\n    df_cancer = di.as_df()\n    X = df_cancer.drop([\"target\"], axis=1)\n    y = df_cancer[\"target\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n    svc_model = SVC()\n    svc_model.fit(X_train, y_train)\n    y_predict = svc_model.predict(X_test)\n\n    if not os.path.exists(\"model\"):\n        os.makedirs(\"model\")\n\n    with open(\"model/breast_cancer_classifier.pkl\", \"wb\") as f:\n        dump(svc_model, f, protocol=5)\n\n    metrics = {\n        \"f1_score\": sklearn.metrics.f1_score(y_test, y_predict),\n        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_predict),\n        \"precision\": sklearn.metrics.precision_score(y_test, y_predict),\n        \"recall\": sklearn.metrics.recall_score(y_test, y_predict),\n    }\n    model = project.log_model(name=\"breast_cancer_classifier\", kind=\"sklearn\", source=\"./model/\")\n    model.log_metrics(metrics)\n    return model\n</code></pre> <p>The function takes the analysis dataset as input, creates an SVC model with the scikit-learn framework and logs the model with its metrics.</p> <p>Let us register it:</p> <pre><code>train_fn = project.new_function(\n    name=\"train-classifier\",\n    kind=\"python\",\n    python_version=\"PYTHON3_10\",\n    code_src=\"src/train-model.py\",\n    handler=\"train_model\",\n    requirements=[\"numpy&lt;2\"],\n)\n</code></pre> <p>and run it:</p> <pre><code>dataset = gen_data_run.output(\"dataset\")\ntrain_run = train_fn.run(action=\"job\", inputs={\"di\": dataset.key}, wait=True)\n</code></pre> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = train_run.output(\"model\")\nmodel.spec.path\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlspeech/kubeaispeech/","title":"Managing Speech-to-Text Models with KubeAI Runtime","text":"<p>To support the speech-to-text scenario within the platform it is possible to use the KubeAI runtime when the KubeAI operator is enabled. </p> <p>To accomplish this, it is possible to use the KubeAI-supported runtime, namely FasterWhisper.</p> <p>For details about the specification, see the corresponding section of Modelserve reference.</p>"},{"location":"scenarios/mlspeech/kubeaispeech/#exposing-speech-to-text-models","title":"Exposing Speech-to-Text  Models","text":"<p>To expose the  speech-to-text  model, it is possible to use Core UI or Python SDK. To define the corresponding function, the following parameters should be specified:</p> <ul> <li>model name</li> <li>model URL. Currently the model can be loaded either from HuggingFace (<code>hf://</code> prefix) or from S3 storage of the platform (<code>s3://</code>)).</li> </ul> <p>To serve the text speech-to-text model, the function should be run with the <code>serve</code> action, specifying additional parameters. In particular, it may be necessary to specify the HW profile to use with number of processors or resource specification, and further parameters and arguments accepted by the KubeAI model specification:</p> <ul> <li><code>args</code>: command-line arguments to pass to the engine</li> <li><code>env</code>: custom environment values (key-value pair)</li> <li><code>secrets</code>: project secrets to pass the values </li> <li><code>files</code>: extra file specifications for the deployment</li> <li><code>scaling</code>: scaling specification as of KubeAI documentation</li> <li><code>caching_profile</code>: cache profile as of KubeAI documentation.</li> </ul> <p>For example to deploy a model from HuggingFace, the following procedure may be used:</p> <pre><code>audio_function = project.new_function(\"audio\",\n                                    kind=\"kubeai-speech\",\n                                    model_name=\"audiomodel\",\n                                    url=\"hf://Systran/faster-whisper-medium.en\")\n\n\nrun = audio_function.run(action=\"serve\")                                    \n</code></pre> <p>Once deployed, the model is available and it is possible to call the OpenAI-compatible API from within the platform (<code>/openai/v1/transcriptions</code> endpoint):</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=f\"http://{KUBEAI_ENDPOINT}/openai/v1\", api_key=\"ignore\")\naudio_file= open(\"kubeai.mp4\", \"rb\")\n\ntranscription = client.audio.transcriptions.create(\n    model=f\"audiomodel-123zxc\", \n    file=audio_file\n)\n\nprint(transcription.text)\n</code></pre> <p>By default, the <code>KUBEAI_ENDPOINT</code> is <code>kubeai</code>.</p> <p>Model name</p> <p>Please note how the model name is defined: it is composed of the name of the model as specified in the function and a random value: <code>&lt;model_name&gt;-&lt;run_id&gt;</code>. The name of the generated model as well as the endpoint information can be seen in the run specification (see <code>openai</code> and <code>service</code> section)</p>"},{"location":"scenarios/postgrest/apigw/","title":"Expose service externally","text":"<p>Finally, we make the PostgREST service available externally. Access API Gateways on the left menu and click <code>CREATE</code>.</p> <p>Fill the fields as follows:</p> <ul> <li>Name: name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Service: select the one referring to the PostgREST service you created. Its name may be something like <code>postgrest-my-postgrest</code>. Port will automatically be filled.</li> <li>Host: full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>pgrest.services.example.com</code>.</li> <li>Path: Leave the default <code>/</code>.</li> <li>Authentication: <code>None</code>.</li> </ul> <p></p> <p>Save and the API gateway will be created. You can try a simple query like the following, even in your browser, to view its results (remember to change domain according to your case): <pre><code>https://pgrest.services.example.com/readings?limit=3\n</code></pre></p>"},{"location":"scenarios/postgrest/data/","title":"Insert data into the database","text":"<p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We will now insert some data into the database we created earlier. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file is available in the <code>documentation/examples/postgrest</code> path within the repository of this documentation.</p> <p>Import required libraries:</p> <pre><code>import os\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport requests\n</code></pre> <p>Connect to the database. You will need the value of POSTGRES_URL you got from the owner's secret in the first stage of the scenario.</p> <pre><code>engine = create_engine('postgresql://owner-UrN9ct:88aX8tLFJ95qYU7@database-postgres-cluster/mydb')\n</code></pre> <p>Download a CSV file and parse it (may take a few minutes):</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n\nwith requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n\ndf = pd.read_csv(filename, sep=\";\")\n</code></pre> <p>The following will create a table and insert the dataframe into it. If it fails, resources allocated to the Jupyter workspace may be insufficient. The table will be created automatically, or replaced if it already exists. <pre><code>df.to_sql(\"readings\", engine, if_exists=\"replace\")\n</code></pre></p> <p>Run a test select query to check data has been successfully inserted: <pre><code>select = \"SELECT * FROM readings LIMIT 3\"\nselect_df = pd.read_sql(select,con=engine)\nselect_df.head()\n</code></pre></p> <p>If everything went right, a few rows are returned. We will now create a PostgREST service to expose this data via a REST API.</p>"},{"location":"scenarios/postgrest/intro/","title":"PostgREST scenario introduction","text":"<p>In this scenario, we download some data into a Postgres database, then use PostgREST - a tool to make Postgres tables accessible via REST API - to expose this data and run a simple request to view the results.</p>"},{"location":"scenarios/postgrest/intro/#database-set-up","title":"Database set-up","text":"<p>Let's start by setting up the database. Access your KRM instance and Postgres DBs on the left menu, then click Create.</p> <ul> <li><code>Name</code>: This is just an identifier for Kubernetes. Type <code>my-db</code>.</li> <li><code>Database</code>: The actual name on the database. Type <code>mydb</code>.</li> <li>Toggle on <code>Drop on delete</code>, which conveniently deletes the database when you delete the custom resource.</li> </ul> <p></p> <p>Click Save. You should now see your database listed.</p>"},{"location":"scenarios/postgrest/intro/#add-users-to-database","title":"Add users to database","text":"<p>PostgREST needs a user to authenticate and a user to assume its role when the API is called. These can be separate users, but for our purposes, we will create just one to fill both roles. Click Show on your database's entry and then Add user on the bottom. Enter values as follows:</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-owner</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>owner</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Owner</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>owner</code>.</li> </ul> <p></p>"},{"location":"scenarios/postgrest/intro/#retrieve-postgres_url","title":"Retrieve POSTGRES_URL","text":"<p>Together with the user, a secret has also been created. Go to Secrets on the left menu; the list should contain a secret with a name referring the user you created. Find it and click Show.</p> <p>Write down the following information somewhere, as we will need it later: - Name of the secret - Value of <code>POSTGRES_URL</code> (click on Decode to obtain it) - Value of <code>ROLE</code></p> <p></p>"},{"location":"scenarios/postgrest/postgrest/","title":"Create PostgREST service","text":"<p>We will go back to KRM to create a PostgREST service and expose the database's table via API.</p>"},{"location":"scenarios/postgrest/postgrest/#inspect-users-secrets","title":"Inspect users' secrets","text":"<p>We need some parameters to configure PostgREST. Similarly to what you did in the first stage of the scenario, go to Secrets on the left and look for the two secrets, belonging to the owner and reader users you created.</p> <p>For the owner, you will need the Name of the secret. For the reader, you will need the content of the ROLE field.</p>"},{"location":"scenarios/postgrest/postgrest/#creating-the-postgrest-service","title":"Creating the PostgREST service","text":"<p>Click PostgREST Data Services on the left and then Create.</p> <p>Fill the first few fields as follows:</p> <ul> <li><code>Name</code>: Anything you'd like, it's once again an identifier for Kubernetes.</li> <li><code>Schema</code>: <code>public</code></li> <li>Toggle on <code>With existing DB user</code>.</li> <li><code>Existing DB user name</code>: Value of ROLE in the owner's secret.</li> </ul> <p>Under Connection, fill the fields as follows:</p> <ul> <li><code>Secret name</code>: Name of the owner's secret.</li> </ul> <p>When you hit Save, the PostgREST instance will be launched.</p> <p></p>"},{"location":"sdk/intro/","title":"Python SDK Reference","text":"<p>Python SDK provides the functionality of the Core API and allows for working with the platform entities directly from interactive workspaces or your prefered development environment.</p> <p>See here the SDK documentation (refer to the version corresponding to the version of the platform):</p> <ul> <li>SDK Reference</li> </ul>"},{"location":"tasks/code-source/","title":"Code source","text":"<p>In several runtime objects, it is possible to execute a program (e.g. a function, a query or a workflow) written in a source. This source can be a single code file, a plain text string or a base64 encoded string, a zip archive or a git repository. Beside the source, you need also to define a <code>handler</code>, which is the entrypoint of the code.</p>"},{"location":"tasks/code-source/#code-source-definition","title":"Code source definition","text":"<p>In the SDK, there are three different types of source code:</p> <ul> <li><code>code</code> which is a plain string source.</li> <li><code>base64</code> which is a base64 encoded string source.</li> <li><code>code_src</code> which is a code source URI.</li> </ul>"},{"location":"tasks/code-source/#plain-text-source","title":"Plain text source","text":"<p>You can define a plain text source using the <code>code</code> parameter.</p> <p>Here follow an example of a plain text source with the Python runtime:</p> <pre><code>my_code = \"\"\"\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"filtered\"])\ndef myfunction(di: Dataitem, col1: str):\n    df = di.as_df()\n    df = df[[\"col1\"]]\n    return df\n\"\"\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will execute the code in the <code>my_code</code> variable.</p>"},{"location":"tasks/code-source/#base64-encoded-source","title":"Base64 encoded source","text":"<p>You can define a base64 encoded source using the <code>base64</code> parameter.</p> <p>Here follow an example of a base64 encoded source with the Python runtime:</p> <pre><code># Same function as above encoded in base64\nbase64_code = \"ZnJvbSBkaWdpdGFsaHViX3J1bnRpbWVfcHl0aG9uIGltcG9ydCBoYW5kbGVyCgpAaGFuZGxlcihvdXRwdXRzPVsiZmlsdGVyZWQiXSkKZGVmIG15ZnVuY3Rpb24oZGk6IERhdGFpdGVtLCBjb2wxOiBzdHIpOgogICAgZGYgPSBkaS5hc19kZigpCiAgICBkZiA9IGRmW1siY29sMSJdXQogICAgcmV0dXJuIGRm\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       base64=base64_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will execute the code in the <code>base64_code</code> variable.</p>"},{"location":"tasks/code-source/#code-source-uri","title":"Code source URI","text":"<p>You can define a code source URI using the <code>code_src</code> parameter. We support the following types of URIs:</p> <ul> <li>Local file path</li> <li>Git repository</li> <li>S3 zip archive</li> </ul>"},{"location":"tasks/code-source/#local-file-path","title":"Local file path","text":"<p>The local file path can be specified with the <code>path/to/file.ext</code> format.</p> <pre><code>my_code = \"src/my-func.py\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>In this case the function will import the code in the <code>src/my-func.py</code> file, encodes it in base64 and then executes it. If the file is not found, the function will raise an exception.</p>"},{"location":"tasks/code-source/#remote-git-repository","title":"Remote git repository","text":"<p>The remote git repository can be specified with the <code>git+https://repo-host.com/some-user/some-repo</code> format. The anteposition <code>git+</code> is required, the rest of the URL is the repository URL.</p> <pre><code>my_repo = \"git+https://repo-host/some-user/some-repo\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_repo,\n                       handler=\"path:function\")\n</code></pre> <p>When a git repository is specified, the function try to clone the repository and execute the code in the specified in the <code>handler</code> entrypoint (See below). If the repository does not exist, the function will raise an exception.</p>"},{"location":"tasks/code-source/#credentials","title":"Credentials","text":"<p>You may need credentials to access the repository. The credentials can be specified in three environment variables: <code>GIT_TOKEN</code>, <code>GIT_USER</code>and <code>GIT_PASSWORD</code>.</p> <p>Note</p> <p>You must set the env variable before defining the function. Token auth is recommended and takes precedence over basic auth. Please also verify that the repo provider supports basic auth.</p>"},{"location":"tasks/code-source/#token","title":"Token","text":"<p>The token is a plain string. It will be passed in the URL according to the git provider.</p> <p>Example:</p> <pre><code># GitHub token\nos.environ[\"GIT_TOKEN\"] = \"github_pat_...\"\n\n# GitLab token\nos.environ[\"GIT_TOKEN\"] = \"glpat...\"\n\n# function definition\n</code></pre>"},{"location":"tasks/code-source/#user-and-password","title":"User and password","text":"<p>User and password are plain strings.</p> <pre><code>os.environ[\"GIT_USER\"] = \"some-user\"\nos.environ[\"GIT_PASSWORD\"] = \"some-password\"\n\n# function definition\n</code></pre>"},{"location":"tasks/code-source/#remote-zip-s3-archive","title":"Remote zip s3 archive","text":"<p>The remote zip s3 archive can be specified with the <code>zip+s3://some-bucket/some-key.zip</code> format. The anteposition <code>zip+</code> is required, the rest of the URL is an S3 URL in the form <code>s3://some-bucket/some-key.zip</code>. The code source is archived in a zip file, which is unpacked at runtime.</p> <pre><code>my_archive = \"zip+s3://some-bucket/some-key.zip\"\n\nfunc = dh.new_function(project=\"my-project\",\n                       name=\"python-function\",\n                       kind=\"python\",\n                       python_version=\"PYTHON3_9\",\n                       code_src=my_archive,\n                       handler=\"path:function\")\n</code></pre>"},{"location":"tasks/code-source/#handler","title":"Handler","text":"<p>The <code>handler</code> parameter is the entrypoint of the code. There are some rules to follow when defining it.</p> <p>If the source code is:</p> <ul> <li>Plain text</li> <li>Base64 encoded</li> <li>Local file path</li> </ul> <p>Then the entrypoint should be the function name.</p> <pre><code>my_code = \"\"\"\ndef myfunction():\n    ...\n\"\"\"\n\nfunc = dh.new_function(...,\n                       code=my_code,\n                       handler=\"myfunction\")\n</code></pre> <p>If the source code is:</p> <ul> <li>S3 zip archive</li> <li>Git repository</li> </ul> <p>Then the entrypoint should be the path to the file where the code is stored (expressed with <code>.</code> separator) and the name of the function separated by a <code>:</code>.</p> <pre><code>my_code = \"git+https://repo-host/some-user/some-repo\"\n\nfunc = dh.new_function(...,\n                       code_src=my_code,\n                       handler=\"src.subdir.etc:myfunction\")\n</code></pre>"},{"location":"tasks/data/","title":"Data and transformations","text":"<p>The platform supports data of different types to be stored and operated by the underlying storage subsystems.</p> <p>Specifically, the platform natively supports two types of storages:</p> <ul> <li>persistence object storage (datalake S3 Minio), which manages immutable data in the form of files.</li> <li>operational relational data storage (PostgreSQL database), which is used for efficient querying of mutable data. Postgres is rich with extensions, most notably for geo-spatial and time-series data.</li> </ul> <p>The data is represented in the platform as entities of different types, depending on its usage and format. More specifically, we distinguish:</p> <ul> <li>data items, which represent immutable data sets resulting from different transformation operations and are ready for use in differerent types of analysis. Data items are enriched with metadata (versions, lineage, stats, profiling, schema, ...) and unique keys and managed and persisted to the datalake directly by the platform in the form of Parquet files. It is possible to treat tabular data (items of <code>table</code> kind) as, for example, DataFrames, using conventional libraries.</li> <li>artifacts, which represent arbitrary files, not limited to tabular format, stored to the datalake with some extra metadata.</li> </ul> <p>Each data entity may be accessed and manipulated by the platform via UI or using the API, for example with SDK.</p>"},{"location":"tasks/data/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/data/#artifacts","title":"Artifacts","text":"<p>Artifacts can be managed as entities from the UI. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new artifact</li> <li><code>filter</code> artifacts by name and kind</li> <li><code>expand</code> an artifact to see its 5 latest versions</li> <li><code>show</code> the details of an artifact</li> <li><code>edit</code> an artifact</li> <li><code>delete</code> an artifact</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete artifacts using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the artifact</li> <li><code>Kind</code>: kind of the artifact</li> <li>(Spec) <code>Path</code>: remote path where the artifact is stored. If you instead upload the artifact at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later.</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description of the artifact</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the artifact</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path to the artifact, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view an artifact's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update","title":"Update","text":"<p>You can update an artifact by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete","title":"Delete","text":"<p>You can delete an artifact from either its detail page or the list of artifacts, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#data-items","title":"Data items","text":"<p>Data items can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new data item</li> <li><code>expand</code> a data item and see its 5 latest versions</li> <li><code>show</code> the details of a data item</li> <li><code>edit</code> a data item</li> <li><code>delete</code> a data item</li> <li><code>filter</code> data items by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete data items using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create_1","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name of the dataitem</li> <li><code>Kind</code>: kind of the dataitem</li> <li>(Spec) <code>Path</code>: remote path where the data item is stored. If you instead upload the data item at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later:</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the data item</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path of the data item, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#kind","title":"Kind","text":"<p>There are 2 possible kinds for dataitems:</p> <ul> <li><code>Dataitem</code>: indicates it is a generic data item. There are no specific attributes in the creation page.</li> <li><code>table</code>: indicates that the data item points to a table. The optional parameter is the schema of the table in table_schema format.</li> </ul>"},{"location":"tasks/data/#read_1","title":"Read","text":"<p>Click <code>SHOW</code> to view a data item's details.</p> <p></p> <p>Based on the <code>kind</code>, there may be a <code>schema</code>, indicating that the dataitem point to a table.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update_1","title":"Update","text":"<p>You can update a data item by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete_1","title":"Delete","text":"<p>You can delete a data item from either its detail page or the list of data items, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#management-via-sdk","title":"Management via SDK","text":""},{"location":"tasks/data/#managing-artifacts-with-sdk","title":"Managing artifacts with SDK","text":"<p>Artifacts can be created and managed as entities with the SDK CRUD methods. Check the SDK Artifacts documentation for more information.</p>"},{"location":"tasks/data/#managing-dataitems-with-sdk","title":"Managing dataitems with SDK","text":"<p>Dataitems can be created and managed as entities with the SDK CRUD methods. Check the SDK Dataitem documentation for more information.</p>"},{"location":"tasks/functions/","title":"Functions and Runtimes","text":"<p>Functions are the logical description of something that the platform may execute and track for you. A function may represent code to run as a job, an ML model inference to be used as batch procedure or as a service, a data validation, etc.</p> <p>In the platform we perform actions over functions (also referred to as \"tasks\"), such as job execution, deploy, container image build. A single action execution is called run, and the platform keeps track of these runs, with metadata about function version, operation parameters, and runtime parameters for a single execution.</p> <p>They are associated with a given runtime, which implements the actual execution and determines which actions are available. Examples are DBT, Container, Python, etc. Runtimes  are highly specialized components which can translate the representation of a given execution, as expressed in the run, into an actual execution operation performed via libraries, code, external tools etc.</p> <p>Runtimes define the key point of extension of the platform: new runtimes may be added in order to implement the low-level logic of \"translating\" the high level operation definition into an executable run. For example, DBT runtime allows for defining the transformation as a task that, given the input table reference, produces a datastt appyling the function defined as SQL code. The runtime in this case is responsible for converting the specification and the references to a dedicated Kubernetes Job that runs DBT transformation and stores the corresponding dataset.</p> <p>The set of the supported runtimes is documented in Runtimes References section. Independently of the specific runtime implementation, the flow of actions with respect to the function definition and execution is the following:</p> <ul> <li>define a new function providing its name, runtime, definition (e.g., source code), and configuration (e.g., dependencies). The function definition is saved by the project. Each change to the function spec creates a new function version so that the executions of different function versions are independently tracked.</li> <li>execute a task over the function providing the configuration of the task (e.g., the K8S resources needed for the execution), the execution parameters and inputs (if any). This creates a new task specification and a new run instance tracked by the platform.</li> </ul> <p>The definition and execution of the functions may be performed either via UI or via Python SDK.</p>"},{"location":"tasks/functions/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/functions/#functions","title":"Functions","text":"<p>Functions can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new function</li> <li><code>expand</code> a function to see its 5 latest versions</li> <li><code>show</code> the details of a function</li> <li><code>edit</code> a function</li> <li><code>delete</code> a function</li> <li><code>filter</code> functions by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete functions using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/functions/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the function</li> <li><code>Kind</code>: kind of function</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p><code>Spec</code> fields will change depending on the function's kind.</p>"},{"location":"tasks/functions/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a function's details.</p> <p></p> <p>Tabs next to <code>SUMMARY</code> will change depending on the function's <code>kind</code>. Some of them allow you to create runs, but we will see this in a later section.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/functions/#update","title":"Update","text":"<p>You can update a function by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/functions/#delete","title":"Delete","text":"<p>You can delete a function from either its detail page or the list of functions, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/functions/#runs","title":"Runs","text":""},{"location":"tasks/functions/#create_1","title":"Create","text":"<p>A run represents the execution of a task through a function. As such, the starting point to create a run is the function it is based on. Select one of the functions you created. You will notice multiple tabs at the top, next to <code>SUMMARY</code>. These tabs may differ depending on the function's <code>kind</code>.</p> <p></p> <p>Click <code>CREATE</code> to create a new run. You will start a 3-steps process to create a run.</p> <p>The first step will ask for parameters that depend on the function's <code>kind</code> and the task you are creating the run for, but will generally also ask if you wish to configure resources to allocate, environment variables, secrets, volumes and node modules.</p> <p>The second step will ask, if applicable, to specify inputs, outputs and parameters.</p> <p>The third step will simply present a recap.</p> <p></p>"},{"location":"tasks/functions/#view-and-manage","title":"View and manage","text":"<p>By going through a function's tabs, you can access the corresponding runs, but you may also access all runs from the Runs section in the left menu (also available as Jobs and runs in the dashboard).</p> <p>You can filter runs by name, kind and status.</p> <p></p> <p>Click on a run to view its details.</p> <p></p> <p>From here, click on <code>LOGS</code> to view its logs.</p> <p></p>"},{"location":"tasks/functions/#management-via-sdk","title":"Management via SDK","text":"<p>Functions can be created and managed as entities with the SDK CRUD methods. Check the SDK Functions documentation for more information.</p>"},{"location":"tasks/kubernetes-resources/","title":"Using Kubernetes Resources for Runs","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on Kubernetes functionalities and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul>"},{"location":"tasks/kubernetes-resources/#how-to-define-resource-requirements","title":"How to define resource requirements","text":"<p>In the platform, kubernetes resource requirements may be defined in two ways:  * by users at run time, by requesting resources to be allocated for a given run, * by administrators at deployment time, by configuring defaults and limits along with pre-configured templates and profiles</p>"},{"location":"tasks/kubernetes-resources/#request-resources-at-runtime","title":"Request resources at runtime","text":"<p>The platform lets users require additional k8s resource to be allocated to a given function's run, either via Core UI or via SDK. Please note that it is possible to describe only some properties, leaving the rest blank without constraints. All the defaults are managed by the platform in accordance with the underlying Kubernetes deployment.</p> <p>To define requirements for single runs, developers need to include in the run specification the resource definition, in accordance with the schema.</p> <p>For example, to request for a certain amount of compute resources, the spec must contain the detailed definition as follows:</p> <pre><code>resources:\n  cpu:\n    requests: 8\n  mem:\n    requests: 32Gi\n  gpu:\n    limits: \"1\"\n</code></pre> <p>In order to provide such definitions, users can leverage the SDK or the Core UI to programmatically or interactively define their request. Please see the Kubernetes Resources section of the documentation for more information.</p>"},{"location":"tasks/kubernetes-resources/#resource-templates-and-profiles","title":"Resource templates and profiles","text":"<p>It is possible to rely on a set of preconfigured HW profiles defined during the platform deployment.  The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. Please see Resource templates section of the documentation for more information.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p>"},{"location":"tasks/kubernetes-resources/#available-resources","title":"Available resources","text":"<p>The section lists all the resources available to users for runs. </p>"},{"location":"tasks/kubernetes-resources/#volumes","title":"Volumes","text":"<p>Users can ask for a persistent volume claim (pvc) to be created and mounted on the container being launched by the task. You need to declare the volume type as <code>persistent_volume_claim</code>, a name for the PVC for the user (e.g., <code>my-pvc</code>), the mount path on the container and a spec with the size of the PV to be reserved. The platform will create the volume and bind it to the pod lifecycle.</p> <pre><code>volumes:\n        - volume_type: \"persistent_volume_claim\",\n          name: \"my-pvc\",\n          mount_path: \"/data\",\n          spec: \n            size: \"10Gi\",\n</code></pre> <p>Note: the platform can be configured to block the usage of pre-existing volumes for security reasons. Volumes created by the platform for specific runs as ReadWriteOnce and used exclusively by the platform.</p>"},{"location":"tasks/kubernetes-resources/#hardware-resources","title":"Hardware Resources","text":"<p>Users can request a specific amount of hardware resources (cpu, memory, gpu) for a given run by declaring them  via the <code>resources</code> spec parameter.</p> <p>Supported resources are:</p> <ul> <li>CPU</li> <li>RAM memory</li> <li>GPU</li> </ul>"},{"location":"tasks/kubernetes-resources/#cpu","title":"CPU","text":"<p>To request a specific amount of CPU for the run, declare the resource type as <code>cpu</code> and specify request and/or limit values.</p> <pre><code>resources:\n  cpu:\n    requests: \"10\"\n    limits: \"12\"\n</code></pre>"},{"location":"tasks/kubernetes-resources/#ram-memory","title":"RAM memory","text":"<p>To request a specific amount of RAM memory for the run, declare the resource type as <code>mem</code> and specify request and/or limit values.</p> <pre><code>resources:\n  mem:\n    requests: 32Gi\n    limits: 64Gi\n</code></pre>"},{"location":"tasks/kubernetes-resources/#gpu","title":"GPU","text":"<p>To request GPU resources, specify the resource type <code>gpu</code> and set the requested value as a limit.</p> <pre><code>resources:\n  gpu:\n    limits: \"1\"    \n</code></pre>"},{"location":"tasks/kubernetes-resources/#secrets","title":"Secrets","text":"<p>Users can request a secret injection into the run being launched by passing the identifier inside the <code>secrets</code> field. Secrets must be stored via the platform: externally defined secrets (for example in k8s) are not accessible to users for security reasons.</p> <pre><code>secrets:\n  - my-secret-key\n</code></pre>"},{"location":"tasks/kubernetes-resources/#envs","title":"Envs","text":"<p>User can inject environment variables injection into the container being launched by passing definition of variables as key/value inside the <code>envs</code> field.</p> <pre><code>envs:\n  - name: ENV1\n    value: VALU123123\n  - name: ENV2\n    value: VALU123123  \n</code></pre>"},{"location":"tasks/kubernetes-resources/#node-selection","title":"Node selection","text":"<p>Users can request a node selector for the run being launched by defining the selector(s) as a key/value list.  The platform will add the selectors as-is to k8s resources such as Jobs, Pods, Deployments when appropriate.</p> <pre><code>node_selector:\n  - key: selectorKey\n    value: selectValue\n</code></pre> <p>See K8s Documentation for reference.</p>"},{"location":"tasks/kubernetes-resources/#tolerations","title":"Tolerations","text":"<p>To define tolerations add the definition inside the <code>tolerations</code> field of the spec, following Kubernetes specifications. Please see Kubernetes documentation.</p>"},{"location":"tasks/kubernetes-resources/#affinity","title":"Affinity","text":"<p>To define affinity add the definition inside the <code>affinity</code> field of the spec, following Kubernetes specifications. Please see Kubernetes documentation.</p>"},{"location":"tasks/kubernetes-resources/#fs-group","title":"FS group","text":"<p>To properly map volumes mounted for runs, users can specify the group id used for mount operations. This step is required when the USER used to run the process does not match the default. Define the <code>fs_group</code> field by specifying the group id as integer.</p> <pre><code>fs_group: 1000\n</code></pre>"},{"location":"tasks/kubernetes-resources/#run-as-user","title":"Run as user","text":"<p>The process run inside the container is owned by the USER defined in the container manifest. For security reasons, the platform does not allow containers to be run as root. User can ask for a different, specific user id to be used, by defining the <code>run_as_user</code> field. It accepts an integer value.</p> <pre><code>run_as_user: 1000\n</code></pre>"},{"location":"tasks/kubernetes-resources/#run-as-group","title":"Run as group","text":"<p>The process run inside the container is owned by the GROUP defined in the container manifest. For security reasons, the platform does not allow containers to be run as root. User can ask for a different, specific group id to be used, by defining the <code>run_as_group</code> field.  It accepts an integer value.</p> <pre><code>run_as_group: 1000\n</code></pre>"},{"location":"tasks/models/","title":"ML Models","text":"<p>Support for MLOps is one of the key functionality of the platform. Creation, management, and serving ML models is supported by the platform via ML Model entities and the corresponding functionality for their registration and serving.</p> <p>ML Model entity represent the relevant information about the model - framework and algorithms used to create it, hyper parameters and metrics, necessary artifacts constituting the model, etc. The platform support a list of standard model kinds as well as generic models. Specifically, it is possible to define models of the following kinds</p> <ul> <li><code>sklearn</code> - ML models created with Scikit-learn framework and packaged as a single artifact.</li> <li><code>mlflow</code> - ML models created with any MLFlow-compatible framework (or <code>flavor</code> in MLFlow terminology) and logged following the MLFlow model format.</li> <li><code>huggingface</code> - LLM created using the HuggingFace framework and format, either standard one or fine-tuned.</li> <li><code>model</code> - generic ML Model with custom packaging and framework.</li> </ul> <p>For the specific ML Model formats the platform provides the support for serving those models as inference API in line with the V2 open inference protocol. These is achieved with the corresponding model serving runtimes.</p>"},{"location":"tasks/models/#management-via-ui","title":"Management via UI","text":"<p>Models can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new model</li> <li><code>expand</code> a model to see its 5 latest versions</li> <li><code>show</code> the details of a model</li> <li><code>edit</code> a model</li> <li><code>delete</code> a model</li> <li><code>filter</code> models by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete models using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/models/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the model</li> <li><code>Kind</code>: kind of the model</li> <li>(Spec) <code>Path</code>: remote path where the model is stored. If you instead upload the model at the bottom of the form, this will be the path to where it will be stored.</li> </ul>"},{"location":"tasks/models/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a model's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/models/#update","title":"Update","text":"<p>You can update a model by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/models/#delete","title":"Delete","text":"<p>You can delete a model from either its detail page or the list of models, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/models/#management-via-sdk","title":"Management via SDK","text":"<p>Models can be created and managed as entities with the SDK CRUD methods. Check the SDK Model documentation for more information.</p>"},{"location":"tasks/pat/","title":"Personal Access Tokens (PATs)","text":"<p>Personal Access Tokens (PATs) are a special type of access token that users can generate directly from the platform\u2019s web console. PATs are designed to simplify user authentication for long-lived or automation-related use cases, offering the same functionality as standard access tokens with additional control features.</p> <p>Key Characteristics:</p> <ul> <li> <p>No Automatic Expiration by Default:     PATs do not expire automatically unless an expiration policy is enforced by platform administrators. This makes them suitable for use cases requiring persistent access without frequent reauthentication.</p> </li> <li> <p>Interoperable with Standard Access Token Workflows:     PATs can be used in any context where a standard access token is accepted, including calling APIs and authenticating with platform services.</p> </li> <li> <p>Supports Token Exchange (Security Token Service - STS):     PATs can be used in token exchange flows to obtain short-lived standard access tokens and associated credentials, aligning with security best practices for downstream service interactions.</p> </li> <li> <p>Named Tokens:     Each PAT must be assigned a human-readable name upon creation. This name helps users identify and manage their tokens more easily in the web console.</p> </li> <li> <p>User-Controlled Lifecycle Management:     PATs can be revoked at any time by their owner via the web console, providing a manual method of invalidating tokens if they are no longer needed or are believed to be compromised.</p> </li> <li> <p>Opaque Format:     PATs are opaque strings, meaning their contents are not encoded in a readable format like JWTs (JSON Web Tokens). Their structure and claims are not introspectable by clients and must be validated by the platform.</p> </li> </ul> <p>Typical Use Cases</p> <ul> <li> <p>Command-line tools or scripts requiring persistent access without interactive login</p> </li> <li> <p>Integration with CI/CD pipelines</p> </li> <li> <p>Applications that require non-expiring or manually managed credentials</p> </li> <li> <p>Access delegation to long-lived background jobs</p> </li> </ul>"},{"location":"tasks/pat/#security-considerations","title":"Security Considerations","text":"<p>Because PATs typically do not expire automatically and are opaque, they should be treated as sensitive credentials. It is strongly recommended to:</p> <ul> <li> <p>Store PATs securely (e.g., in encrypted secrets managers)</p> </li> <li> <p>Use token exchange to obtain short-lived credentials for actual API calls</p> </li> <li> <p>Regularly review and rotate PATs</p> </li> <li> <p>Revoke unused or compromised tokens immediately</p> </li> </ul>"},{"location":"tasks/pat/#token-management-via-ui","title":"Token management via UI","text":"<p>When logged in the web console, users can create, review and revoke Personal Access Tokens from the user menu, accessible via the top right dropdown with the username. </p> <p></p> <p>Select configuration to open the user management page and then scroll down to the Personal Access Tokens section.</p> <p></p> <p>When adding a PAT, the form asks for a name and then lets users select which kind of permissions will be given to the newly created token. On creation, the token value will be shown once, and then stored secretly in the platform.</p> <p>Copy the token value and store securely: it won't be readable anymore!</p> <p></p> <p>From now on, the token can be freely used wherever an access token would be required. At any given time, owner can revoke the token from the same page by selecting Delete and confirming the removal.</p>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project represents a data and AI application and is a container for different entities (code, assets, configuration, ...) that form the application. It is the context in which you can run functions and manage models, data, and artifacts. Projects may be created and managed from the UI, but also by using DH Core's API, for example via Python SDK.</p>"},{"location":"tasks/projects/#management-via-ui","title":"Management via UI","text":"<p>In the following sections we document project management via the <code>Core Console</code> UI.</p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A project is created by clicking <code>CREATE A NEW PROJECT</code> in the console's home page.</p> <p></p> <p>A form asking for the project's details is then shown:</p> <p></p> <p>The following parameters are mandatory:</p> <ul> <li><code>name</code>: name of the project, also acts as identifier of the project</li> </ul> <p><code>Metadata</code> parameters are optional and may be changed later:</p> <ul> <li><code>name</code>: name of the project</li> <li><code>description</code>: a human-readable description of the project</li> <li><code>labels</code>: list of labels</li> </ul> <p><code>Save</code> and the project will appear in the home page.</p>"},{"location":"tasks/projects/#read","title":"Read","text":"<p>All projects present in the database are listed in the home page. Each tile shows:</p> <ul> <li>Identifier of the project</li> <li>Name of the project (hidden if same as identifier)</li> <li>Description</li> <li>Date of creation</li> <li>Date of last modification</li> </ul> <p></p> <p>Click on the tile to access the project's dashboard:</p> <p></p> <p>This dashboard shows a summary of the resources associated with the project and allows you to access the management of these resources.</p> <ul> <li><code>Jobs and runs</code>: list and status of performed runs</li> <li><code>Models</code>: number and list of latest models</li> <li><code>Functions and code</code>: number and list of latest functions</li> <li><code>Data items</code>: number and list of latest data items</li> <li><code>Artifacts</code>: number and list of latest artifacts</li> </ul> <p>You can return to the list of projects at any time by clicking Projects at the bottom of the left menu, or switch directly to a specific project by using the drop-down menu in the upper left of the interface.</p> <p></p>"},{"location":"tasks/projects/#update","title":"Update","text":"<p>To update a project's <code>Metadata</code>, first click <code>Configuration</code> in the left menu.</p> <p></p> <p>Click <code>Edit</code> in the top right and the edit form for <code>Metadata</code> properties will be shown. In the example below, a label was added.</p> <p></p> <p>When you're done updating the project, click Save.</p>"},{"location":"tasks/projects/#share","title":"Share","text":"<p>To allow other users to view and interact with the project, you must share it with them. From the <code>Configuration</code> page, click <code>Share</code> in the upper right.</p> <p></p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>You can delete a project from the <code>Configuration</code> page, by clicking <code>Delete</code>. You will be asked to confirm by entering the project's identifier.</p> <p></p>"},{"location":"tasks/projects/#management-via-sdk","title":"Management via SDK","text":"<p>Projects can be created and managed as entities with the SDK. Check the SDK Project documentation for more information.</p>"},{"location":"tasks/resources/","title":"Resource Management with KRM","text":"<p>Different platform entities are associated with and represented as Kubernetes resources: they are deployed as services, user volumes and secrets, captured as Custom Resources, etc. Kubernetes Resource Manager (KRM) component allows for performing various operations over these resources depending on their kind.</p> <p></p> <p>KRM navigation menu provides access to different types of resources. This includes both standard resources (Services, Deployments, Persistent Volume Claims, Secrets) and custom resources based on Custom Resource Definitions currently installed on the platform. Some custom resources are managed with the customized UI (e.g., PostgreSQL instances, PostgREST Data services o Dremio Data service), while the others may be managed with the standard UI based on their JSON schema.</p>"},{"location":"tasks/resources/#management-of-standard-kubernetes-resources","title":"Management of Standard Kubernetes Resources","text":"<p>KRM allows for accessing and managing the standard K8S resources relevant for the DigitalHub platform: space (through Persistent Volume Claims), services and deployments, and secrets.</p>"},{"location":"tasks/resources/#listing-k8s-services","title":"Listing K8S Services","text":"<p>Accessing the <code>Services</code> menu of the KRM, it is possible to list the (subset of) services deployed on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each service KRM shows its name, type (e.g., Coder workspace type), exposed port type and value. In the service details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#listing-k8s-deployments","title":"Listing K8S Deployments","text":"<p>Accessing the <code>Deployments</code> menu of the KRM, it is possible to list the (subset of) deployments on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each deployment KRM shows its name and availability of instances. In the deployment details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#managing-persistent-volume-claims","title":"Managing Persistent Volume Claims","text":"<p>In certain cases, the operations developed with the platform may require more substantial disk space, e.g., for training / producing significant amounts of data. In this case, it is possible to attach to the tasks the corresponding Persistent Volume Claim (PVC) references. To create a new PVC for the use of the pipeline or Job, KRM provides the corresponding interface.</p> <p>Accessing <code>Persistent Volume Claims</code> menu, it is possible to list and manage the PVCs of the platform.</p> <p></p> <p>For each PVC, you can see the status (Pending or Bound) of the PVC, the name of the volume (if specified), the storage class and the size in Gi. The details view provides further metadata regarding the PVC.</p> <p>It is also possible to delete the PVC and create new ones.</p> <p>Deleting PVC</p> <p>Please note that deleting a PVC bound to a Pod or a Job may affect negatively their execution.</p> <p>To create a new PVC, provide the following:</p> <ul> <li>Name: Name of the resource</li> <li>Space: Disk space request</li> <li>Storage class name: Storage class name (select one of the available in your deployment)</li> <li>Volume: Name of the volume (Important! Can be specified only if a volume already exists, otherwise, the PVC cannot be bound.)</li> <li>Access modes: Access modes (standard K8S values)</li> <li>Mode: PVC mode (Filesystem or Block)</li> </ul> <p></p>"},{"location":"tasks/resources/#listing-k8s-secrets","title":"Listing K8S Secrets","text":"<p>Accessing the <code>Secrets</code> menu of the KRM, it is possible to list the (subset of) secrets on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each secret KRM shows its name, type, and number of elements. In the secret details view it is possible to access other metadata and also a list of secret elements. The values are not available directly; to retrieve the actual value of the secret element, use <code>Decode</code> button that will copy the decoded content of the secret into Clipboard.</p> <p></p>"},{"location":"tasks/resources/#managing-custom-resources","title":"Managing Custom Resources","text":"<p>KRM allows for the management of generic CRs as well as for the management of some predefined ones, such as PostgreSQL instances, PostgREST and Dremio Data services.</p>"},{"location":"tasks/resources/#managing-postgresql-instances-with-krm","title":"Managing PostgreSQL instances with KRM","text":"<p>Using PostgreSQL operator (https://github.com/movetokube/postgres-operator) it is possible to create new DB instances and the DB users to organize the data storage.</p> <p>Accessing <code>Postgres DBs</code> menu of the KRM, it is possible to list, create, and delete PostgreSQL databases.</p> <p></p> <p>To create a new Database, provide the following:</p> <ul> <li>name of the database to create</li> <li>whether to drop the DB on resource deletion</li> <li>Comma-separated list of PostgreSQL extensions to enable (e.g., timescale and/or postgis) as supported by the platform deployment (optional).</li> <li>Comma-separated list of schemas to create in DB (optional)</li> <li>Name of the master role for the DB access management (optional)</li> </ul> <p></p> <p>In the Database details view it is possible also to configure the DB users that can access and operate the Database (create, edit, view, delete). To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the user to be created</li> <li>access privileges (e.g., Owner, Read, or Write)</li> <li>name of the secret to be create to store the user credentials and DB access information. This results in creating a secret  <code>&lt;user-cr-name&gt;-&lt;secret-name&gt;</code> that can be accessed in the Secrets section of KRM.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-s3-resources-with-krm","title":"Managing S3 resources with KRM","text":"<p>If supported by the deployment, using Minio S3 operator (http://github.com/scc-digitalhub/minio-operator/) it is possible to create new S3 buckets, policies, and create/associate users to them.</p> <p>Accessing <code>S3 Buckets</code> menu of the KRM, it is possible to list, create, and delete S3 buckets, policies, and users.</p> <p></p> <p>To create a new Bucket, provide the following:</p> <ul> <li>name of the bucket to create</li> <li>optional quota for the bucket</li> </ul> <p></p> <p>In the S3 Policies tab view it is possible also to configure the S3 policies. To create a new policy, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the policy to be created</li> <li>standard S3 policy spec in JSON format.</li> </ul> <p></p> <p>In the S3 Users tab view it is possible also to configure the S3 users. To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>access key for the user to be created</li> <li>secret key for the user</li> <li>list of policies to associate to the user.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-postgrest-data-services-with-krm","title":"Managing PostgREST Data Services with KRM","text":"<p>You can deploy new PostgREST data services through KRM. A PostgREST service exposes a set of PostgreSQL tables through a REST API, allowing to query and even modify them.</p> <p>Access <code>PostgREST Data Services</code> on the left menu.</p> <p></p> <p>When creating a new PostgREST service, fields are as follows:</p> <ul> <li>Name of the resource</li> <li>Schema to expose</li> <li>Existing DB user (role) on behalf of which the service will operate OR the list of actions to enable and the list of tables which will be exposed and on which these actions may be performed. The user will be created automatically for this second option.</li> <li>Connection information.<ul> <li>If you choose not to provide an existing secret, Host, Database, User and Password are required.</li> <li>If you decide to use a secret, you must provide the secret's name. Two possible configurations are valid:<ul> <li>If the secret contains <code>POSTGRES_URL</code> (the full connection string, as <code>postgresql://user:password@host:port/database</code>), all other connection fields will be ignored.</li> <li>If the secret contains <code>USERNAME</code> and <code>PASSWORD</code>, Host and Database must be provided, or the resource will enter an error state.</li> </ul> </li> </ul> </li> </ul> <p>Port is optional and defaults to <code>5432</code> if not provided.</p> <p>Extra connection parameters may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable SSL: <code>sslmode=disable</code>.</p> <p>Connection information</p> <p>Please note that when no existing DB user is specified, the user specified in the Connection section must have sufficient privileges to manage roles. By default, the owner/writer/reader users created by the Postgres operator do not have this permission.</p> <p>Schema exposure</p> <p>PostgREST exposes all tables and views in the specified schema. In order to have better control over the exposed data, it is recommended to create a separate schema (e.g., <code>api</code>) and provide access to data through views or stored procedures. You can use SQLPad to do this.</p> <p></p> <p>Check out the official documentation for more information on PostgREST.</p>"},{"location":"tasks/resources/#managing-dremio-data-services-with-krm","title":"Managing Dremio Data Services with KRM","text":"<p>You can deploy new Dremio data services through KRM. A Dremio data service exposes Dremio data through a REST API.</p> <p>Access <code>Dremio Data Services</code> on the left menu.</p> <p></p> <p>To create a new service, provide the following:</p> <ul> <li>Name of the resource</li> <li>List of virtual datasets to expose</li> <li>Connection information.<ul> <li>Host is required.</li> <li>Port is optional and will default to <code>32010</code> if not provided.</li> <li>User and Password are required, unless you choose to use a secret, in which case the secret's name must be provided. The secret should contain <code>USER</code> and <code>PASSWORD</code>.</li> <li>Extra connection parameters are optional and may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable certificate verification: <code>useEncryption=false&amp;disableCertificateVerification=true</code>.</li> </ul> </li> </ul> <p>If you instantiated Dremio through Coder, the value of Host is the value of Arrow Flight Endpoint, stripped of <code>:</code> and port. User is <code>admin</code> and Password is the value you entered when creating the workspace.</p> <p></p> <p>A Dremio REST service will be deployed, connected to the specified Dremio instance and exposing a simple REST API over the listed datasets.</p>"},{"location":"tasks/resources/#exposing-services-externally","title":"Exposing services externally","text":"<p>Various APIs and services (e.g., PostgREST or Dremio data services, serverless functions) may be exposed externally, outside of the platform, on a public domain of the platform. Using KRM, the operation amounts to defining a new API gateway resource that will be transformed into the corresponding ingress routing specification.</p> <p></p> <p>To create a new API gateway, provide the following:</p> <ul> <li>Name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Kubernetes service to be exposed (select it from the dropdown list and port will automatically be provided).</li> <li>Host defines the full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>myservice.services.example.com</code>.</li> <li>Relative path to expose the service on.</li> <li>Authentication information. Currently, services may be unprotected (<code>None</code>) or protected with <code>Basic</code> authentication, specifying username and password.</li> </ul>"},{"location":"tasks/resources/#defining-and-managing-crd-schemas","title":"Defining and Managing CRD Schemas","text":"<p>To have a valid representation of the CRs in the system, it is necessary to have a JSON specification schema for each CRDs. Normally, such schema is provided with the CRD definition and is used by KRM to manage the resources. However, in certain cases a CRD may have no structured schema definition attached. To allow for managing such resources, it is possible to provide a custom schema for the CRD.</p> <p>Creating a schema is fairly simple. Access the Settings section from the left menu and click Create.</p> <p>The CRD drop-down menu will list all Custom Resource Definitions available on the Kubernetes instance; when you pick one, the Version field will automatically be filled with the version of the currently active schema.</p> <p>Provide the Schema definition and save it in KRM for future CR management.</p>"},{"location":"tasks/run-resources/","title":"Configuring resource-critical executions","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on the standard Kubernetes functionality and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul> <p>In the platform, these requirements may be defined in two ways.</p> <p>First, it is possible to configure them explictly when defining the function run, either via Core UI or via SDK. Please note that it is possible to describe only some of these properties, leaving the rest blank without constraints. All the defaults are managed by the underlying Kubernetes deployment.</p> <p>Second, it is possible to rely on a set of preconfigured HW profiles defined by the platform deployment. The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p>"},{"location":"tasks/secrets/","title":"Secret Management","text":"<p>Working with different operations may imply the usage of a sensitive values, such as external API credentials, storage credentials, etc.</p> <p>In order to avoid embedding the credentials in the code of functions, the platform supports an explicit management of credentials as secrets. This operation exploits the underlying secret management subsystem, such as Kubernetes Secret Manager.</p> <p>Besides the secrets managed natively by the platform to integrate e.g., default storage credentials, it is possible to define custom secrets at the level of a single project. The project secrets are managed as any other project-related entities, such as functions, dataitems, etc.</p> <p>At the level of the project the secrets are represented as key-value pairs. The management of secrets is delegated to a secret provider, and currently only Kubernetes Secret Manager is supported. Each project has its own Kubernetes secret, where all the key-value pairs are stored.</p> <p>To create a new secret value it is possible to use the Core UI console or directly via API, e.g., using the SDK.</p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-ui","title":"Creating and Managing Secrets via UI","text":"<p>Core console can be used to manage project secrets. To create a new one, it is necessary to provide a secret key and a value to be stored.</p> <p></p> <p>The entries may be then deleted and updated, as well as their metadata.</p>"},{"location":"tasks/secrets/#managing-secrets-via-sdk","title":"Managing Secrets via SDK","text":"<p>Secrets can be created and managed as entities with the SDK CRUD methods. Check the SDK Secrets documentation for more information.</p>"},{"location":"tasks/triggers/","title":"Triggers and automation","text":"<p>Triggers define a way for users to describe a set of conditions which should result in the execution of a function's task, i.e. a run.</p> <p>Triggers solve the problem of automatically executing a function when a specific condition is achieved, such as a schedule, an event, a change of status of a persisted entity. Their purpose is to let users define simple automations, which execute a task based on a given variable input, following the  not to implement pipelines.</p> <p>Every trigger must be bound to a specific function's task: when the trigger fires, the result will be an execution of that very specific task, i.e. a run.</p> <p>Triggers are handled by actuators, dedicated, internal components which will process a trigger's definition and then monitor the conditions required for firing the trigger. For example, with a scheduler trigger defining a cron expression, the actuator will fire the trigger every time the expression is satisfied.</p> <p>The platform handles the persistence, consistency and lifecycle of triggers by storing them as entities related to functions.</p> <p>Users can run or stop triggers at will, enabling or disabling the execution.</p>"},{"location":"tasks/triggers/#templates-and-inputs","title":"Templates and inputs","text":"<p>Triggers are designed to produce a function's run every time they are fired. In the platform, a run is a self described, fully enclosed definition of a function's execution. As such, the configuration (context, resources, parameters, inputs, secrets...) is passed as spec for the run. In the trigger flow, the spec is dynamically built by the platform, leveraging the context of the user and the triggers execution. Every parameter meant to go in the run's spec has to be either statically pre-defined or dynamically generated at runtime. </p> <p>The trigger's spec includes a <code>template</code> field which serves as the base used to derive the run's <code>spec</code>: static parameters are copied, while the context and the environment are built by core. This solution can satisfy basic needs, such as \"execute this function every day\", but fails to cover more complex uses, where the function's parameters must be dynamically populated.</p> <pre><code>template:\n  envs:\n    - name: MY_ENV\n      value: VALUE\n  parameters:\n    param1: value123\n  resources:\n    cpu:\n      requests: \"1\"\n    mem:\n      requests: 64Mi\n</code></pre> <p>To solve this need, triggers can produce inputs, based on their context and the specific execution event. These inputs can be consumed by user-defined functions to perform specific, parametrized tasks: for example a validation function could perform a check on a given data table. In order to consume inputs, the function must accept variable parameters.</p> <p>For example, a function could receive a parameter shaped as a core entity as follow:</p> <pre><code>def validate(project, di:DataItem):\n    # download dataitem as local file\n    path = di.download(overwrite=True)\n\n...\n</code></pre> <p>This function could be used via a lifecycle trigger to perform an automation which will validate every data table stored in the project's repository, by consuming the input produced by the trigger as function parameter.</p> <p>Core supports a simple templating language, based on Mustache, which lets developers define placeholders inside the run's template, and have those placeholders inflated with the actual value at runtime.</p> <p>Following the previous example, the template could expose the data item key for the python code to consume by defining the parametrized value in the template:</p> <pre><code>template:\n  inputs:\n    di: \"{{input.key}}\"\n</code></pre> <p>This definition will instruct the templating engine in producing a run with a named input <code>di</code> valorized as the store key of the data item triggering the execution. Every resulting run will then receive a different, specific input enabling the automation to work.</p> <p>The resulting spec for an execution triggered by entity <code>my-test-data</code> will be:</p> <pre><code>spec:\n  inputs:\n    di: store://my-prj/dataitem/table/my-test-data:00...\n</code></pre>"},{"location":"tasks/triggers/#scheduler-trigger","title":"Scheduler trigger","text":"<p>The platform supports a basic, cron-like scheduler which will execute the associated task every time the cron expression is verified.</p> <p>The only configuration parameter is: * schedule: a cron-like expression describing the requested interval or time</p> <p>For example: <pre><code>spec: \n    schedule: 0 * 0 ? * * * #every minute\n</code></pre></p> <p>The expression is based on Quartz syntax, see Quartz doc for reference.</p> <p>Additionally, the scheduler supports the <code>@</code> syntax for repeated tasks, supporting:</p> <ul> <li><code>@hourly</code> for tasks repeated every hour</li> <li><code>@daily</code> for tasks repeated every day</li> <li><code>@weekly</code> for tasks repeated every week</li> <li><code>@monthly</code> for tasks repeated every month</li> </ul> <p>Example <pre><code>spec: \n    schedule: '@hourly' #every hour\n</code></pre></p>"},{"location":"tasks/triggers/#lifecycle-trigger","title":"Lifecycle trigger","text":"<p>The lifecycle actuator monitors the lifecycle of entities stored in the platform and can trigger a function's execution on state changes, such as files being uploaded or models being created. This lets users define simple automations for reacting to changes in the platform's store such as validating a content, or deploying a new version of a given model.</p> <p>The template exposes an <code>input</code> object which contains the full definition of the entity triggering the specific event. Developers can consume the input as they see fit, by instrumenting their templates with variables and then consuming the content as parameters for the functions.</p> <p>In order to define which content is relevant for a given trigger, the spec requires the following parameters to be defined:</p> <ul> <li> <p>key: a store key accessor, with wildcard support, describing the content. The accessor can be as specific as a full key, identifying a single version of an item, or be more relaxed to match every version of the same item, all items of the same kind, or all items in the store. </p> </li> <li> <p>states: a list of states (CREATED,UPLOADING,READY,ERROR,DELETED) which will be used to detect an event of interest.</p> </li> </ul> <p>For example, the following will produce a function's run every time a table with the name ending in <code>.csv</code> is uploaded in the store and ready for usage:</p> <pre><code>spec:\n    key: \"store://my-proj/dataitem/table/*.csv\"\n    states:\n    - READY\n</code></pre> <p>Note: the key must match the current project.</p>"},{"location":"tasks/triggers/#management-via-ui","title":"Management via UI","text":"<p>Triggers can be managed via the user console, by navigating to the function's task of interest and then filling the <code>Create trigger</code> form in every detail.</p> <p></p> <p>Template parameters can be filled in under the Task and Run sections, using variable expansion via <code>{VAR}</code> when needed.</p> <p></p> <p>Afterwards, triggers can be managed from their section as a normal entity.</p> <p></p>"},{"location":"tasks/triggers/#management-via-sdk","title":"Management via SDK","text":"<p>Triggers are managed as entities with the SDK CRUD methods, connected to functions. Check the SDK Functions documentation for more information.</p>"},{"location":"tasks/workflows/","title":"Workflows","text":"<p>Workflows allow for organizing the single operations in a advanced management pipelines, to perform a series operation of data processing, ML model training and serving, etc. Workflows represent long-running procedures defined as Directed Acyclic Graphs (DAGs) where each node is a single unit of work performed by the platform (e.g., as a Kubernetes Job).</p> <p>As in case of functions, it is possible for the platform to have different workflow runtimes. Currently, the only workflow runtime implemented is the one based on Hera infrastructure. See Hera Runtime for further details about how the workflow is defined and executed with the Hera component of the platform.</p> <p>Similarly, to functions the workflows may be managed via console UI or via Python SDK.</p>"},{"location":"tasks/workflows/#management-via-ui","title":"Management via UI","text":"<p>Workflows can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new workflow</li> <li><code>expand</code> a workflow to see its 5 latest versions</li> <li><code>show</code> the details of a workflow</li> <li><code>edit</code> a workflow</li> <li><code>delete</code> a workflow</li> <li><code>filter</code> workflows by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete workflows using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/workflows/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the workflow</li> <li><code>Kind</code>: kind of workflow</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p>In case of a <code>hera</code> workflow, the source code and handler fields are required as well.</p>"},{"location":"tasks/workflows/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a workflow's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p> <p>In case of <code>hera</code> workflows, the executions of the workflow instances can be monitored with the corresponding DAG viewer.</p> <p></p>"},{"location":"tasks/workflows/#update","title":"Update","text":"<p>You can update a workflow by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/workflows/#delete","title":"Delete","text":"<p>You can delete a workflow from either its detail page or the list of workflows, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/workflows/#management-via-sdk","title":"Management via SDK","text":"<p>Workflows can be created and managed as entities with the SDK CRUD methods. Check the SDK Workflows documentation for more information.</p>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p> <p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>Let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}