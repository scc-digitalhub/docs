{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Digital Hub is an open-source platform for interoperability of data and services, built by integrating several open-source projects to manage, elaborate, expose and serve data through modern standards.</p> <p>This documentation is aimed at users and will illustrate the vision and architecture of the Digital Hub, teach you the purpose of its components, how to access and use them, as well as guide you through the process of developing a simple project.</p>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>AI Domain solutions are complex AI applications which integrates different ML and Data products to deliver a full AI product</li> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform adopts Nuclio as the serverless platform, and supports Python, Java and Go as programming languages.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p>The foundation for the ML layer is the open source MLRun framework, which offers an open MLOps environment for building  ML applications across their lifecycle (depicted in picture)</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with MLRun, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (via MLRun), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"architecture/#ai-solutions","title":"AI solutions","text":"<p>Building over the base layers, the platform aims at supporting full scale AI solutions, which encompass the full stack of tools and resources required to deliver a specific solution in the domain.</p> <p>Datasets, ML models, workflows, services all contribute to the construction of a fully integrated and replicable AI solution.</p> <p>Note: the AI layer is still in early development phase.</p>"},{"location":"core/","title":"Core","text":"<p>The Digital Hub relies on a number of core software for its essential needs like storage, servicing and deployment of applications.</p>"},{"location":"core/#storage","title":"Storage","text":"<p>The platform uses a unified store composed by two types of storage:</p> <ul> <li>MinIO, a S3-compatible object store as persistence store for (un)structured, immutable data</li> <li>Postgres, a relational database as operational store for mutable data, rich with extensions, including ones for geospatial and time-series data.</li> </ul> <p>By using Dremio, data can be viewed, accessed and queried in a unified way. This component is described in higher detail in its own section.</p>"},{"location":"core/#function-as-a-service","title":"Function-as-a-Service","text":"<p>Nuclio allows deploying and hosting serverless functions, most notably written in Python, but also other languages such as Java or Go, that may be executed as REST APIs. An API gateway can then be created to publish them and optionally require authentication. As Nuclio functions run within containers, Nuclio uses Kubernetes to handle their deployment and availability.</p> <p>Kubeflow is a tool to simplify deploying machine-learning workflows on Kubernetes, by defining DAGs where each step is a function with input/output data.</p>"},{"location":"core/#workspaces","title":"Workspaces","text":"<p>Coder enables organizations to set development tools up in the cloud, so developers can access them conveniently and within a collaborative environment. In the platform, it is used as a hub to deploy and access a number of other components.</p> <p>Among its primary features are templates, which can be used to easily create workspaces and launch tools. The Digital Hub comes with a number of templates already available.</p>"},{"location":"core/#monitoring","title":"Monitoring","text":"<p>TODO - Monitoring features are work-in-progress.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-on-minikube","title":"Installation on minikube","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>Minikube</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<ol> <li>Start minikube (change 192.168.49.0 if your network setup is different): <pre><code>    minikube start --insecure-registry \"192.168.49.0/24\" --memory 12288 --cpus 4\n</code></pre></li> <li>Get minikube external IP: <pre><code>    minikube ip\n</code></pre></li> <li>Change the IP in  'global.registry.url' and 'global.externalHostAddress' properties in values file (chart/digitalhub/values.yaml) with the one obtained in the previous step.</li> <li>Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></li> <li>Install DigitalHub with Helm: <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --timeout 15m0s\n</code></pre></li> <li>Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></li> </ol> <p>Once installed, you should see the references (URLs) for the different tools of the platform.</p>"},{"location":"installation/#install-with-ms-azure","title":"Install with MS Azure","text":"<p>TODO</p>"},{"location":"installation/#install-with-openstack","title":"Install with OpenStack","text":"<p>TODO</p>"},{"location":"components/dashboard/","title":"Landing Page","text":"<p>The landing page is a central access point to reach a number of tools that are automatically run when the platform is installed. It provides the  access to the platform components and to the monitoring subsystem of the platform.</p> <p></p> <p>Components</p> <ul> <li>DH Core Console, UI for the Digital Hub management</li> <li>KRM, or Kubernetes Resource Manager, is the tool for organizing and managing standard and custom Kubernetes resources</li> <li>MLRun, a framework for MLOps</li> <li>MinIO, an S3-compatible object datalake</li> <li>Nuclio, a platform for serverless functions</li> <li>Kubeflow, a tool for ML workflows on Kubernetes</li> </ul> <p>Monitoring</p> <p>TODO - Monitoring features are work-in-progress.</p>"},{"location":"components/dh_console/","title":"Digital Hub Console","text":"<p>The Digital Hub console is a front-end application backed by the Digital Hub Core API. It is a management interface to Core objects, namely:</p> <ul> <li>functions for all the configured runtimes (see the Runtimes documentation section for more information)</li> <li>dataitems</li> <li>artifacts</li> </ul> <p>Such objects are project-scoped. When you access the console, you land to the project management page, where you can create or delete projects.</p>"},{"location":"components/dh_console/#create-a-project","title":"Create a Project","text":"<p>In order to create a new project, press the button on the first element of the list</p> <p></p> <p>Now you can fill the form with the data of your new project, adding Name, Description and its Metadata </p> <p>Following the selection of a project, you can get an overview of the associated objects on its dashboard and manage them on the dedicated pages.</p>"},{"location":"components/dh_console/#dashboard","title":"Dashboard","text":"<p>The console dashboard shows the resources that have been created with a series of cards and allows you to quickly access them. In addition to the artifacts, data items and functions, the last card shows the runs present and their respective status </p>"},{"location":"components/dh_console/#objects","title":"Objects","text":""},{"location":"components/dh_console/#functions","title":"Functions","text":""},{"location":"components/dh_console/#dataitems","title":"Dataitems","text":""},{"location":"components/dh_console/#artifacts","title":"Artifacts","text":""},{"location":"components/dh_console/#secrets","title":"Secrets","text":""},{"location":"components/dh_console/#versioning","title":"Versioning","text":"<p>Functions, dataitems and artifacts are versioned. When you visualize the details of an object, all of its versions are listed and browsable. Moreover, when you visualize a dataitem, its schema and data preview are available.</p>"},{"location":"components/dh_console/#running-functions","title":"Running functions","text":"<p>The console can be used to create function runs. When you visualize a function, different operations are available depending on its kind (i.e., its runtime). For example, when you create a Nefertem function, you can then perform either <code>validate</code>, <code>profile</code>, <code>infer</code> or <code>metric</code> tasks providing the desired run configuration.</p> <p>TODO screenshot Nefertem function</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with username <code>admin</code> and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from Coder, using its template. After launching it from Coder you can access Grafana on Grafana UI (http://nodeipaddress:30110).</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> (should be <code>mlrun</code>)</li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> (should be <code>mlrun</code>)</li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/kubeflow/","title":"KubeFlow Pipelines","text":"<p>TODO</p>"},{"location":"components/mlrun/","title":"MLRun","text":"<p>MLRun is a MLOps framework for building and managing machine-learning applications and automating the workflow.</p> <ul> <li>Ingest and transform data</li> <li>Develop ML models, train and deploy them</li> <li>Track performance and detect problems</li> </ul> <p>How to access</p> <p>MLRun may be accessed from the dashboard. From its interface, you will be able to monitor projects and workflows. After launching it from Coder you can access MLRun on MLRun UI (http://nodeipaddress:30060)</p>"},{"location":"components/mlrun/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/nuclio/","title":"Nuclio","text":"<p>TODO</p>"},{"location":"components/resourcemanager/","title":"Kubernetes Resource Manager","text":"<p>Kubernetes Resource Manager (KRM) is an application to manage several types of Kubernetes resources:</p> <ul> <li>Custom resources</li> <li>Services</li> <li>Deployments</li> <li>Volumes</li> <li>Jobs</li> </ul> <p>It consists in a back-end, written in Java, which connects to the Kubernetes API to perform actions on resources, and a front-end, written in React and based on React-admin.</p> <p>Instructions on how to install and start an instance can be found on the repository.</p>"},{"location":"components/resourcemanager/#standard-kubernetes-resources","title":"Standard Kubernetes Resources","text":"<p>With KRM you can control the main Kubernetes resources (e.g., services, deployments), manage Persistent Volume Claims, and access the secrets.Click the corresponding button in the left menu, and view the details of one item by clicking its Show button. </p>"},{"location":"components/resourcemanager/#custom-resources","title":"Custom resources","text":"<p>Custom resources can be viewed, created, edited and deleted through the use of the interface. </p> <p>If you don't see a specific kind of custom resource listed to the left, it means neither Kubernetes nor KRM contain a schema for it. A schema is required so that the application may understand and describe the related resources.</p> <p>If some resources already exist, they will be immediately be visible.</p>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"runtimes/container/","title":"Container runtime","text":"<p>The Container runtime allows you to deploy deployments, jobs and services on Kubernetes.</p>"},{"location":"runtimes/container/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python 3.9 or 3.10</li> <li>digitalhub sdk</li> </ul> <p>Install digitalhub sdk and collect digitalhub container modules:</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/\npip install core/modules/container/ --no-deps\n</code></pre>"},{"location":"runtimes/container/#function","title":"Function","text":"<p>The Container runtime introduces a function of kind <code>container</code> that allows you to deploy deployments, jobs and services on Kubernetes.</p>"},{"location":"runtimes/container/#container-function-parameters","title":"Container function parameters","text":"<p>When you create a function of kind <code>container</code>, you must specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>container</code></li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>image</code>: the container image to deploy</li> <li><code>base_image</code>: the base container image to use in the build task</li> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>source_remote</code>: the remote source of the function (git repository)</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> <li><code>base_image</code>: the base container image.</li> <li><code>command</code>: the command to run inside the container.</li> <li><code>args</code>: the arguments to pass to the entrypoint.</li> </ul> <p>For example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project('my_project')\nfunction = dh.new_function(\n    kind='dbt',\n    name='my_function',\n    image=\"hello-world:latest\"\n)\n</code></pre>"},{"location":"runtimes/container/#task","title":"Task","text":"<p>The Container runtime introduces three task's kinds:</p> <ul> <li><code>job</code>: to deploy a job</li> <li><code>deploy</code>: to deploy a deployment</li> <li><code>serve</code>: to deploy a service</li> <li><code>build</code>: to build an image</li> </ul>"},{"location":"runtimes/container/#run-and-task-parameters","title":"Run and task parameters","text":"<p>When you want to execute a task, you must pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. Can be <code>job</code>, <code>deploy</code>, <code>serve</code> or <code>build</code>.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For the <code>serve</code> action, you can also pass the following task parameters:</p> <ul> <li><code>service_ports</code>: a list of ports to expose</li> <li><code>service_type</code>: the type of service</li> </ul> <p>For example:</p> <pre><code>run = function.run(\n    action='job'\n)\n</code></pre>"},{"location":"runtimes/container/#notes","title":"Notes","text":"<p>The Container runtime does not support local execution.</p>"},{"location":"runtimes/container/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\nproj = dh.get_or_create_project(\"project-container\")\n\n# Run container\nfunc_cont = proj.new_function(name=\"function-container\",\n                              kind=\"container\",\n                              image=\"hello-world:latest\")\nrun_cont = func_cont.run(\"job\")\n</code></pre>"},{"location":"runtimes/dbt/","title":"DBT runtime","text":"<p>The DBT runtime allows you to run DBT transformations on your data. It is a wrapper around the DBT CLI tool. The runtime introduces a function of kind <code>dbt</code> and a task of kind <code>transform</code>.</p>"},{"location":"runtimes/dbt/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python 3.9 or 3.10</li> <li>digitalhub sdk</li> <li>dbt-postgres</li> </ul> <p>We need first to install dbt:</p> <pre><code>pip install dbt-postgres==1.6.7 pandas==2.1.4\n</code></pre> <p>and then we can install digitalhub sdk and collect digitalhub dbt modules</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/ data/ ./\npip install data/modules/dbt\n</code></pre> <p>If you want to exeute the dbt runtime only remotely, you can avoid to install dbt.</p>"},{"location":"runtimes/dbt/#function","title":"Function","text":"<p>The DBT runtime introduces a function of kind <code>dbt</code> that allows you to execute sql dbt queries on your data.</p>"},{"location":"runtimes/dbt/#dbt-function-parameters","title":"DBT function parameters","text":"<p>When you create a function of kind <code>dbt</code>, you need to specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>dbt</code></li> <li><code>source</code>: the sql query to execute. See section below</li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function (git repository)</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> </ul>"},{"location":"runtimes/dbt/#source","title":"Source","text":"<p>The <code>source</code> parameter must be a dictionary containing reference to the sql query to be executed. The parameter is structured as a dictionary with the following keys:</p> <ul> <li><code>source</code>: the source URI to the code. It accepts the following values:<ol> <li>git+https://repo-host/repo-owner/repo.git#indication-where-to-checkout: the code is fetched from a git repository. The link points to the root of the repository, the fragment is as simple indication of the branch, tag or commit to checkout. The runtime will clone the repository and checkout the indicated branch, tag or commit.</li> <li>zip+s3://path-to-some-code.zip: the code is fetched from a zip file in the minio digitalhub instance. The link points to the path to the zip file. The runtime will download the zip file and extract it. It fails if the zip file is not valid.</li> </ol> </li> <li><code>code</code>: the python string code</li> <li><code>base64</code>: the base64 encoded code</li> <li><code>lang</code>: the language of the code use in the console higlihter</li> </ul> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my_project\")\n\nsql = \"\"\"\nSELECT * FROM {{ ref(\"my_table\") }}\n\"\"\"\n\ndataitem = project.new_dataitem(\"my_dataitem\", kind=\"table\", path=\"path-to-some-data\")\n\nfunction = dh.new_function(\n    kind=\"dbt\",\n    name=\"my_function\",\n    source={\"code\": sql}\n)\n</code></pre>"},{"location":"runtimes/dbt/#task","title":"Task","text":"<p>The DBT runtime introduces a task of kind <code>transform</code> that allows you to run a DBT transformation on your data.</p>"},{"location":"runtimes/dbt/#transform-task-parameters","title":"Transform task parameters","text":"<p>When you want to execute a task of kind <code>transform</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>transform</code>.</li> <li><code>inputs</code>: the list of referenced tables in the sql query mapped to the dataitem keys.</li> <li><code>outputs</code>: a list containing one element that map the key <code>output_table</code> with a name of the output query table and output dataitem.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>Example:</p> <pre><code>run = function.run(\n    action=\"transform\",\n    inputs=[{\"my_table\": my_dataitem.key}],\n    outputs=[{\"output_table\": \"my_output_table\"}],\n)\n</code></pre>"},{"location":"runtimes/dbt/#runtime-workflow","title":"Runtime workflow","text":"<p>The DBT runtime execution workflow is the following:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally. The runtime tries to get the file from the <code>path</code> attribute in the dataitem specification. At the moment, we support the following path types:<ul> <li><code>http(s)://&lt;url&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>sql://&lt;database&gt;(/&lt;schema-optional&gt;)/&lt;table&gt;</code></li> <li><code>&lt;local-path&gt;</code></li> </ul> </li> <li>The runtime inserts the data into a temporary versioned table in the default postgres database. These tables are named <code>&lt;dataitem-name&gt;_v&lt;dataitem-id&gt;</code>, and will be deleted at the end of the execution.</li> <li>The runtime creates all the necessary DBT artifacts (profiles.yml, dbt_project.yml, etc.) and runs the DBT transformation.</li> <li>The runtime stores the output table into the default postgres database as result of the DBT execution. The table name is built from the <code>outputs</code> parameter. Then, the runtime creates a dataitem with the <code>outputs</code> name parameter and saves it into the Core backend. You can retrieve the dataitem with the <code>run.outputs()</code> method. In general, the output table versioned is named <code>&lt;dataitem-output-name&gt;_v&lt;dataitem-output-id&gt;</code> and is stored in the default postgres database passed to the runtime via env variable.</li> </ol>"},{"location":"runtimes/dbt/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-dbt\")\n\n# Create new input dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\ndi = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n\n# Create new function\nsql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref(\"employees\") }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = \"60\"\n\"\"\"\nfunction = project.new_function(name=\"function-dbt\",\n                                kind=\"dbt\",\n                                source={\"code\": sql})\n\n# Run function\nrun = function.run(\"transform\",\n                   inputs=[{\"employees\": di.key}],\n                   outputs=[{\"output_table\": \"department-60\"}])\n\n# Refresh run\nrun.refresh()\n</code></pre>"},{"location":"runtimes/mlrun/","title":"Mlrun runtime","text":"<p>The mlrun runtime allows you to execute mlrun function. It's a wrapper around mlrun methods. The runtime introduces a function of kind <code>mlrun</code> and a task of kind <code>job</code>.</p>"},{"location":"runtimes/mlrun/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python 3.9 or 3.10</li> <li>digitalhub sdk</li> <li>mlrun</li> </ul> <p>We need first to collect digitalhub mlrun modules:</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/ data/ ml/ ./\npip install -r ml/modules/mlrun/requirements-wrapper.txt\npip install -r ml/modules/mlrun\n</code></pre> <p>If you want to exeute the mlrun runtime only remotely, you can avoid to install the requirement-wrapper.</p>"},{"location":"runtimes/mlrun/#function","title":"Function","text":"<p>The mlrun runtime introduces a function of kind <code>mlrun</code> that allows you to execute sql mlrun queries on your data.</p>"},{"location":"runtimes/mlrun/#mlrun-function-parameters","title":"Mlrun function parameters","text":"<p>When you create a function of kind <code>mlrun</code>, you need to specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>mlrun</code></li> <li><code>source</code>: the source dictionary that contains the code, encoded code or path to code to be executed by mlrun. See section below</li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function (git repository)</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> </ul>"},{"location":"runtimes/mlrun/#source","title":"Source","text":"<p>The <code>source</code> parameter must be a dictionary containing reference to the sql query to be executed. The parameter is structured as a dictionary with the following keys:</p> <ul> <li><code>source</code>: the source URI to the code. It accepts the following values:<ol> <li>git+https://repo-host/repo-owner/repo.git#indication-where-to-checkout: the code is fetched from a git repository. The link points to the root of the repository, the fragment is as simple indication of the branch, tag or commit to checkout. The runtime will clone the repository and checkout the indicated branch, tag or commit.</li> <li>zip+s3://path-to-some-code.zip: the code is fetched from a zip file in the minio digitalhub instance. The link points to the path to the zip file. The runtime will download the zip file and extract it. It fails if the zip file is not valid.</li> </ol> </li> <li><code>code</code>: the python string code</li> <li><code>base64</code>: the base64 encoded code</li> <li><code>lang</code>: the language of the code use in the console higlihter</li> </ul> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project('my_project')\n\npath = 'path-to-some-code.py'\ndataitem = project.new_dataitem(\"my_dataitem\", kind=\"table\", path=\"path-to-some-data\")\n\nfunction = dh.new_function(\n    kind='mlrun',\n    name='my_function',\n    source={\"source\": path}\n)\n</code></pre>"},{"location":"runtimes/mlrun/#task","title":"Task","text":"<p>The mlrun runtime introduces a task of kind <code>job</code> that allows you to execute a mlrun function.</p>"},{"location":"runtimes/mlrun/#job-task-parameters","title":"Job task parameters","text":"<p>When you want to execute a task of kind <code>job</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>job</code>.</li> </ul> <p>The following parameters are optional, but usually you need to pass them:</p> <ul> <li><code>inputs</code>: the list of referenced items used in the mlrun function.</li> <li><code>outputs</code>: a list referenced items produced by the mlrun function.</li> <li><code>parameters</code>: a dictionary of parameters to pass to the mlrun function <code>mlrun.run_function()</code></li> <li><code>values</code>: a list of output values that are not <code>artifacts</code>, <code>dataitems</code> or <code>models</code></li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>Example:</p> <pre><code>run = function.run(\n    action='job',\n    inputs=[{\"mlrun-input-param-name\": my_dataitem.key}],\n    outputs=[{\"mlrun-input-param-name\": \"my-output-name\"}],\n    parameters={\"inputs\": {\"key\": \"value\"}},\n    values=[\"simple-mlrun-output-value-name\"]\n)\n</code></pre>"},{"location":"runtimes/mlrun/#runtime-workflow","title":"Runtime workflow","text":"<p>The mlrun runtime execution workflow is the following:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally.</li> <li>It creates mlrun project and function.</li> <li>It passes the local fetched data path to the mlrun function referenced by the input key as parameter and the content of <code>parameters</code>.</li> <li>It executes the mlrun function and parses the results. It maps the outputs with the name passed in the <code>outputs</code> parameter. If the outputs are not <code>artifacts</code>, <code>dataitems</code> or <code>models</code>, the output is mapped with the <code>values</code>.</li> <li>You can retrieve the outputs with the <code>run.outputs()</code> method.</li> </ol>"},{"location":"runtimes/mlrun/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-mlrun\")\n\n# Create new input dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n\n# Create new dataitem\ndataitem = project.new_dataitem(name=\"url-dataitem\",\n                                kind=\"table\",\n                                path=url)\n\n# Create new function\ndownloader_function = project.new_function(name=\"mlrun-downloader\",\n                                           kind=\"mlrun\",\n                                           source={\"source\":\"pipeline.py\"},\n                                           handler=\"downloader\",\n                                           image=\"mlrun/mlrun\")\n\n# Run function\ndownloader_run = downloader_function.run(\"job\",\n                                         inputs=[{\"url\": dataitem.key}],\n                                         outputs=[{\"dataset\": \"dataset\"}])\n\n# Run refresh\ndownloader_run.refresh()\n</code></pre> <p>pipeline.py file:</p> <pre><code>import mlrun\nimport pandas as pd\n\n@mlrun.handler(outputs=[\"dataset\"])\ndef downloader(context, url: mlrun.DataItem):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(format='parquet')\n    return df\n</code></pre>"},{"location":"runtimes/nefertem/","title":"Nefertem runtime","text":"<p>The Nefertem runtime allows you to run Nefertem validation, profiling or inference on your data. It is a wrapper around the Nefertem library. The runtime introduces a function of kind <code>neferetm</code> and four task of kind <code>validate</code>, <code>profile</code>, <code>infer</code> and <code>metric</code>.</p>"},{"location":"runtimes/nefertem/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python &gt;= 3.9</li> <li>digitalhub</li> <li>digitalhub-data-nefertem</li> <li>Nefertem plugins available in the Nefertem repository</li> </ul> <p>If you want to execute Nefertem tasks locally, you need to install digitalhub-core-nefertem package with <code>local</code> flag:</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/ data/ ./\npip install data/modules/nefertem[local]\n</code></pre>"},{"location":"runtimes/nefertem/#function","title":"Function","text":"<p>The Nefertem runtime introduces a function of kind <code>neferetm</code> that allows you to execute various tasks on your data.</p>"},{"location":"runtimes/nefertem/#nefertem-function-parameters","title":"Nefertem function parameters","text":"<p>When you create a function of kind <code>neferetm</code>, you need to specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>neferetm</code></li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function (git repository)</li> <li><code>source_code</code>: pointer to the source code of the function</li> <li><code>constraints</code>: the constraints of the function to be applied on the data. Valid only for <code>validate</code> tasks</li> <li><code>error_report</code>: the error report output format. Valid only for <code>validate</code> tasks</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> </ul> <p>For example:</p> <pre><code>import digitalhub_core as dh\n\nconstraint = {\n  'constraint': 'type',\n  'field': 'field-name',\n  'field_type': 'string',\n  'name': 'check_country_string',\n  'resources': ['ref-source'],\n  'title': '',\n  'type': 'const-type',\n  'value': 'string',\n  'weight': 5\n}\nfunction = dh.new_function(name=\"nefertem-function\",\n                           kind=\"nefertem\",\n                           constraints=[constraint])\n</code></pre>"},{"location":"runtimes/nefertem/#task","title":"Task","text":"<p>The Nefertem runtime introduces three tasks of kind <code>validate</code>, <code>profile</code> and <code>infer</code> that allows you to run a Nefertem validation, profiling or inference on your data.</p>"},{"location":"runtimes/nefertem/#validate-task-parameters","title":"Validate task parameters","text":"<p>When you want to execute a task of kind <code>validate</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>validate</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the list of nefertem resources referenced in the constraint mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs=[{\"employees\": di.key}])\n</code></pre>"},{"location":"runtimes/nefertem/#profile-task-parameters","title":"Profile task parameters","text":"<p>When you want to execute a task of kind <code>profile</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>profile</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the list of nefertem resources referenced mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"profile\",\n                   framework=\"frictionless\",\n                   inputs=[{\"employees\": di.key}])\n</code></pre>"},{"location":"runtimes/nefertem/#infer-task-parameters","title":"Infer task parameters","text":"<p>When you want to execute a task of kind <code>infer</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>infer</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the list of nefertem resources referenced mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"infer\",\n                   framework=\"frictionless\",\n                   inputs=[{\"employees\": di.key}])\n</code></pre>"},{"location":"runtimes/nefertem/#runtime-workflow","title":"Runtime workflow","text":"<p>The Nefertem runtime execution workflow is the following:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally. The runtime tries to get the file from the <code>path</code> attribute. At the moment, we support the following path types:<ul> <li><code>http(s)://&lt;url&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>sql://&lt;database&gt;(/&lt;schema-optional&gt;)/&lt;table&gt;</code></li> <li><code>&lt;local-path&gt;</code></li> </ul> </li> <li>The runtime creates a Nefertem <code>DataResource</code> from the input dataitem. The <code>DataResource</code> is a Nefertem object that represents the data to be validated, profiled, inferred or measured.</li> <li>The runtime then create a Nefertem <code>run</code> and execute it. The Nefertem <code>run</code> executes three methods based on the task, and produces a <code>run_metadata</code> report file:</li> <li>If the task is <code>validate</code>:<ul> <li><code>run.validate()</code></li> <li><code>run.log_report()</code> -&gt; produces a <code>NefertemReport</code></li> <li><code>run.persist_report()</code> -&gt; produces one or more validation framework reports</li> </ul> </li> <li>If the task is <code>profile</code>:<ul> <li><code>run.profile()</code></li> <li><code>run.log_profile()</code> -&gt; produces a <code>NefertemProfile</code></li> <li><code>run.persist_profile()</code> -&gt; produces one or more profiling framework reports</li> </ul> </li> <li>If the task is <code>infer</code>:<ul> <li><code>run.infer()</code></li> <li><code>run.log_schema()</code> -&gt; produces a <code>NefertemSchema</code></li> <li><code>run.persist_schema()</code> -&gt; produces one or more inference framework reports</li> </ul> </li> <li>The runtime then creates an <code>Artifact</code> object for each file produced by Nefertem and saves it into the Core backend. It then uploads all the files to the default s3 storage provided. You can collect the artifacts with the <code>run.outputs()</code> method. In general, the saving path is <code>s3://&lt;bucket-from-env&gt;/&lt;project-name&gt;/artifacts/ntruns/&lt;nefertem-run-uuid&gt;/&lt;file&gt;</code>.</li> </ol>"},{"location":"runtimes/nefertem/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-nefertem\")\n\n# Create dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\ndi = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n\n# Create function\nconstraint = {\n  'constraint': 'type',\n  'field': 'SALARY',\n  'field_type': 'number',\n  'name': 'check_value_integer',\n  'title': '',\n  'resources': ['employees'],\n  'type': 'frictionless',\n  'value': 'number',\n  'weight': 5\n}\nfunction = project.new_function(name=\"function-nefertem\",\n                                kind=\"nefertem\",\n                                constraints=[constraint])\n\n# Run validate task\nrun = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs=[{\"employees\": di.key}])\n\n# Refresh run\nrun.refresh()\n</code></pre>"},{"location":"scenarios/dremio_grafana/scenario/","title":"Data transformation and usage with Dremio and Grafana","text":"<p>In this scenario we will learn how to use Dremio to transform data and create some virtual datasets on top of it. Then we will visualize the transformed data in a dashboard created with Grafana.</p> <p>In order to collect the initial data and make it accessible to Dremio, we will follow the first step of the ETL scenario, in which we download some traffic data and store it in the DigitalHub datalake.</p>"},{"location":"scenarios/dremio_grafana/scenario/#collect-the-data","title":"Collect the data","text":"<p>NOTE: the procedure is only summarized here, as it is already described in depth in the ETL scenario introduction and Collect the data pages.</p> <ol> <li>Access Jupyter from your Coder instance and create a new notebook using the <code>Python 3 (ipykernel)</code> kernel</li> <li>Set up the environment and create a project</li> <li>Set the URL to the data:</li> </ol> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\n</code></pre> <ol> <li>Create the <code>src</code> folder, define the download function and register it</li> <li>Execute it locally and wait for its completion:</li> </ol> <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre>"},{"location":"scenarios/dremio_grafana/scenario/#access-the-data-from-dremio","title":"Access the data from Dremio","text":"<p>Access Dremio from your Coder instance or create a new Dremio workspace. You should see MinIO already configured as an object storage and you should find the downloaded data in a .parquet file at the path <code>minio/datalake/projects/demo-etl/artifacts/download-data-downloader/0/dataset.parquet</code>.</p> <p>Click on the file to open Dataset Settings, verify that the selected format is <code>Parquet</code> and save it as a Dremio dataset, so that it can be queried.</p> <p>Now you can see the data either by clicking again on the dataset or via the SQL runner, by executing a query such as:</p> <pre><code>SELECT *\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\nORDER BY data, \"codice spira\"\n</code></pre> <p>Create a new Dremio space named <code>demo_etl</code>. We will create three virtual datasets and save them here.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-measurement-data","title":"Extract measurement data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic measurements to save them as a separate dataset:</p> <pre><code>SELECT \"dataset.parquet\".data, \"dataset.parquet\".\"codice spira\", \"00:00-01:00\", \"01:00-02:00\", \"02:00-03:00\", \"03:00-04:00\", \"04:00-05:00\", \"05:00-06:00\", \"06:00-07:00\", \"07:00-08:00\", \"08:00-09:00\", \"09:00-10:00\", \"10:00-11:00\", \"11:00-12:00\", \"12:00-13:00\", \"13:00-14:00\", \"14:00-15:00\", \"15:00-16:00\", \"16:00-17:00\", \"17:00-18:00\", \"18:00-19:00\", \"19:00-20:00\", \"20:00-21:00\", \"21:00-22:00\", \"22:00-23:00\", \"23:00-24:00\"\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-traffic-sensors-data","title":"Extract traffic sensors data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic sensors data (e.g. their geographical position) as a separate dataset:</p> <pre><code>SELECT DISTINCT \"dataset.parquet\".\"codice spira\", \"dataset.parquet\".tipologia, \"dataset.parquet\".id_uni, \"dataset.parquet\".codice, \"dataset.parquet\".Livello, \"dataset.parquet\".\"codice arco\", \"dataset.parquet\".\"codice via\", \"dataset.parquet\".\"Nome via\", \"dataset.parquet\".\"Nodo da\", \"dataset.parquet\".\"Nodo a\", \"dataset.parquet\".stato, \"dataset.parquet\".direzione, \"dataset.parquet\".angolo, \"dataset.parquet\".longitudine, \"dataset.parquet\".latitudine, \"dataset.parquet\".geopoint\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>spire</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#transform-hourly-measurements-into-daily-measurements","title":"Transform hourly measurements into daily measurements","text":"<p>Open the SQL runner and execute the following query, which will sum the measurement columns, each corresponding to an hour, to obtain the daily value and save it as a new dataset:</p> <pre><code>SELECT data, \"codice spira\", \"00:00-01:00\"+\"01:00-02:00\"+\"02:00-03:00\"+\"03:00-04:00\"+\"04:00-05:00\"+\"05:00-06:00\"+\"06:00-07:00\"+\"07:00-08:00\"+\"08:00-09:00\"+\"09:00-10:00\"+\"10:00-11:00\"+\"11:00-12:00\"\n+\"12:00-13:00\"+\"13:00-14:00\"+\"14:00-15:00\"+\"15:00-16:00\"+\"16:00-17:00\"+\"17:00-18:00\"+\"18:00-19:00\"+\"19:00-20:00\"+\"20:00-21:00\"+\"21:00-22:00\"+\"22:00-23:00\"+\"23:00-24:00\" AS totale_giornaliero\nFROM (\n  SELECT * FROM \"demo_etl\".misurazioni\n) nested_0;\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni_giornaliere</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#connect-grafana-to-dremio","title":"Connect Grafana to Dremio","text":"<p>Access Grafana from your Coder instance or create a new Grafana workspace. Open the left menu and navigate to Connections - Data Sources. Add a new <code>Dremio</code> data source configured as follows:</p> <ul> <li>Name: <code>Dremio</code></li> <li>URL: the Internal Endpoint you see on Coder for your Dremio workspace</li> <li>User: <code>admin</code></li> <li>Password: <code>&lt;dremio_password_set_on_coder&gt;</code></li> </ul> <p>Now you can create a dashboard to visualize Dremio data.</p> <p>An example dashboard is available as a JSON file at the <code>user/examples/dremio_grafana</code> path within the repository of this documentation. In order to use it, you can import it in Grafana instead of creating a new dashboard. You will need to update the <code>datasource.uid</code> field, which holds a reference to the Dremio data source in your Grafana instance, throughout the JSON model. The easiest way to obtain your ID is by navigating to the data source configuration page and copying it from the URL:</p> <pre><code>https://&lt;grafana_host&gt;/connections/datasources/edit/&lt;YOUR_DATASOURCE_ID&gt;\n</code></pre> <p>The dashboard includes three panels: a map of the traffic sensors, a table with the daily number of vehicles registered by each sensor and a graph of the vehicles registered monthly.</p> <p></p> <p>We can now use the dashboard to explore the data. We can either interact with the map to get the information related to each sensor, or use the dashboard filters to select different time ranges and analyze traffic evolution over time.</p>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in: <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre></p> <p>Define a function for downloading data as-is and persisting it in the data-lake: <pre><code>%%writefile \"src/download-data.py\"\n\nimport mlrun\nimport pandas as pd\nimport requests\n\n@mlrun.handler(outputs=[\"dataset\"])\ndef downloader(context, url: mlrun.DataItem):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(format='csv',sep=\";\")\n    return df\n</code></pre></p> <p>Register the function in MLRun: <pre><code>project.set_function(\"src/download-data.py\", name=\"download-data\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"downloader\")\n</code></pre></p> <p>Then, execute it (locally) as a test. Note that it may take a few minutes. <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre></p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. By default, this key follows the format <code>&lt;function-name&gt;-&lt;handler&gt;-&lt;output&gt;</code>.</p> <p>Write this key into a variable, so we can read the artifact: <pre><code>DF_KEY = 'store://datasets/demo-etl/download-data-downloader_dataset'\n</code></pre></p> <p>Load the data item and then into a data frame: <pre><code>di = mlrun.get_dataitem(DF_KEY)\ndf = di.as_df()\n</code></pre></p> <p>Run <code>df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API: <pre><code>%%writefile 'src/api.py'\n\nimport mlrun\nimport pandas as pd\nimport os\n\nDF_URL = os.environ[\"DF_URL\"]\ndf = None\n\n\ndef init_context(context):\n    global df\n    context.logger.info(\"retrieve data from {}\".format(DF_URL))\n    di = mlrun.run.get_dataitem(DF_URL)\n    df = di.as_df()\n\n\ndef handler(context, event):\n    global df\n    if df is None:\n        return context.Response(\n            body=\"\", headers={}, content_type=\"application/json\", status_code=500\n        )\n\n    # mock REST api\n    method = event.method\n    path = event.path\n    fields = event.fields\n\n    id = False\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields['page'])\n\n    if \"size\" in fields:\n        pageSize = int(fields['size'])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    res = {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n\n    return context.Response(\n        body=res, headers={}, content_type=\"application/json\", status_code=200\n    )\n</code></pre></p> <p>Register the function: <pre><code>api_fn = project.set_function(\"src/api.py\", name=\"api\", kind=\"nuclio\", image=\"mlrun/mlrun\", handler='handler')\n</code></pre></p> <p>Configure the function for deployment: <pre><code>DF_KEY = 'store://datasets/demo-etl/process-measures-process_dataset-measures'\napi_fn.set_env(name='DF_URL', value=DF_KEY)\napi_fn.with_requests(mem='64M',cpu=\"250m\")\napi_fn.spec.replicas = 1\nproject.save()\n</code></pre></p> <p>Deploy (may take a few minutes): <pre><code>api_fn.deploy()\n</code></pre></p> <p>Invoke the API and print its results: <pre><code>res = api_fn.invoke(\"/?page=5&amp;size=10\")\nprint(res)\n</code></pre></p> <p>You can also use pandas to load the result in a data frame: <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre></p>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to your Coder instance, go to the dashboard and access Nuclio. You will notice a <code>demo-etl</code> project, which we created earlier. When you access it, you will see the <code>demo-etl-api</code> function listed, but click on the API GATEWAYS tab on top instead. Then, click on NEW API GATEWAY.</p> <p>On the left, if you wish, set Authentication to Basic and choose Username and Password.</p> <p>In the middle, set any Name you want. Host must use the same domain as the other components of the Digital Hub. For example, if you access Coder at <code>coder.my-digitalhub-instance.it</code>, the Host field should use a value like <code>demo-etl-api.my-digitalhub-instance.it</code>.</p> <p>On the right, under Primary, you must enter the name of the function, in this case <code>demo-etl-api</code>.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you entered in Host! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, are available in the <code>documentation/examples/etl</code> path within the repository of this documentation.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries: <pre><code>import mlrun\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Load environment variables for MLRun: <pre><code>ENV_FILE = \".mlrun.env\"\nif os.path.exists(ENV_FILE):\n    mlrun.set_env_from_file(ENV_FILE)\n</code></pre></p> <p>Create a MLRun project: <pre><code>PROJECT = \"demo-etl\"\nproject = mlrun.get_or_create_project(PROJECT, \"./\")\n</code></pre></p> <p>Check that the project has been created successfully: <pre><code>print(project)\n</code></pre></p>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> Download the file and save it locally: <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre></p> <p>Use pandas to read the file into a dataframe: <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together: <pre><code>%%writefile \"pipeline.py\"\n\nfrom kfp import dsl\nimport mlrun\n\nURL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\n\n@dsl.pipeline(name=\"Demo ETL pipeline\")\ndef pipeline():\n    project = mlrun.get_current_project()\n\n    downloader = project.run_function(\"download-data\",inputs={'url':URL},outputs=[\"dataset\"])\n\n    process_spire = project.run_function(\"process-spire\",inputs={'di': downloader.outputs[\"dataset\"]})\n\n    process_measures = project.run_function(\"process-measures\",inputs={'di': downloader.outputs[\"dataset\"]})\n</code></pre></p> <p>Register the workflow: <pre><code>project.set_workflow(\"pipeline\",\"./pipeline.py\", handler=\"pipeline\")\n</code></pre></p> <p>And run it, this time remotely: <pre><code>project.run(\"pipeline\")\n</code></pre></p> <p>The next section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store: <pre><code>%%writefile \"src/process-spire.py\"\n\nimport mlrun\nimport pandas as pd\n\nKEYS=['codice spira','longitudine','latitudine','Livello','tipologia','codice','codice arco','codice via','Nome via', 'stato','direzione','angolo','geopoint']\n\n@mlrun.handler(outputs=[\"dataset-spire\"])\ndef process(context, di: mlrun.DataItem):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n\n    return sdf\n</code></pre></p> <p>Register the function in MLRun: <pre><code>project.set_function(\"src/process-spire.py\", name=\"process-spire\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"process\")\n</code></pre></p> <p>Run it locally: <pre><code>project.run_function(\"process-spire\", inputs={'di': DF_KEY}, local=True)\n</code></pre></p> <p>The results has been saved as an artifact in the data store. Assign its unique key to a variable, load the data item and convert it to a data frame: <pre><code>SDF_KEY = 'store://datasets/demo-etl/process-spire-process_dataset-spire'\nsdf = mlrun.get_dataitem(SDF_KEY).as_df()\n</code></pre></p> <p>Now you can view the results with <code>sdf.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot: <pre><code>df = mlrun.get_dataitem(DF_KEY).as_df()\nkeys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = df[columns]\n</code></pre></p> <p>Derive dataset for recorded traffic within each time slot for each spire: <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre></p> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function: <pre><code>%%writefile \"src/process-measures.py\"\n\nimport mlrun\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@mlrun.handler(outputs=[\"dataset-measures\"])\ndef process(context, di: mlrun.DataItem):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre></p> <p>Register it: <pre><code>project.set_function(\"src/process-measures.py\", name=\"process-measures\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"process\")\n</code></pre></p> <p>Run it locally: <pre><code>project.run_function(\"process-measures\", inputs={'di': DF_KEY}, local=True)\n</code></pre></p> <p>Inspect the resulting data artifact: <pre><code>MDF_KEY = 'store://datasets/demo-etl/process-measures-process_dataset-measures'\nmdf = mlrun.get_dataitem(MDF_KEY).as_df()\nmdf.head()\n</code></pre></p> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/etl/streamlit/","title":"Visualize data with Streamlit","text":"<p>We can take this one step further and visualize our data in a graph using Streamlit, a library to create web apps and visualize data by writing simple scripts. Let's get familiar with it.</p>"},{"location":"scenarios/etl/streamlit/#setup","title":"Setup","text":"<p>From the Jupyter notebook you've been using, write the result of the API call to a file:</p> <pre><code>with open(\"result.json\", \"w\") as file:\n    file.write(res['data'])\n</code></pre> <p>Create the script that Streamlit will run:</p> <pre><code>%%writefile 'streamlit-app.py'\n\nimport pandas as pd\nimport streamlit as st\n\nrdf = pd.read_json(\"result.json\", orient=\"records\")\n\n# Replace colons in column names as they can cause issues with Streamlit\nrdf.columns = rdf.columns.str.replace(\":\", \"\")\n\nst.write(\"\"\"My data\"\"\")\nst.line_chart(rdf, x=\"codice spira\", y=\"1200-1300\")\n</code></pre>"},{"location":"scenarios/etl/streamlit/#launch-app","title":"Launch app","text":"<p>In a new code cell, run the following to install Streamlit in the workspace. It's actually not code: the <code>!</code> at the beginning tells Jupyter to run the contents as a shell command.</p> <pre><code>!pip install streamlit\n</code></pre> <p>Similarly, run the following command. This will start hosting the Streamlit web app, so the cell will remain running. The <code>browser.gatherUsageStats</code> flag is set to <code>false</code> because, otherwise, Streamlit will automatically gather usage stats and print a warning about it. <pre><code>!streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre></p> <p>Next, go to your Coder instance and access the Jupyter workspace you've been using.</p> <p></p> <p></p> <p>Click on Ports, type <code>8501</code> (Streamlit's default port), then click the button next to it. It will open a tab to the Streamlit app, where you can visualize data!</p> <p></p> <p>The graph we displayed is very simple, but you are welcome to experiment with more Streamlit features. Don't forget to stop the above code cell, to stop the app.</p> <p>Connect to workspace remotely</p> <p>Alternatively to running shell commands from Jupyter and port-forwarding through the Coder interface, you could connect your local shell to the workspace remotely. You do not need to do this if you already used the method above.</p> <p>Login to Coder with the following command. A tab will open in your browser, containing a token you must copy and paste to the shell (it may ask for your credentials, if your browser isn't already logged in).</p> <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> <p>With this, your shell is authenticated to the Coder instance, and the following command will be able to connect your shell to the workspace remotely, while tunneling port 8501:</p> <pre><code>ssh -L 8501:localhost:8501 coder.my-jupyter-workspace\n</code></pre> <p>Install streamlit:</p> <pre><code>pip install streamlit\n</code></pre> <p>Run the app:</p> <pre><code>streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Access <code>localhost:8501</code> on your browser to view the app!</p>"},{"location":"scenarios/etl/streamlit/#as-docker-container","title":"As Docker container","text":"<p>Streamlit apps can be run as Docker containers. For this section, we will run the same application locally as a container, so you will need either Docker or Podman installed on your machine. Instructions refer to Docker, but if you prefer to use Podman, commands are equivalent: simply replace instances of <code>docker</code> with <code>podman</code>.</p> <p>Download the <code>result.json</code> file obtained previously on your machine, as we will need its data for the app. Also download the <code>streamlit-app.py</code> file.</p> <p>Create a file named <code>Dockerfile</code> and paste the following contents in it: <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY streamlit-app.py streamlit-app.py\nCOPY result.json result.json\n\nRUN pip3 install altair pandas streamlit\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit-app.py\", \"--browser.gatherUsageStats=false\"]\n</code></pre></p> <p>The Dockerfile describes how the image for the container will be built. In short, it installs the required libraries, copies the files you downloaded, then launches the Streamlit script.</p> <p>Make sure the three files are in the same directory, then open a shell in it and run the following, which builds the Docker image: <pre><code>docker build -t streamlit .\n</code></pre></p> <p>Once it's finished, you can verify the image exists with: <pre><code>docker images\n</code></pre></p> <p>Now, run a container: <pre><code>docker run -p 8501:8501 --name streamlit-app streamlit\n</code></pre></p> <p>Port already in use</p> <p>If you run into an error, it's likely that you didn't quit the remote session you opened while following the previous section, meaning port 8501 is already in use.</p> <p>Open your browser and visit <code>localhost:8501</code> to view the data!</p> <p>To stop the container, simply press Ctrl+C, then run the following to remove the container: <pre><code>docker rm -f streamlit-app\n</code></pre></p>"},{"location":"scenarios/etl-core/scenario/","title":"ETL with digitalhub-core and DBT scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding organizations, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>digitalhub-core</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, is available in the <code>documentation/examples/etl-core</code> path within the repository of this documentation, or in the path <code>tutorials/08-dbt-demo.ipynb</code> of the Jupyter instance.</p>"},{"location":"scenarios/etl-core/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-dbt\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl-core/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The DBT runtime will use the dataitem specifications to fetch the data and perform the <code>transform</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/etl-core/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a tranformation on data with DBT. Our function will be an SQL query that selects all the employees of department 60.</p> <pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '60'\n\"\"\"\n</code></pre> <p>We create the function from the project object:</p> <pre><code>function = project.new_function(name=\"function-dbt\",\n                                kind=\"dbt\",\n                                source={\"code\": sql})\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>dbt</code>.</li> <li><code>source</code> contains the code that is the SQL we'll execute in the function. Must have key <code>code</code> and value the SQL code.query that</li> </ul>"},{"location":"scenarios/etl-core/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>transform</code>)</li> <li>the inputs map the refereced table in the DBT query (<code>{{ ref('employees') }}</code>) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be <code>department-60</code> and will be the sql query table name result and the output dataitem name.</li> </ul> <pre><code>run = function.run(\"transform\",\n                   inputs=[{\"employees\": di.key}],\n                   outputs=[{\"output_table\": \"department-60\"}])\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling <code>run.refresh()</code> will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/etl-core/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. We can fetch the output table and explore it with <code>pandas</code>.</p> <pre><code>df = run.outputs()[0]['department-60'].as_df()\ndf.head()\n</code></pre>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function, providing the model reference and then deploying.</p> <p>Create a model serving function and provide the model: <pre><code>serving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nserving_fn.add_model('cancer-classifier',model_path=trainer_run.outputs[\"model\"], class_name='mlrun.frameworks.sklearn.SklearnModelServer')\n</code></pre></p> <p>Deploy (it may take several minutes): <pre><code>project.deploy_function(serving_fn)\n</code></pre></p> <p>You can now test the endpoint: <pre><code>my_data = {\"inputs\"\n           :[[\n               1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n               9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n               3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n               1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n               1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n            ]\n}\nserving_fn.invoke(\"/v2/models/cancer-classifier/infer\", body=my_data)\n</code></pre></p>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>To make the API accessible from outside, we'll need to create an API gateway in Nuclio.</p> <p>Go to your Coder instance, go to the dashboard and access Nuclio. You will notice a <code>demo-ml</code> project, which we created earlier. When you access it, you will see the <code>demo-ml-serving</code> function listed, but click on the API GATEWAYS tab on top instead. Then, click on NEW API GATEWAY.</p> <p>On the left, if you wish, set Authentication to Basic and choose Username and Password.</p> <p>In the middle, set any Name you want. Host must use the same domain as the other components of the Digital Hub. For example, if you access Coder at <code>coder.my-digitalhub-instance.it</code>, the Host field should use a value like <code>demo-ml.my-digitalhub-instance.it</code>.</p> <p>On the right, under Primary, you must enter the name of the function, in this case <code>demo-ml-serving</code>.</p> <p>Save and, after a few moments, you will be able to call the API at the address you entered in Host! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"ML scenario introduction","text":"<p>This is a scenario that comes as an official tutorial of MLRun. In fact, its related notebook can be found in your Jupyter instance: <code>/tutorial/01-mlrun-basics.ipynb</code>. However, we skip a number of cells to keep it concise and to the point, while preserving the same functionality.</p> <p>To run this notebook, use the <code>Python 3 (ipykernel)</code> kernel. To do this, select <code>kernel</code> in the top bar and <code>change kernel</code> in the dropdown menu. A new window will open, where you can select the desired kernel.</p> <p>The resulting edited notebook, as well as a file for the function we will create, are available in the <code>documentation/examples/ml</code> path within the repository of this documentation.</p> <p>We will prepare data, train a model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p>"},{"location":"scenarios/ml/intro/#set-up","title":"Set-up","text":"<p>Let's initialize our working environment. Import required libraries: <pre><code>import mlrun\nimport os\n</code></pre></p> <p>Load environment variables for MLRun: <pre><code>ENV_FILE = \".mlrun.env\"\nif os.path.exists(ENV_FILE):\n    mlrun.set_env_from_file(ENV_FILE)\n</code></pre></p> <p>Create a MLRun project: <pre><code>PROJECT = \"demo-ml\"\nproject = mlrun.get_or_create_project(PROJECT, \"./\")\n</code></pre></p>"},{"location":"scenarios/ml/intro/#generate-data","title":"Generate data","text":"<p>Define the following function, which generates the dataset as required by the model: <pre><code>%%writefile data-prep.py\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nimport mlrun\n\n\n@mlrun.handler(outputs=[\"dataset\", \"label_column\"])\ndef breast_cancer_generator():\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(\n        data=breast_cancer.data, columns=breast_cancer.feature_names\n    )\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\n    breast_cancer_dataset = pd.concat(\n        [breast_cancer_dataset, breast_cancer_labels], axis=1\n    )\n\n    return breast_cancer_dataset, \"label\"\n</code></pre></p> <p>Register it: <pre><code>data_gen_fn = project.set_function(\"data-prep.py\", name=\"data-prep\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"breast_cancer_generator\")\nproject.save()\n</code></pre></p> <p>Run it locally: <pre><code>gen_data_run = project.run_function(\"data-prep\", local=True)\n</code></pre></p> <p>You can view the state of the execution with <code>gen_data_run.state()</code> or its output with <code>gen_data_run.outputs</code>. You can see a few records from the output artifact: <pre><code>gen_data_run.artifact(\"dataset\").as_df().head()\n</code></pre></p> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/ml/training/","title":"Training the model","text":"<p>MLRun integrates a set of pre-configured, pre-made functions which support both training and evaluation phases for several frameworks:</p> <ul> <li>SciKit-Learn</li> <li>TensorFlow (and Keras)</li> <li>PyTorch</li> <li>XGBoost</li> <li>LightGBM</li> <li>ONNX</li> </ul> <p>MLRun's auto-trainer can train and evaluate models for supported frameworks, in a fully autonomous and automated way.</p> <p>Import the auto-trainer: <pre><code>trainer = mlrun.import_function('hub://auto_trainer')\n</code></pre></p> <p>Run it on the cluster (it may take a few minutes): <pre><code>trainer_run = project.run_function(trainer,\n    inputs={\"dataset\": gen_data_run.outputs[\"dataset\"]},\n    params = {\n        \"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n        \"train_test_split_size\": 0.2,\n        \"label_columns\": \"label\",\n        \"model_name\": 'cancer',\n    }, \n    handler='train',\n)\n</code></pre></p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/postgrest/data/","title":"Insert data into the database","text":"<p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We will now insert some data into the database we created earlier. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file is available in the <code>documentation/examples/postgrest</code> path within the repository of this documentation.</p> <p>Import required libraries: <pre><code>import os\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport requests\n</code></pre></p> <p>Connect to the database. You will need the value of POSTGRES_URL you got from the owner's secret in the first stage of the scenario. <pre><code>engine = create_engine('postgresql://owner-UrN9ct:88aX8tLFJ95qYU7@database-postgres-cluster/mydb')\n</code></pre></p> <p>Download a CSV file and parse it (may take a few minutes): <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n\nwith requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n\ndf = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>The following will create a table and insert the dataframe into it. If it fails, resources allocated to the Jupyter workspace may be insufficient. The table will be created automatically, or replaced if it already exists. <pre><code>df.to_sql(\"readings\", engine, if_exists=\"replace\")\n</code></pre></p> <p>Run a test select query to check data has been successfully inserted: <pre><code>select = \"SELECT * FROM readings LIMIT 3\"\nselect_df = pd.read_sql(select,con=engine)\nselect_df.head()\n</code></pre></p> <p>If everything went right, a few rows are returned. We will now create a PostgREST service to expose this data via a REST API.</p>"},{"location":"scenarios/postgrest/intro/","title":"PostgREST scenario introduction","text":"<p>In this scenario, we download some data into a Postgres database, then use PostgREST - a tool to make Postgres tables accessible via REST API - to expose this data and run a simple request to view the results.</p>"},{"location":"scenarios/postgrest/intro/#database-set-up","title":"Database set-up","text":"<p>Let's start by setting up the database. Access your KRM instance and Postgres DBs on the left menu, then click Create.</p> <ul> <li><code>Name</code>: This is just an identifier for Kubernetes. Type <code>my-db</code>.</li> <li><code>Database</code>: The actual name on the database. Type <code>mydb</code>.</li> <li>Toggle on <code>Drop on delete</code>, which conveniently deletes the database when you delete the custom resource.</li> </ul> <p></p> <p>Click Save. You should now see your database listed.</p>"},{"location":"scenarios/postgrest/intro/#add-users-to-database","title":"Add users to database","text":"<p>Click Show on your database's entry and then Add user on the bottom. We will create two users: PostgREST needs one to authenticate and another to assume its role when the API is called.</p> <p>Create the first one as follows:</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-owner</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>owner</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Owner</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>owner</code>.</li> </ul> <p></p> <p>Add a second user. This one will only be able to read data.</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-reader</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>reader</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Reader</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>reader</code>.</li> </ul>"},{"location":"scenarios/postgrest/intro/#retrieve-postgres_url","title":"Retrieve POSTGRES_URL","text":"<p>Together with the users, two secrets have also been created. Go to Secrets on the left menu; the list should contain two secrets with names referring the users you created.</p> <p>Look for the owner's secret, click Show and then Decode on the <code>POSTGRES_URL</code> entry. It will automatically copy the connection string to the clipboard. Write this down somewhere, as we will use it to log into the database and insert some data.</p> <p></p>"},{"location":"scenarios/postgrest/postgrest/","title":"Expose data with PostgREST","text":"<p>We will go back to KRM to create a PostgREST service and expose the database's table via API.</p>"},{"location":"scenarios/postgrest/postgrest/#inspect-users-secrets","title":"Inspect users' secrets","text":"<p>There are a number of parameters we need to configure PostgREST. Similarly to what you did in the first stage of the scenario, go to Secrets on the left and look for the two secrets, belonging to the owner and reader users you created.</p> <p>You will need the following properties, so write them down somewhere for convenience:</p> <p>For the owner:</p> <ul> <li>The Name of the secret.</li> <li>DATABASE_NAME</li> <li>HOST</li> </ul> <p>For the reader:</p> <ul> <li>ROLE</li> </ul>"},{"location":"scenarios/postgrest/postgrest/#creating-the-postgrest-service","title":"Creating the PostgREST service","text":"<p>Click PostgREST Data Services on the left and then Create.</p> <p>Fill the first few fields as follows:</p> <ul> <li><code>Name</code>: Anything you'd like, it's once again an identifier for Kubernetes.</li> <li><code>Schema</code>: <code>public</code></li> <li>Toggle on <code>With existing DB user</code>.</li> <li><code>Existing DB user name</code>: Value of the reader's secret ROLE.</li> </ul> <p>Under Connection, fill the fields as follows:</p> <ul> <li><code>DB Host</code>: Value of the owner's secret HOST.</li> <li><code>DB Port</code>: <code>5432</code>. In case the host's value contains the port, remove it from there (delete the <code>:</code> symbol) and put it here.</li> <li><code>Database name</code>: Value of the owner's secret DATABASE_NAME.</li> <li>Toggle on <code>With existing secret</code>.</li> <li><code>Secret name</code>: The Name of the owner's secret.</li> </ul> <p>When you hit Save, the PostgREST instance will be launched!</p> <p></p>"},{"location":"scenarios/validation/scenario/","title":"Validation with digitalhub-core and Nefertem scenario introduction","text":"<p>This scenario demonstrates how to use digitalhub-core and Nefertem to validate data.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>digitalhub-core</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, are available in the <code>documentation/examples/validation</code>path within the repository of this documentation.</p>"},{"location":"scenarios/validation/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-nefertem\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/validation/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The Nefertem runtime will use the dataitem specifications to fetch the data and perform the <code>validate</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/validation/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a validation task on the dataitem. First we define the constraint that we want to validate. A constaint is a rule that we wanto to check against the data. In this case, we want to check that the <code>SALARY</code> column is of type <code>int</code>. We define the constraint as a dictionary:</p> <pre><code>constraint = {\n  'constraint': 'type',\n  'field': 'SALARY',\n  'field_type': 'number',\n  'name': 'check_value_integer',\n  'title': '',\n  'resources': ['employees'],\n  'type': 'frictionless',\n  'value': 'number',\n  'weight': 5\n}\n</code></pre> <p>With the constraint defined, we can now create the function from the project object. We pass the following parameters:</p> <pre><code>function = project.new_function(name=\"function-nefertem\",\n                                kind=\"nefertem\",\n                                constraints=[constraint])\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>nefertem</code>.</li> <li><code>constraints</code> is the list of constraints that we want to validate.</li> </ul>"},{"location":"scenarios/validation/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>validate</code>)</li> <li>framework we want to use (in this case, <code>frictionless</code>)</li> <li>the inputs map the resource defined in the constraint with our dataitem key. The runtime will collect the data referenced in the dataitem path an treat that data as Nefertem <code>DataResource</code>.</li> </ul> <pre><code>run = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs=[{\"employees\": di.key}])\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERRROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling run.refresh() will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/validation/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. A Neferetem run produces various artifacts, like reports produced by Nefertem and the framework used for validation (in our case, a Frictionless report). We can get the artifact list from the run:</p> <pre><code>artifacts = run.outputs()\n\n\nprint(artifacts)\n</code></pre> <p>If we want to get the report, we can get it from the artifact and read it:</p> <pre><code>path = artifacts[0][\"run-metadata\"].download()\n\nwith open(path) as f:\n    print(f.read())\n</code></pre>"},{"location":"tasks/data/","title":"Data and transformations","text":"<p>The platform supports data of different type to be stored and operated by the underlying storage subsystems.</p> <p>Digital Hub natively supports two types of storages:</p> <ul> <li>persistence object storage (datalake S3 Minio), which manages immutable data in the form of files.</li> <li>operational relational data storage (PostgreSQL database), which is used for efficient querying of mutable data. Postgres    is rich with extensions, most notably for geo-spatial and time-series data.</li> </ul> <p>The data is represented in the platform as entities of different types, depending on its usage and format. More specifically, we distinguish</p> <ul> <li>data items which represent immutable tabular datasets resulting from different transformation operations and ready for use in differerent types of analysis. Data items are enriched with metadata (e.g., versions, lineage, stats, profiling, schema) and unique keys and managed and persisted to the datalake directly by the platform in the form of Apache Parquet files.</li> <li>artifacts which represent arbitrary files stored to the datalake with some extra metadata, but are not limited to tabular formats.</li> </ul> <p>Each data entity may be accessed and manipulated by the platform via UI or using the API, e.g., with SDK.</p>"},{"location":"tasks/data/#manipulating-data-via-ui","title":"Manipulating data via UI","text":""},{"location":"tasks/data/#artifacts","title":"Artifacts","text":"<p>Artifacts can be created and managed as entities with the console. This can be done accessing through the user's menu or using the shortcut on the dashboard.</p> <p></p> <p>Pressing on Artifact side menu button, the paginated list of the artifacts is showed. From this pages is possible:</p> <ul> <li><code>create</code> a new artifact</li> <li><code>expand</code> an artifact and see the last 5 versions</li> <li><code>show</code> the details of an artifact</li> <li><code>edit</code> an artifact</li> <li><code>delete</code> an artifact</li> <li><code>filter</code> the artifact by name and kind</li> </ul> <p></p> <p>In the next section, we will see how to create, read, update and delete artifacts.</p>"},{"location":"tasks/data/#crud","title":"CRUD","text":"<p>Here we analyze how to Create, Read, Update and Delete Artifacts using the UI, similarly to what happens with the SDK.</p>"},{"location":"tasks/data/#create","title":"Create","text":"<p>A project is created pressing the button <code>CREATE</code> in the Artifacts' list page. After pressing the button, the dialog asking the Artifact's parameter is shown:</p> <p></p> <p>It has the following mandatory parameters: The mandatory parameters are:</p> <ul> <li><code>name</code>: the name of the artifact</li> <li><code>kind</code>: the kind of the artifact</li> </ul> <p>The only <code>Metadata</code> mandatory parameter is:</p> <ul> <li><code>path</code>: the remote path where the artifact is stored</li> </ul> <p>The other <code>Metadata</code> parameters are optional and mutable after the creation:</p> <ul> <li><code>name</code>: the name of the artifact</li> <li><code>version</code>: the version of the artifact</li> <li><code>description</code>: a human readable description of the artifact</li> <li><code>updated</code>: the date of the last modification made to the artifact</li> <li><code>src_path</code>: local path of the artifact, used in case of upload into remote storage</li> <li><code>labels</code>: the labels of the artifact</li> </ul>"},{"location":"tasks/data/#read","title":"Read","text":"<p>To read an artifact you can click on the <code>SHOW</code> button.</p> <p></p> <p>The page shows the following details</p> <ul> <li><code>id</code>: the id of the artifact</li> <li><code>kind</code>: the kind of the artifact</li> <li><code>Key</code>: the unique URL that identifies the resource</li> </ul> <p>The <code>Metadata</code> values are:</p> <ul> <li><code>name</code>: the name of the artifact</li> <li><code>description</code>: a human readable description of the artifact</li> <li><code>version</code>: the version of the artifact</li> <li><code>created</code>: the date of the creation to the artifact</li> <li><code>updated</code>: the date of the last modification made to the artifact</li> <li><code>labels</code>: the labels of the artifact</li> <li><code>path</code>: the remote path where the artifact is stored</li> <li><code>src_path</code>: local path of the artifact, used in case of upload into remote storage</li> </ul> <p>On the right side of this page are all the version of the resource is listed and the actual version is highlighted. Selecting a different element the different version is shown.</p> <p></p> <p>From the menu on top is possible to <code>EDIT</code>, <code>DELETE</code>, <code>INSPECT</code> or <code>EXPORT</code> the current artifact. For the first 2 options there are specific section of this document.</p> <p>Clicking on <code>INSPECTOR</code> a dialog that shows the artifact in JSON format is shown.</p> <p></p> <p>Clicking the <code>EXPORT</code> button the artifact is downloaded in a yaml file.</p>"},{"location":"tasks/data/#update","title":"Update","text":"<p>You can update artifact's <code>Metadata</code> pressing the button <code>EDIT</code> in the list or in the show page. All the <code>Metadata</code> values can be modified</p> <ul> <li><code>name</code>: the name of the artifact</li> <li><code>description</code>: a human readable description of the artifact</li> <li><code>version</code>: the version of the artifact</li> <li><code>updated</code>: the date of the last modification made to the artifact</li> <li><code>labels</code>: the labels of the artifact</li> <li><code>path</code>: the remote path where the artifact is stored</li> <li><code>src_path</code>: local path of the artifact, used in case of upload into remote storage</li> </ul> <p></p>"},{"location":"tasks/data/#delete","title":"Delete","text":"<p>You can delete an artifact from the list or from the detail pressing the button <code>DELETE</code>. A dialog asking confirmation is shown</p> <p></p>"},{"location":"tasks/data/#dataitems","title":"Dataitems","text":"<p>Dataitems can be created and managed as entities with the console. This can be done accessing through the user's menu or using the shortcut on the dashboard.</p> <p></p> <p>Pressing on Data items side menu button, the paginated list of the resource is showed. From this pages is possible:</p> <ul> <li><code>create</code> a new dataitem</li> <li><code>expand</code> an dataitem and see the last 5 versions</li> <li><code>show</code> the details of an dataitem</li> <li><code>edit</code> an dataitem</li> <li><code>delete</code> an dataitem</li> <li><code>filter</code> the dataitem by name and kind</li> </ul> <p></p> <p>In the next section, we will see how to create, read, update and delete dataitems.</p>"},{"location":"tasks/data/#crud_1","title":"CRUD","text":"<p>Here we analyze how to Create, Read, Update and Delete Dataitems using the UI, similarly to what happens with the SDK.</p>"},{"location":"tasks/data/#create_1","title":"Create","text":"<p>A project is created pressing the button <code>CREATE</code> in the Dataitems' list page. After pressing the button, the dialog asking the Dataitem's parameter is shown:</p> <p></p> <p>It has the following mandatory parameters: The mandatory parameters are:</p> <ul> <li><code>name</code>: the name of the dataitem</li> <li><code>kind</code>: the kind of the dataitem</li> </ul> <p>The only <code>Metadata</code> mandatory parameter is:</p> <ul> <li><code>path</code>: the remote path where the dataitem is stored</li> </ul> <p>The other <code>Metadata</code> parameters are optional and mutable after the creation:</p> <ul> <li><code>name</code>: the name of the dataitem</li> <li><code>version</code>: the version of the dataitem</li> <li><code>description</code>: a human readable description of the dataitem</li> <li><code>updated</code>: the date of the last modification made to the dataitem</li> <li><code>src_path</code>: local path of the dataitem, used in case of upload into remote storage</li> <li><code>labels</code>: the labels of the dataitem</li> </ul>"},{"location":"tasks/data/#kind","title":"Kind","text":"<p>There are 2 possible kinds for dataitems:</p> <ul> <li><code>Dataitem</code>: indicates that the dataitem is a generic dataitem. There are no specific attributes in the creation page.</li> <li><code>table</code>: indicates that the dataitem point to a table. The optional parameter is the schema of the table in table_schema format</li> </ul>"},{"location":"tasks/data/#read_1","title":"Read","text":"<p>To read an dataitem you can click on the <code>SHOW</code> button.</p> <p></p> <p>The page shows the following details</p> <ul> <li><code>id</code>: the id of the dataitem</li> <li><code>kind</code>: the kind of the dataitem</li> <li><code>Key</code>: the unique URL that identifies the resource</li> </ul> <p>The <code>Metadata</code> values are:</p> <ul> <li><code>name</code>: the name of the dataitem</li> <li><code>description</code>: a human readable description of the dataitem</li> <li><code>version</code>: the version of the dataitem</li> <li><code>created</code>: the date of the creation to the dataitem</li> <li><code>updated</code>: the date of the last modification made to the dataitem</li> <li><code>labels</code>: the labels of the dataitem</li> <li><code>path</code>: the remote path where the dataitem is stored</li> </ul> <p>Based on the kind of the dataitem, there may be <code>schema</code>, indicates that the dataitem point to a table.</p> <p>On the right side of this page are all the version of the resource is listed and the actual version is highlighted. Selecting a different element the different version is shown.</p> <p></p> <p>From the menu on top is possible to <code>EDIT</code>, <code>DELETE</code>, <code>INSPECT</code> or <code>EXPORT</code> the current dataitem. For the first 2 options there are specific section of this document.</p> <p>Clicking on <code>INSPECTOR</code> a dialog that shows the dataitem in JSON format is shown.</p> <p></p> <p>Clicking the <code>EXPORT</code> button the dataitem is downloaded in a yaml file.</p>"},{"location":"tasks/data/#update_1","title":"Update","text":"<p>You can update dataitem's <code>Metadata</code> pressing the button <code>EDIT</code> in the list or in the show page. All the <code>Metadata</code> values can be modified</p> <ul> <li><code>name</code>: the name of the dataitem</li> <li><code>description</code>: a human readable description of the dataitem</li> <li><code>version</code>: the version of the dataitem</li> <li><code>updated</code>: the date of the last modification made to the dataitem</li> <li><code>labels</code>: the labels of the dataitem</li> <li><code>path</code>: the remote path where the dataitem is stored</li> </ul> <p>Based on the kind of the dataitem, there may be <code>schema</code>, indicates that the dataitem point to a table.</p> <p></p>"},{"location":"tasks/data/#delete_1","title":"Delete","text":"<p>You can delete an dataitem from the list or from the detail pressing the button <code>DELETE</code>. A dialog asking confirmation is shown</p> <p></p>"},{"location":"tasks/data/#managing-data-with-sdk","title":"Managing data with SDk","text":""},{"location":"tasks/data/#artifacts_1","title":"Artifacts","text":"<p>Artifacts (ARTIFACT) are (binary) objects stored in one of the artifact stores of the platform, and available to every process, module and component as files (or data streams). Artifacts can be created and managed as entities with the SDK CRUD methods. This can be done directly from the package or through the <code>Project</code> object. To manage artifacts, you need to have <code>digitalhub_core</code> layer installed.</p> <p>In the first section, we will see how to create, read, update and delete artifacts. In the second section, we will see what can be done with the <code>Artifact</code> object.</p>"},{"location":"tasks/data/#crud_2","title":"CRUD","text":"<p>An <code>artifact</code> is created entity can be managed with the following methods.</p> <ul> <li><code>new_artifact</code>: create a new artifact</li> <li><code>get_artifact</code>: get an artifact</li> <li><code>update_artifact</code>: update an artifact</li> <li><code>delete_artifact</code>: delete an artifact</li> <li><code>list_artifacts</code>: list all artifacts</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Project</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\nartifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\n## From project\nartifact = project.new_artifact(name=\"my-artifact\",\n                                kind=\"artifact\",\n                                path=\"s3://my-bucket/my-artifact.ext\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete an artifact. It focus on managing artifacts from library. If you want to manage artifacts from the project, you can use the <code>Project</code> object and avoid to specify the <code>project</code> parameter.</p>"},{"location":"tasks/data/#create_2","title":"Create","text":"<p>To create an artifact you can use the <code>new_artifact()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> <li><code>name</code>: the name of the artifact</li> <li><code>kind</code>: the kind of the artifact</li> <li><code>path</code>: the remote path where the artifact is stored</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>uuid</code>: the uuid of the artifact (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the artifact</li> <li><code>source</code>: the remote source of the artifact (git repository)</li> <li><code>labels</code>: the labels of the artifact</li> <li><code>embedded</code>: whether the artifact is embedded or not. If <code>True</code>, the artifact is embedded (all the spec details are expressed) in the project. If <code>False</code>, the artifact is not embedded in the project</li> <li><code>src_path</code>: local path of the artifact, used in case of upload into remote storage</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n</code></pre>"},{"location":"tasks/data/#read_2","title":"Read","text":"<p>To read an artifact you can use the <code>get_artifact()</code> or <code>import_artifact()</code> methods. The first one searches for the artifact into the backend, the second one load it from a local yaml.</p>"},{"location":"tasks/data/#get","title":"Get","text":"<p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the artifact as identifier. It returns the latest version of the artifact</li> <li><code>entity_id</code>: to use the uuid of the artifact as identifier. It returns the specified version of the artifact</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.get_artifact(project=\"my-project\",\n                           entity_name=\"my-artifact\")\n\nartifact = dh.get_artifact(project=\"my-project\",\n                           entity_id=\"uuid-of-my-artifact\")\n</code></pre>"},{"location":"tasks/data/#import","title":"Import","text":"<p>The mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the artifact yaml</li> </ul> <p>Example:</p> <pre><code>artifact = dh.import_artifact(file=\"./some-path/my-artifact.yaml\")\n</code></pre>"},{"location":"tasks/data/#update_2","title":"Update","text":"<p>To update an artifact you can use the <code>update_artifact()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>artifact</code>: artifact object to be updated</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\nartifact.metadata.description = \"My new description\"\n\nartifact = dh.update_artifact(artifact=artifact)\n</code></pre>"},{"location":"tasks/data/#delete_2","title":"Delete","text":"<p>To delete an artifact you can use the <code>delete_artifact()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the artifact as identifier</li> <li><code>entity_id</code>: to use the uuid of the artifact as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the artifact will be deleted. Its mutually exclusive with the <code>entity_id</code> parameter</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\ndh.delete_artifact(project=\"my-project\",\n                   entity_id=artifact.id)\n</code></pre>"},{"location":"tasks/data/#list","title":"List","text":"<p>To list all artifacts you can use the <code>list_artifacts()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>artifacts = dh.list_artifacts(project=\"my-project\")\n</code></pre>"},{"location":"tasks/data/#artifact-object","title":"Artifact object","text":"<p>The <code>Artifact</code> object is built using the <code>new_artifact()</code> method. There are several variations of the <code>Artifact</code> object based on the <code>kind</code> of the artifact. The SDK supports the following kinds:</p> <ul> <li><code>artifact</code>: represents a generic artifact</li> </ul> <p>For each different kind, the <code>Artifact</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>. All the <code>Artifact</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity artifact into backend or locally as yaml.</p> <p>To create a specific artifact, you must use the desired <code>kind</code> in the <code>new_artifact()</code> method.</p>"},{"location":"tasks/data/#artifact","title":"Artifact","text":"<p>The <code>artifact</code> kind indicates that the artifact is a generic artifact. There are no specific <code>spec</code> parameters.</p> <p>The <code>artifact</code> kind has the following methods:</p> <ul> <li><code>as_file()</code>: collects the artifact into a local temporary file</li> <li><code>download()</code>: downloads the artifact into a specified path</li> <li><code>upload()</code>: uploads the artifact to a specified path</li> </ul>"},{"location":"tasks/data/#as-file","title":"As file","text":"<p>The <code>as_file()</code> method returns the artifact as a temporary file. The file is not automatically deleted when the program ends. The method returns the path of the downloaded artifact.</p>"},{"location":"tasks/data/#download","title":"Download","text":"<p>The <code>download()</code> method downloads the artifact into a specified path. The method returns the path of the downloaded artifact. The method accepts the following parameters:</p> <ul> <li><code>target</code>: remote path of the artifact to be downloaded (eg. <code>s3://my-bucket/my-artifact.ext</code>). By default, it is used the <code>spec</code> <code>path</code></li> <li><code>dst</code>: local path where the artifact will be downloaded. By default, it is in the current working directory</li> <li><code>overwrite</code>: if <code>True</code>, the target path will be overwritten if it already exists</li> </ul>"},{"location":"tasks/data/#upload","title":"Upload","text":"<p>The <code>upload()</code> method uploads the artifact to a specified path. The method returns the path of the uploaded artifact. The method accepts the following parameters:</p> <ul> <li><code>source</code>: local path of the artifact to be uploaded</li> <li><code>target</code>: remote path of the artifact to be uploaded (eg. <code>s3://my-bucket/my-artifact.ext</code>). By default, it is used the <code>spec</code> <code>path</code></li> </ul>"},{"location":"tasks/data/#dataitems_1","title":"Dataitems","text":"<p>Data items (DATAITEM) are data objects which contain a dataset of a given type, stored in an addressable repository and accessible to every component able to understand the type (kind) and the source (path). Do note that data items could be stored in the artifact store as artifacts, but that is not a dependency or a requirement. Dataitems can be created and managed as entities with the SDK CRUD methods. This can be done directly from the package or through the <code>Project</code> object. To manage dataitems, you need to have <code>digitalhub_data</code> layer installed.</p> <p>In the first section, we will see how to create, read, update and delete dataitems. In the second section, we will see what can be done with the <code>Dataitem</code> object.</p>"},{"location":"tasks/data/#crud_3","title":"CRUD","text":"<p>An <code>dataitem</code> is created entity can be managed with the following methods.</p> <ul> <li><code>new_dataitem</code>: create a new dataitem</li> <li><code>get_dataitem</code>: get a dataitem</li> <li><code>update_dataitem</code>: update a dataitem</li> <li><code>delete_dataitem</code>: delete a dataitem</li> <li><code>list_dataitems</code>: list all dataitems</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Project</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\ndataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\n## From project\ndataitem = project.new_dataitem(name=\"my-dataitem\",\n                                kind=\"dataitem\",\n                                path=\"s3://my-bucket/my-dataitem.ext\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete a dataitem. It focus on managing dataitems from library. If you want to manage dataitems from the project, you can use the <code>Project</code> object and avoid to specify the <code>project</code> parameter.</p>"},{"location":"tasks/data/#create_3","title":"Create","text":"<p>To create a dataitem you can use the <code>new_dataitem()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the dataitem will be created</li> <li><code>name</code>: the name of the dataitem</li> <li><code>kind</code>: the kind of the dataitem</li> <li><code>path</code>: the remote path where the dataitem is stored</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>uuid</code>: the uuid of the dataitem (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the dataitem</li> <li><code>source</code>: the remote source of the dataitem (git repository)</li> <li><code>labels</code>: the labels of the dataitem</li> <li><code>embedded</code>: whether the dataitem is embedded or not. If <code>True</code>, the dataitem is embedded (all the spec details are expressed) in the project. If <code>False</code>, the dataitem is not embedded in the project</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n</code></pre>"},{"location":"tasks/data/#read_3","title":"Read","text":"<p>To read a dataitem you can use the <code>get_dataitem()</code> or <code>import_dataitem()</code> methods. The first one searches for the dataitem into the backend, the second one load it from a local yaml.</p>"},{"location":"tasks/data/#get_1","title":"Get","text":"<p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the dataitem will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the dataitem as identifier. It returns the latest version of the dataitem</li> <li><code>entity_id</code>: to use the uuid of the dataitem as identifier. It returns the specified version of the dataitem</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.get_dataitem(project=\"my-project\",\n                           entity_name=\"my-dataitem\")\n\ndataitem = dh.get_dataitem(project=\"my-project\",\n                           entity_id=\"uuid-of-my-dataitem\")\n</code></pre>"},{"location":"tasks/data/#import_1","title":"Import","text":"<p>The mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the dataitem yaml</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.import_dataitem(file=\"./some-path/my-dataitem.yaml\")\n</code></pre>"},{"location":"tasks/data/#update_3","title":"Update","text":"<p>To update a dataitem you can use the <code>update_dataitem()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>dataitem</code>: dataitem object to be updated</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\ndataitem.metadata.description = \"My new description\"\n\ndataitem = dh.update_dataitem(dataitem=dataitem)\n</code></pre>"},{"location":"tasks/data/#delete_3","title":"Delete","text":"<p>To delete a dataitem you can use the <code>delete_dataitem()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the dataitem will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the dataitem as identifier</li> <li><code>entity_id</code>: to use the uuid of the dataitem as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the dataitem will be deleted. Its mutually exclusive with the <code>entity_id</code> parameter</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\ndh.delete_dataitem(project=\"my-project\",\n                   entity_id=dataitem.id)\n</code></pre>"},{"location":"tasks/data/#list_1","title":"List","text":"<p>To list all dataitems you can use the <code>list_dataitems()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the dataitem will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>dataitems = dh.list_dataitems(project=\"my-project\")\n</code></pre>"},{"location":"tasks/data/#dataitem-object","title":"Dataitem object","text":"<p>The <code>Dataitem</code> object is built using the <code>new_dataitem()</code> method. There are several variations of the <code>Dataitem</code> object based on the <code>kind</code> of the dataitem. The SDK supports the following kinds:</p> <ul> <li><code>dataitem</code>: represents a generic dataitem</li> <li><code>table</code>: represents a table dataitem</li> </ul> <p>For each different kind, the <code>Dataitem</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>.</p> <p>To create a specific dataitem, you must use the desired <code>kind</code> in the <code>new_dataitem()</code> method. All the <code>Dataitem</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity dataitem into backend or locally as yaml.</p>"},{"location":"tasks/data/#dataitem","title":"Dataitem","text":"<p>The <code>dataitem</code> kind indicates that the dataitem is a generic dataitem. There are no specific <code>spec</code> parameters nor specific method exposed. It acts as a generic dataitem.</p>"},{"location":"tasks/data/#table","title":"Table","text":"<p>The <code>table</code> kind indicates that the dataitem point to a table. The optional <code>spec</code> parameters are:</p> <ul> <li><code>schema</code>: the schema of the table in table_schema format</li> </ul> <p>The <code>table</code> kind also has the following methods:</p> <ul> <li><code>as_df()</code>: to collect the data in a pandas dataframe</li> <li><code>write_df()</code>: to write the dataitem as parquet</li> </ul>"},{"location":"tasks/data/#read-table","title":"Read table","text":"<p>The <code>as_df()</code> method returns the data in a pandas dataframe. The method accepts the following parameters:</p> <ul> <li><code>format</code>: the format of the data. If not provided, the format will be inferred from the file extension. We support ONLY parquet or csv.</li> <li><code>kwargs</code>: keyword arguments passed to the pandas <code>read_parquet</code> or <code>read_csv</code> method</li> </ul>"},{"location":"tasks/data/#write-table","title":"Write table","text":"<p>The <code>write_df()</code> method writes the dataitem as parquet. The method accepts the following parameters:</p> <ul> <li><code>target_path</code>: the path of the target parquet file. If not provided, the target path will created by the SDK and the dataitem will be stored in the default store</li> <li><code>kwargs</code>: keyword arguments passed to the pandas <code>to_parquet</code> method</li> </ul>"},{"location":"tasks/functions/","title":"Functions and Runtimes","text":"<p>Functions are the logical description of an executable. They are associated with a given runtime, which implements the actual execution and determines which are the actions available. Examples are dbt, nefertem, mlrun, etc.</p> <p>Runtimes are the entities responsible for the actual execution of a given run. They are highly specialized components which can translate the representation of a given execution as expressed in the run into an actual execution operation performed via libraries, code, external tools etc.</p>"},{"location":"tasks/functions/#managing-functions-with-sdk","title":"Managing functions with SDK","text":"<p>In the following sections, we will see how to create, read, update and delete functions and what can be done with the <code>Function</code> object with the SDK.</p>"},{"location":"tasks/functions/#crud","title":"CRUD","text":"<p>You can manage the entity <code>Function</code> with the following methods:</p> <ul> <li><code>new_function</code>: create a new function</li> <li><code>get_function</code>: get a function</li> <li><code>update_function</code>: update a function</li> <li><code>delete_function</code>: delete a function</li> <li><code>list_functions</code>: list all functions</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Project</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\nfunction = dh.new_function(project=\"my-project\",\n    name=\"my-function\",\n    kind=\"function-kind\",\n    **kwargs)\n\n## From project\nfunction = project.new_function(name=\"my-function\",\n    kind=\"function-kind\",\n    **kwargs)\n</code></pre>"},{"location":"tasks/functions/#create","title":"Create","text":"<p>To create a function you can use the <code>new_function()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function</li> </ul> <p>The the optional parameters are:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>function = dh.new_function(project=\"my-project\",\n                           name=\"my-function\",\n                           kind=\"function-kind\",\n                           **kwargs)\n</code></pre>"},{"location":"tasks/functions/#read","title":"Read","text":"<p>To read a function you can use the <code>get_function()</code> or <code>import_function()</code> methods. The first one searches for the function into the backend, the second one load it from a local yaml.</p>"},{"location":"tasks/functions/#get","title":"Get","text":"<p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the function as identifier. It returns the latest version of the function</li> <li><code>entity_id</code>: to use the uuid of the function as identifier. It returns the specified version of the function</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>function = dh.get_function(project=\"my-project\",\n                           entity_name=\"my-function\")\n\nfunction = dh.get_function(project=\"my-project\",\n                           entity_id=\"uuid-of-my-function\")\n</code></pre>"},{"location":"tasks/functions/#import","title":"Import","text":"<p>The mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the function yaml</li> </ul> <p>Example:</p> <pre><code>function = dh.import_function(file=\"my-function.yaml\")\n</code></pre>"},{"location":"tasks/functions/#update","title":"Update","text":"<p>To update a function you can use the <code>update_function()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>function</code>: the function object to update</li> </ul> <p>The the optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>function = dh.update_function(function=function,\n                              **kwargs)\n</code></pre>"},{"location":"tasks/functions/#delete","title":"Delete","text":"<p>To delete a function you can use the <code>delete_function()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the function as identifier</li> <li><code>entity_id</code>: to use the uuid of the function as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the function will be deleted. Its mutually exclusive with the <code>entity_id</code> parameter</li> <li><code>cascade</code>: if <code>True</code>, all <code>Task</code> and <code>Run</code> objects associated with the function will be deleted</li> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>function = dh.delete_function(project=\"my-project\",\n                              entity_name=\"my-function\")\n</code></pre>"},{"location":"tasks/functions/#list","title":"List","text":"<p>To list all functions you can use the <code>list_functions()</code> method.</p> <p>The mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>The optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>functions = dh.list_functions(project=\"my-project\")\n</code></pre>"},{"location":"tasks/functions/#function-object","title":"Function object","text":"<p>The <code>Function</code> object represents an executable function. The object exposes methods for saving and exporting the entity function into backend or locally as yaml and to execute it.</p>"},{"location":"tasks/functions/#save","title":"Save","text":"<p>To save a function into the backend you can use the <code>save()</code> method.</p> <p>The method accepts the following optional parameters:</p> <ul> <li><code>update</code>: a boolean value, if <code>True</code> the function will be updated on the backend</li> </ul> <p>Example:</p> <pre><code>function.save()\n</code></pre>"},{"location":"tasks/functions/#export","title":"Export","text":"<p>To export a function as yaml you can use the <code>export()</code> method.</p> <p>The method accepts the following optional parameters:</p> <ul> <li><code>filename</code>: the name of the file to export</li> </ul> <p>Example:</p> <pre><code>function.export(filename=\"my-function.yaml\")\n</code></pre>"},{"location":"tasks/functions/#run","title":"Run","text":"<p>To run a function you can use the <code>run()</code> method. This method is a shortcut for:</p> <ul> <li>creating a <code>Task</code> object</li> <li>creating a <code>Run</code> object</li> <li>executing the <code>Run</code> object</li> </ul> <p>The method accepts the following mandatory parameters:</p> <ul> <li><code>action</code>: the action to be executed. The possible values for this parameter depends on the <code>kind</code> of the function. See the runtimes section for more information.</li> </ul> <p>The optional task parameters are as follows. For Kuberenetes:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For runtime specific task parameters, see the runtime documentation.</p> <p>The optional run parameters are:</p> <ul> <li><code>inputs</code>: a list of map inputs</li> <li><code>outputs</code>: a list of map outputs</li> <li><code>parameters</code>: a map of parameters</li> <li><code>values</code>: a list of values</li> <li><code>local_execution</code>: if <code>True</code>, the function will be executed locally</li> </ul> <p>Example:</p> <pre><code>run = function.run(\n    action=\"job\",\n    inputs=[{\"input-param\": \"input-value\"}],\n    outputs=[{\"output-param\": \"output-value\"}]\n)\n</code></pre>"},{"location":"tasks/git/","title":"Git best practices","text":"<p>TODO align</p> <p>Outside of test cases, projects should be backed by a Git repository, to keep track of their history.</p> <p>As we've seen in the projects section, projects may be created with Git initialized, or loaded by cloning a repository.</p>"},{"location":"tasks/git/#resources","title":"Resources","text":"<ul> <li>MLRun's Git best practices</li> </ul>"},{"location":"tasks/kubernetes-resources/","title":"Using Kubernetes Resources for Runs","text":"<p>With Digitalhub SDK you can manage Kubernetes resources for your tasks. When you run a function you can require some Kubernetes resources for the task.</p>"},{"location":"tasks/kubernetes-resources/#resources-available-and-data-injection","title":"Resources available and data injection","text":"<p>With Digitalhub SDK you request mainly these Kubernetes resources:</p> <ul> <li>Volumes (pvc, configmap, secret)</li> <li>Hardware resources (cpu, memory, gpu)</li> </ul> <p>In addition you can inject into the task's container:</p> <ul> <li>Secrets</li> <li>Environment variables</li> </ul>"},{"location":"tasks/kubernetes-resources/#volumes","title":"Volumes","text":"<p>With SDK you can request four types of volumes:</p> <ul> <li>Persistent volume claims (pvc)</li> <li>ConfigMap</li> <li>Secret</li> </ul>"},{"location":"tasks/kubernetes-resources/#persistent-volume-claims-pvc","title":"Persistent volume claims (PVC)","text":"<p>You can ask for a persistent volume claim (pvc) to be mounted on the container being launched by the task. You need to declare the volume type as <code>persistent_volume_claim</code>, a name for the PVC for the user (e.g., <code>my-pvc</code>), the mount path on the container and a spec with the name of the PVC on Kubernetes (e.g., <code>pvc-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"my-pvc\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"claim_name\": \"pvc-name-on-k8s\",\n            }\n}]\n\nfunction.run(volumes=volumes)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#configmap","title":"ConfigMap","text":"<p>You can ask for a configmap to be mounted on the container being launched by the task. You need to declare the volume type as <code>config_map</code>, a name for the ConfigMap for the user (e.g., <code>my-config-map</code>), the mount path on the container and a spec with the name of the ConfigMap on Kubernetes (e.g., <code>config-map-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"config_map\",\n        \"name\": \"my-config-map\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"name\": \"config-map-name-on-k8s\"\n        }\n}]\n\nfunction.run(volumes=volumes)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#secret","title":"Secret","text":"<p>You can ask for a secret to be mounted on the container being launched by the task. You need to declare the volume type as <code>secret</code>, a name for the Secret for the user (e.g., <code>my-secret</code>), the mount path on the container and a spec with the name of the Secret on Kubernetes (e.g., <code>secret-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"secret\",\n        \"name\": \"my-secret\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"secret_name\": \"secret-name-on-k8s\"\n        }\n}]\n\nfunction.run(volumes=volumes)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#hardware-resources","title":"Hardware resources","text":"<p>You can request a specific amount of hardware resources (cpu, memory, gpu) for the task, declared thorugh the <code>resources</code> task parameter; <code>resources</code> must be a list of Resource objects represented as a dictionary. At the moment Digitalhub SDK supports:</p> <ul> <li>CPU</li> <li>RAM memory</li> <li>GPU</li> </ul>"},{"location":"tasks/kubernetes-resources/#cpu","title":"CPU","text":"<p>You can request a specific amount of CPU for the task. You need to declare the resource type as <code>cpu</code>, request and/or limit specifications.</p> <pre><code>resources = [{\n    \"resource_type\": \"cpu\",\n    \"requests\": \"12\",\n    \"limits\": \"16\"\n}]\n\nfunction.run(resources=resources)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#ram-memory","title":"RAM memory","text":"<p>You can request a specific amount of RAM memory for the task. You need to declare the resource type as <code>memory</code>, request and/or limit specifications.</p> <pre><code>resources = [{\n    \"resource_type\": \"memory\",\n    \"requests\": \"64Gi\"\n}]\nfunction.run(resources=resources)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#gpu","title":"GPU","text":"<p>You can request a specific amount of GPU for the task. You need to declare the resource type as <code>gpu</code>, request and/or limit specifications. There could be administation-specific requirements for requesting a GPU. You may need to use <code>tolerations</code> or <code>affinity</code> parameters to request the GPU. Both of these parameters are described in the Kubernetes documentation. Other times you may need to specify a list of labels with the <code>labels</code> parameter.</p> <p>Here is an example for the digitahub in FBK that uses the <code>tolerations</code> parameter:</p> <pre><code>resources = [{\n    \"resource_type\": \"gpu\",\n    \"limits\": \"1\"\n}]\ntoleration = [{\n    \"key\": \"nvidia.com/gpu\",\n    \"operator\": \"Equal\",\n    \"value\": \"v100\",\n    \"effect\": \"NoSchedule\"\n}]\nfunction.run(resources=resources, tolerations=toleration)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#values-injection","title":"Values injection","text":"<p>You can ask the backend to inject values into the container being launched by the task. You can inject:</p> <ul> <li>Secrets</li> <li>Environment variables</li> </ul>"},{"location":"tasks/kubernetes-resources/#secrets","title":"Secrets","text":"<p>You can request a secret injection into the container being launched by the task by passing the reference to the backend with the <code>secrets</code> task parameters.</p> <pre><code>secrets = [\"my-secret\"]\nfunction.run(secrets=secrets)\n</code></pre>"},{"location":"tasks/kubernetes-resources/#environment-variables","title":"Environment variables","text":"<p>You can request an environment variable injection into the container being launched by the task by passing the reference to the backend with the <code>env</code> task parameters.</p> <pre><code>env = [{\n    \"name\": \"env-name\",\n    \"value\": \"value\"\n}]\nfunction.run(env=env)\n</code></pre>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project in Digital Hub is a container for everything (code, assets, configuration, ...) that concerns an application. It is the context in which you can run functions and manage data and artifacts. Projects may be created and managed from the UI, but also by using DH Core's API using, e.g, Python SDK.</p>"},{"location":"tasks/projects/#managing-projects-via-ui","title":"Managing Projects via UI","text":"<p>In the following sections we document the Project management through UI available using the  <code>Digital Hub Console</code>.</p>"},{"location":"tasks/projects/#crud","title":"CRUD","text":"<p>Here we analyze how to Create, Read, Update and Delete Projects using the UI, similarly to what happens with the SDK</p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A project is created pressing the button <code>CREATE</code> in the Homepage od the Console.</p> <p></p> <p>After pressing the button, the dialog asking the Project's parameter is shown:</p> <p></p> <p>It has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project, it is also the identifier of the project</li> <li><code>description</code>: a human readable description of the project</li> </ul> <p>The other <code>Metadata</code> parameters are optional and mutable after the creation:</p> <ul> <li><code>name</code>: the name of the project</li> <li><code>description</code>: a human readable description of the project</li> <li><code>updated</code>: the date of the last modification made to the project</li> <li><code>labels</code>: a list of labels (strings)</li> </ul> <p>Pressing on the <code>Save</code> button, the project is added to the list of the projects in Homepage</p>"},{"location":"tasks/projects/#read","title":"Read","text":"<p>In the Home Page are listed all the projects present in the database. The tile shows:</p> <ul> <li><code>name</code>: the name of the project</li> <li><code>id</code>: the identifier of the project</li> <li><code>created</code>: the date of the creation of the project</li> <li><code>updated</code>: the date of the last modification made to the project</li> </ul> <p>On the bottom the button for <code>Open</code> and enter in the Project and the <code>Delete</code></p> <p></p> <p>Clicking on the <code>Open</code> button, the following Dashboard is shown</p> <p></p> <p>This dashboard reports a summary of the resources associated with the project and a series of features to access the management of these resources.</p> <ul> <li><code>Artifacts</code>: the number and the list of the last Artifacts created</li> <li><code>Data items</code>: the number and the list of the last Data items created</li> <li><code>Functions and code</code>: the number and the list of the last Functions created</li> <li><code>Jobs and runs</code>: the status and list of runs performed</li> </ul> <p>From any page of the dashboard it is possible to change the project by selecting from the menu at the top of the bar</p> <p></p>"},{"location":"tasks/projects/#update","title":"Update","text":"<p>You can update a project <code>Metadata</code> pressing the button <code>Configuration</code> in the side Menubar.</p> <p></p> <p>Pressing the <code>Edit</code> button on the top right of the page the form for editing the <code>Metadata</code> values of the project is shown. In the example below, the labels <code>test</code> and <code>prj1</code> are added</p> <p> After the modification, pressing Save the new configuration is stored</p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>You can delete a project from the <code>Home page</code>  and from the <code>Configuration</code> pressing the <code>Delete</code> button. For confirm the choice of deleting, insert the name of the project in the dialog</p> <p></p>"},{"location":"tasks/projects/#managing-projects-via-sdk","title":"Managing Projects via SDK","text":"<p>In the following sections we document the project CRUD methods available in the SDK and the methods exposed by the <code>Project</code> entity.</p>"},{"location":"tasks/projects/#crud_1","title":"CRUD","text":"<p>Here we analyze how to create, read, update and delete projects using the SDK.</p>"},{"location":"tasks/projects/#create_1","title":"Create","text":"<p>A project is created with <code>new_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project, it is also the identifier of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>context</code>: path where project can export yaml files locally</li> <li><code>description</code>: a human readable description of the project</li> <li><code>source</code>: a Git repository URL where lies the project code</li> <li><code>labels</code>: a list of labels</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.new_project(\"my-project\", context=\"./\", description=\"my new project\")\n</code></pre>"},{"location":"tasks/projects/#config","title":"Config","text":"<p>The <code>config</code> parameter can be used to provide a dictionary containing the project configuration like user and password for basic auth or a bearer token. The format of the dictionary for basic auth must be as this:</p> <pre><code>{\n    \"user\": \"user\",\n    \"password\": \"password\"\n}\n</code></pre> <p>The format of the dictionary for bearer token must be as this:</p> <pre><code>{\n    \"access_token\": \"token\"\n}\n</code></pre> <p>In case you try to get a project without from the backend with invalid credentials, an exception will be raised. Because the backend client is a Singleton object, it will autoconfigure credentials at startup, so the only way to setup proper credentials once it fails to connect is to use the SDK method <code>set_dhub_env()</code>. The method accepts the following optional parameters:</p> <ul> <li><code>endpoint</code>: the endpoint of the backend</li> <li><code>user</code>: the user for basic auth</li> <li><code>password</code>: the password for basic auth</li> <li><code>token</code>: the auth token</li> </ul> <p>Example:</p> <pre><code>dh.set_dhub_env(\n    endpoint=\"https://some-digitalhub:8080\",\n    token=\"token\"\n)\n</code></pre> <p>Note that the <code>set_dhub_env()</code> method ovverrides the environment variables and (if already instantiated) the credentials attributes of the backend client.</p>"},{"location":"tasks/projects/#setup-kwargs","title":"Setup kwargs","text":"<p>The <code>setup_kwargs</code> parameter can be used to provide a dictionary containing the project hook setup arguments. The concept behind this parameter is that at the beginning of the project lifecycle, the project can be configured with an hook script that will be executed when the project is created / got. First of all, the configuration script MUST comply with the following format:</p> <ul> <li>It must be a Python script named <code>setup_project.py</code> inside the project context directory.</li> <li>It must contain an handler (a python function) named <code>setup</code> as entrypoint.</li> <li>The <code>setup</code> function must accept a <code>Project</code> instance as the only positional argument.</li> <li><code>setup_kwargs</code> must be passed as keyword arguments to the <code>setup</code> function.</li> </ul> <p>The project setup will create a <code>.CHECK</code> file at the end of the <code>setup</code> function execution. This sentinel file is used to indicate that the project is set up and new executions will be ready.</p> <p>A use case scenario can be the instantiation of entities used by the user like artifacts or functions.</p> <p>Example:</p> <pre><code>setup_kwargs = {\n    \"some_arg1\": \"arg1\",\n    \"some_arg2\": \"arg2\"\n}\n\n# Setup script\n\ndef setup(project, some_arg1=None, some_arg2=None):\n    # Do something with project and args\n</code></pre>"},{"location":"tasks/projects/#read_1","title":"Read","text":"<p>You can read a project with three methods, from remote backend with <code>get_project()</code>, from local directory with <code>import_project()</code> or from either with <code>load_project()</code>.</p>"},{"location":"tasks/projects/#get-project","title":"Get project","text":"<p>With <code>get_project()</code> you can load a project from the backend. The method requires the following manadatory parameters:</p> <ul> <li><code>name</code>: the name of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.get_project(\"my-project\")\n</code></pre>"},{"location":"tasks/projects/#import-project","title":"Import project","text":"<p>With <code>import_project()</code> you can load a project from a local directory. The method requires the following manadatory parameters:</p> <ul> <li><code>file</code>: path to the yaml project file</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.import_project(\"./my-project.yaml\")\n</code></pre>"},{"location":"tasks/projects/#load-project","title":"Load project","text":"<p>With <code>load_project()</code> you can load a project from the backend or from a local directory. The method requires either one of the following parameters:</p> <ul> <li><code>name</code>: the name of the project</li> <li><code>file</code>: path to the yaml project file</li> </ul> <p>If you pass the name, the method will try to load the project from the backend, otherwise it will try to load it from the local directory. Note that both parameters are mutually exclusive and keyword arguments.</p> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.load_project(name=\"my-project\")\nproject = dh.load_project(file=\"./my-project.yaml\")\n</code></pre>"},{"location":"tasks/projects/#create-or-read","title":"Create or read","text":"<p>The <code>digitalhub</code> SDK provides a method <code>get_or_create_project()</code> that allows you to create a project if it does not exist. The method has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project.</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>context</code>: path where project can export yaml files locally</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.get_or_create_project(\"my-project\", context=\"./\")\n</code></pre>"},{"location":"tasks/projects/#update_1","title":"Update","text":"<p>You can update a project with <code>update_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>project</code>: the project entity that will be updated</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.update_project(project)\n</code></pre>"},{"location":"tasks/projects/#delete_1","title":"Delete","text":"<p>You can delete a project with <code>delete_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>cascade</code>: a boolean value, if <code>True</code> the project and all the related resources will be deleted from the backend. Defaults to <code>True</code>. It is only available in the Core backend.</li> <li><code>clean_context</code>: a boolean value, if <code>True</code> the project context will be deleted (no more object can be created locally under the project).</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.delete_project(\"my-project\")\n</code></pre>"},{"location":"tasks/projects/#project-methods","title":"Project methods","text":"<p>The <code>Project</code> class exposes two basic methods that allow you to save remotely or export locally the project. Furthermore, according to the SDK digitalhub layer installed, the <code>Project</code> class exposes CRUD methods for a variety of entities.</p>"},{"location":"tasks/projects/#save-and-export","title":"Save and export","text":"<p>The methods <code>save()</code> and <code>export()</code> are used to save the project on the backend or export the project locally.</p>"},{"location":"tasks/projects/#save","title":"Save","text":"<p>The <code>save()</code> method is used to save the project on the backend and it accepts the following optional parameters:</p> <ul> <li><code>update</code>: a boolean value, if <code>True</code> the project will be updated on the backend</li> </ul> <p>Please note that the save method will usually raise an exception if called without the <code>update</code> parameter on runtime. This is because the project (managed with CRUD SDK methods) should already exists on the backend if exists as object.</p>"},{"location":"tasks/projects/#export","title":"Export","text":"<p>The <code>export()</code> method is used to export the project locally as yaml file and it accepts the following optional parameters:</p> <ul> <li><code>filename</code>: the name of the file to export</li> </ul> <p>Note that the filename must have the <code>.yaml</code> extension. The project will be exported as a yaml file inside the context directory. If no filename is passed, the project will be exported as a yaml file named <code>project_{project-name}.yaml</code>.</p> <p>According to the SDK digitalhub layer installed, the <code>Project</code> class exposes CRUD methods for a variety of entities.</p>"},{"location":"tasks/projects/#entities-crud","title":"Entities CRUD","text":"<p>The project acts as context for other entities as mentioned in the introduction. With a <code>Project</code> object, you can create, read, update and delete these entities. The methods exposed are basically five for all entities, and are:</p> <ul> <li><code>new</code>: create a new entity</li> <li><code>get</code>: get an entity from backend</li> <li><code>update</code>: update an entity</li> <li><code>delete</code>: delete an entity</li> <li><code>list</code>: list entities related to the project</li> </ul> <p>Each digitalhub layer exposes CRUD handles different aspects of data and ml entities. Here follows a list of the methods exposed by the <code>Project</code> class for each layer. Please refer to the specific entity documentation for more information.</p>"},{"location":"tasks/projects/#core-layer","title":"Core layer","text":"<p>The entity handled by the <code>Project</code> class in the core layer (<code>digitalhub_core</code>) are:</p> <ul> <li><code>functions</code></li> <li><code>artifacts</code></li> <li><code>workflows</code></li> <li><code>secrets</code></li> </ul>"},{"location":"tasks/projects/#data-layer","title":"Data layer","text":"<p>The entity handled by the <code>Project</code> class in the data layer (<code>digitalhub_data</code>) are:</p> <ul> <li><code>dataitems</code></li> </ul>"},{"location":"tasks/projects/#ml-layer","title":"Ml layer","text":"<p>The entity handled by the <code>Project</code> class in the ml layer (<code>digitalhub_ml</code>) are:</p> <ul> <li><code>models</code></li> </ul>"},{"location":"tasks/resources/","title":"Resource Management with KRM","text":"<p>Different platform entities are associated with and represented as Kubernetes resources: they are deployed as services, user volumes and secrets, captured as Custom Resources, etc. Kubernetes Resource Manager (KRM) component allows for performing various operations over these resources depending on their kind.</p> <p></p> <p>KRM navigation menu provides access to different types of resources. This includes both standard resources (Services, Deployments, Persistent Volume Claims, Secrets) and custom resources based on Custom Resource Definitions currently installed on the platform. Some custom resources are managed with the customized UI (e.g., PostgreSQL instances, PostgREST Data services o Dremio Data service), while the others may be managed with the standard UI based on their JSON schema.</p>"},{"location":"tasks/resources/#management-of-standard-kubernetes-resources","title":"Management of Standard Kubernetes Resources","text":"<p>KRM allows for accessing and managing the standard K8S resources relevant for the DigitalHub platform: space (through Persistent Volume Claims), services and deployments, and secrets.</p>"},{"location":"tasks/resources/#listing-k8s-services","title":"Listing K8S Services","text":"<p>Accessing the <code>Services</code> menu of the KRM, it is possible to list the (subset of) services deployed on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each service KRM shows its name, type (e.g., Coder workspace type), exposed port type and value. In the service details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#listing-k8s-deployments","title":"Listing K8S Deployments","text":"<p>Accessing the <code>Deployments</code> menu of the KRM, it is possible to list the (subset of) deployments on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each deployment KRM shows its name and availability of instances. In the deployment details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#managing-persistent-volume-claims","title":"Managing Persistent Volume Claims","text":"<p>In certain cases, the operations developed with the Digital Hub may require more substantial disk space, e.g., for training / producing significant amounts of data. In this case, it is possible to attach to the tasks the corresponding Persistent Volume Claim (PVC) references. To create a new PVC for the use of the pipeline or Job, KRM provides the corresponding interface.</p> <p>Accessing <code>Persistent Volume Claims</code> menu, it is possible to list and manage the PVCs of the platform. </p> <p></p> <p>For each PVC, you can see the status (Pending or Bound) of the PVC, the name of the volume (if specified), the storage class and the size in Gi. The details view provides further metadata regarding the PVC.</p> <p>It is also possible to delete the PVC and create new ones.</p> <p>Deleting PVC</p> <p>Please note that deleting a PVC bound to a Pod or a Job may affect negatively their execution.</p> <p>To create a new PVC, provide the following:</p> <ul> <li>name of the resource</li> <li>Disk space requeste</li> <li>Storage class name (select one of the available in your deployment)</li> <li>(Optional) name of the volume</li> <li>Access modes (standard K8S values)</li> <li>PVC mode (Filesystem or Block)</li> </ul> <p></p>"},{"location":"tasks/resources/#listing-k8s-secrets","title":"Listing K8S Secrets","text":"<p>Accessing the <code>Secrets</code> menu of the KRM, it is possible to list the (subset of) secrets on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each secret KRM shows its name, type, and number of elements. In the secret details view it is possible to access other metadata and also a list of secret elements. The values are not available directly; to retrieve the actual value of the secret element, use <code>Decode</code> button that will copy the decoded content of the secret into Clipboard.</p> <p></p>"},{"location":"tasks/resources/#managing-custom-resources","title":"Managing Custom Resources","text":"<p>KRM allows for the management of generic CRs as well as for the management of some predefined ones, such as PostgreSQL instances, PostgREST and Dremio Data services.</p>"},{"location":"tasks/resources/#managing-postgresql-instances-with-krm","title":"Managing PostgreSQL instances with KRM","text":"<p>Using PostgreSQL operator (https://github.com/movetokube/postgres-operator) it is possible to create new DB instances and the DB users to organize the data storage.</p> <p>Accessing <code>Postgres DBs</code> menu of the KRM, it is possible to list, create, and delete PostgreSQL databases.</p> <p></p> <p>To create a new Database, provide the following:</p> <ul> <li>name of the database to create</li> <li>whether to drop the DB on resource deletion</li> <li>Comma-separated list of PostgreSQL extensions to enable (e.g., timescale and/or postgis) as supported by the platform deployment (optional). </li> <li>Comma-separated list of schemas to create in DB (optional) </li> <li>Name of the master role for the DB access management (optional) </li> </ul> <p></p> <p>In the Database details view it is possible also to configure the DB users that can access and operate the Database (create, edit, view, delete). To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the user to be created</li> <li>access privileges (e.g., Owner, Read, or Write)</li> <li>name of the secret to be create to store the user credentials and DB access information. This results in creating a secret  <code>&lt;user-cr-name&gt;-&lt;secret-name&gt;</code> that can be accessed in the Secrets section of KRM.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-postgrest-data-services-with-krm","title":"Managing PostgREST Data Services with KRM","text":"<p>Using KRM it is possible to instantiate and deploy new PostgREST Data services. A PostgREST service exposes a set of PostgreSQL tables as API, allowing for querying the information and even modifying it.</p> <p>Accessing <code>PostgREST Data Services</code> menu it is possible to list, create, and delete data service instances.</p> <p></p> <p>To create a new data service, it is necessary to provide the information about the exposed schema and tables, the DB access information, and the role with which the service operates. This latter may be specified either as an existing users with the appropriate permissions or may be created if the DB access information is sufficient for  that operation. More specifically, it is necessary to provide the following information:</p> <ul> <li>name of the resource.</li> <li>name of the DB schema to expose.</li> <li>the existing DB user (role) on behalf of which the service will operate OR the list of DB permissions to enable for this service and list of exposed DB tables. In this case the user will be created (if the connection information allows for it).</li> <li>Connection information with cluster DB host and port (optional), name of the database, DB username / password OR, alternatively, DB secret to use in order to extract the connection credentials. In this later case the secret should contain elements <code>USERNAME</code> and <code>PASSWORD</code>, or alternatively <code>POSTGRES_URL</code> with the full connection information.</li> </ul> <p>Connection information</p> <p>Please note that in order to create a new role that will be used by the service to access the data, the user specified with the connection information should have sufficient privileges to perform the operation. By default, the owner/writer/readers users created by the Postgres operator do not have this permission.</p> <p>Schema exposure</p> <p>PostgREST exposes all the tables and views in the schema specified in the configuration. In order to have a better control over the exposed data, it is  recommended to create a separate schema (e.g., 'api') and provide the access to the data via views / stored procedures. To accomplish this, it is possible to use SQLPad to create schemas and views.</p> <p></p> <p>This will result in a deployment of PostgREST microservice connected to the specified database and exposing PostgREST API over the specified schema and tables.  See here for further details.</p>"},{"location":"tasks/resources/#managing-dremio-data-services-with-krm","title":"Managing Dremio Data Services with KRM","text":"<p>Using KRM it is possible to instantiate and deploy new Dremio Data services that expose the data presented in Dremio views as API.</p> <p>Accessing <code>Dremio Data Services</code> menu it is possible to list, create, and delete data service instances.</p> <p></p> <p>To create a new data service, provide the following:</p> <ul> <li>name of the resource</li> <li>list of exposed virtual datasets</li> <li>Connection information with dremio host and port (optional), Dremio username / password OR, alternatively, a secret to use in order to extract the connection credentials. In this later case the secret should contain elements <code>USER</code> and <code>PASSWORD</code>.</li> </ul> <p></p> <p>This will result in a deployment of Dremio REST microservice connected to the specified database and exposing a simple REST API over the specified datasets. </p>"},{"location":"tasks/resources/#exposing-services-externally","title":"Exposing services externally","text":"<p>Various APIs and services (e.g., PostgREST or Dremio data services, Nuclio serverless functions) may be exposed externally, outside of the platform, on a public domain of the platform. Using KRM, the operation amounts to defining a new API gateway resource that will be transformed into the corresponding ingress routing specification. </p> <p></p> <p>To create a new API gateway, provide the following:</p> <ul> <li>name of the gateway</li> <li>Kubernetes service to be exposed (select it from the dropdown list and the port will automatically be provided)</li> <li>host and relative path to be exposed. The host defines the full domain name to be exposed. By default it refers to the 'services' subdomain, e.g., <code>myservice.services.example.com</code> where <code>example.com</code> corresponds to the platform domain.</li> <li>authentication information. Currently, services may be unprotected (<code>None</code>) or protected with <code>Basic</code> authentication, specifying username and password.</li> </ul>"},{"location":"tasks/resources/#defining-and-managing-crd-schemas","title":"Defining and Managing CRD Schemas","text":"<p>To have a valid representation of the CRs in the system, it is necessary to have a JSON specification schema for each CRDs. Normally, such schema is provided with the CRD definition and is used by KRM to manage the resources. However, in certain cases a CRD may have no structured schema definition attached. To allow for managing such resources, it is possible to provide a custom schema for the CRD.</p> <p>Creating a schema is fairly simple. Access the Settings section from the left menu and click Create.</p> <p>The CRD drop-down menu will list all Custom Resource Definitions available on the Kubernetes instance; when you pick one, the Version field will automatically be filled with the version of the currently active schema.</p> <p>Provide the Schema definition and save it in KRM for future CR management.</p>"},{"location":"tasks/secrets/","title":"Secret Management","text":"<p>Working with different operations may implu the usage of a sensitive values, such as external API credentials, storage credentials, etc. </p> <p>In order to avoid embedding the credentials in the code of functions, Digital Hub supports an explicit management of credentials as secrets. This operation exploits the underlying secret management subsystem, such as Kubernetes Secret Manager.</p> <p>Besides the secrets managed natively by the platform to integrate e.g., default storage credentials, it is possible to  define custom secrets at the level of a single project. The project secrets are managed as any other project-related entities, such as functions, dataitems, etc.</p> <p>At the level of the project the secrets are represented as key-value pairs. The management of secrets is delegated to a secret provider, and currently only Kubernetes Secret Manager is supported. Each project has its own Kubernetes secret, where  all the key-value pairs are stored.</p> <p>To create a new secret value it is possible to use Digital Hub UI console or directly via API, e.g., using the SDK. </p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-ui","title":"Creating and Managing Secrets via UI","text":"<p>Digital Hub console can be used to manage project secrets. To create a new one, it is necessary to provide  a secret key and a value to be stored. </p> <p></p> <p>The entries may be then deleted and updated, as well as their metadata.</p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-sdk","title":"Creating and Managing Secrets via SDK","text":"<p>The secrets may be operated by the DigitalHub SDK. More specifically, to create a new secret in the project</p> <pre><code>project = dhcore.get_or_create_project(\"project-secrets\")\n\nsecret0 = project.new_secret(name=\"somesecret\", secret_value=\"value1\")\nprint(secret0)  \n</code></pre> <p>To read the value of an existing secret <pre><code>secret0 = project.get_secret(entity_name=\"somesecret\")\nprint(secret0.read_secret_value())\n</code></pre></p> <p>To update an existing secret <pre><code>secret0.set_secret_value(value=\"value1\")\nprint(secret0.read_secret_value())\n</code></pre></p> <p>To read the project secret entities <pre><code>secrets = project.list_secrets()\nprint(secrets)\n</code></pre></p> <p>To delete a secret <pre><code>project.delete_secret(entity_name=\"somesecret\")\nsecrets = project.list_secrets()\nprint(secrets)\n</code></pre></p>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the Digital Hub platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p> <p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>Let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}