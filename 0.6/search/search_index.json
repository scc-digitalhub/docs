{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Digital Hub is an Open-Source platform for building and managing Data and AI applications and services. Bringing the principles of DataOps, MLOps, DevOps and GitOps, integrating the state-of-art open source technologies and modern standards, DigitalHub extends your  development process and operations to the whole application life-cycle, from exploration to deployment, monitoring, and evolution.</p> <p></p> <ul> <li>Explore. Use on-demand interactive scalable workspaces of your choice to code, explore, experiment and analyze the data and ML. Workspace provide a secure and isolated execution environments natively connected to the platform, such as Jupyter Notebooks or VS Code, Dremio distributed query engine, etc. Use extensibility mechanisms provided by the underlying workspace manager to bring your own workspace templates into the platform.</li> <li>Process. Use persistent storages (Datalake and Relational DBs) to manage structured and non-structured data on top of the data abstraction layer. Elaborate  data, perform data analysis activites and train AI models using frameworks and libraries of your choice (e.g., from python-based to DBT, to arbitrary containers). Manage the supporting computational and storage resources in a declarative and transparent manner.</li> <li>Execute. Delegate the code execution, image preparation, run-time operations and services to the underlying Kubernetes-based execution infrastructure, Serverless platform, and pipeline execution automation environment.</li> <li>Integrate. Build new AI services and expose your data in a standard and interoperable manner, to facilitate the integration within different applications, systems, business intelligence tools and visualizations. </li> </ul> <p>To support this functionality, the platform relies on scalable Kubernetes platform and its extensions (operators) as well as on the modular architecture and functinality model that allows for dealing with arbitrary jobs, functions, frameworks and solutions without affecting your development workflow. The underlying methodology and management approach aim at facilitating the re-use, reproducability, and portability of the solution among different contexts and settings. </p>"},{"location":"#interested","title":"Interested?","text":"<ul> <li>Quick Start. Bring the platform up and explore it in few minutes!</li> <li>Installation. Learn how to install, configure, and manage the platform in different settings.</li> <li>Overview. Deep dive into the platform functionality, architecture, components, and functionality.</li> </ul>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<ul> <li>Native support for the integration of new functionality</li> <li>Standard infrastructures and frameworks</li> <li>Common workflow and abstractions</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform adopts Nuclio as the serverless platform, and supports Python, Java and Go as programming languages.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with ML a range of frameworks, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (via MLRun), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-on-minikube","title":"Installation on minikube","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>Minikube</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<ol> <li>Start minikube (change 192.168.49.0 if your network setup is different): <pre><code>    minikube start --insecure-registry \"192.168.49.0/24\" --memory 12288 --cpus 4\n</code></pre></li> <li>Get minikube external IP: <pre><code>    minikube ip\n</code></pre></li> <li>Change the IP in  'global.registry.url' and 'global.externalHostAddress' properties in values file (chart/digitalhub/values.yaml) with the one obtained in the previous step.</li> <li>Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></li> <li>Install DigitalHub with Helm: <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --timeout 15m0s\n</code></pre></li> <li>Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></li> </ol> <p>Once installed, you should see the references (URLs) for the different tools of the platform.</p>"},{"location":"installation/#install-with-ms-azure","title":"Install with MS Azure","text":"<p>Documentation in progress...</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>To start with DigitalHub, the first step is to install the platform and all its components. For its functionality, DigitalHub relies on Kubernetes, a state-of-art Open-Source containerized application deployment, orchestration and execution platform. While it is possible to run DigitalHub on any Kubernetes installation, the quickest way is to deploy it on Minikube, a local Kubernetes environment with minimal settings. See here instruction on how to set up DigitalHub on Minikube.</p> <p>Once installed, you can access different platform components and perform different operations, ranging from exlorative data science with Jupyter Notebooks, creating projects for data processing or ML tasks, managing necessary resources (e.g., databases or datalake buckets), creating and running different functions, etc.</p>"},{"location":"quickstart/#platform-components-and-functionality","title":"Platform Components and Functionality","text":"<p>To access the different components of the platform start from the landing page, where the components are linked:</p> <ul> <li>Use Coder to create interactive workspaces, such as Jupyter Notebooks, to perform explorative tasks, access and manage the data. See how to use Workspaces for these type of activities.</li> <li>Use DH Core UI to manage your data science and ML project and start with management activities, such as creating data items, defining and executing different functions and operations. Please note that these tasks may be done directly with the DH Core Python SDK from your interactive environment. See how to use DH Console for the management operations.</li> <li>To see and manage the relevant Kubernetes resources (e.g., services, jobs, secrets), as well as custom resources of the platform (e.g., databases, S3 buckets, data services), use Kubernetes Resource Manager. The operations and the functionality of the tool are described in the Resource Management with KRM section of the documentation.</li> <li>Use Minio browser to navigate your datalake, upload and manage the files. The datalake is based on S3 protocol and can be used also programmatically. See the Data and Transformations section on how the data abstraction layer is defined and implemented.</li> <li>If you perform ML task with the Python runtime, you can prepare data, create and log ML Models using DH Core (see, e.g., Python Runtime if you want to use MLRun operations through DHCore).  Alternatively, it is possible to use MLRun subsystem of the platform, MLRun UI provides you the information of the data, models, jobs and services operated with MLRun. See MLRun documentation on how to use MLRun directly.</li> <li>Use Nuclio Serverless platform to deploy and expose functions as services within the platform. Nuclio is also used by MLRun to serve its ML Models as run-time operations. See Nuclio documentation on how to use Nuclio in different scenarios.</li> <li>It is possible to organize the data and ML operations in complex pipelines. Currently the platform relies on Kubeflow Pipelines component for this purpose, orchestrating the activities as single Kubernetes Jobs. See more on this in the corresponding Pipelines section.</li> </ul>"},{"location":"quickstart/#tutorials","title":"Tutorials","text":"<p>Start exploring the platform through a series of tutorials aiming at explaining the key usage scenarios for DigitalHub platform. Specifically</p> <ul> <li>Create your first data management pipeline, from data exploration to automated data ETL procedure running on the platform.</li> <li>Perform DBT data transformation and store the data in a database.</li> <li>Train a simple ML Model and deploy it as a service with Nuclio serverless.</li> <li>Validate the dataset using Nefertem Validation Framework.</li> <li>Use Dremio distributed query engine to organize data and visualize with Grafana.</li> <li>Store data in DB to perform efficient and complex queries and expose the data as REST API.</li> </ul>"},{"location":"admin/authentication/","title":"Platform Authentication and Access Control","text":"<p>Work in progress...</p>"},{"location":"admin/configuration/","title":"Platform Configuration","text":"<p>Work in progress...</p>"},{"location":"admin/overview/","title":"Platform Administration Overview","text":"<p>When deployed in production environment, the platform setup requires additional steps for its secure and efficient use. In this  section we consider several aspects that are required for the platform production setup. </p> <ul> <li>Authentication and Access Control</li> <li>Platform Configuration</li> </ul>"},{"location":"components/dashboard/","title":"Landing Page","text":"<p>The landing page is a central access point to reach a number of tools that are automatically run when the platform is installed. It provides access to the platform components and to the monitoring subsystem of the platform.</p> <p></p> <p>Components</p> <ul> <li>Coder, Tool for managing interactive workspaces</li> <li>DH Core Console, UI for the platform management</li> <li>KRM, or Kubernetes Resource Manager, is the tool for organizing and managing standard and custom Kubernetes resources</li> <li>MLRun, a framework for MLOps</li> <li>Nuclio, a platform for serverless functions</li> <li>Kubeflow, a tool for ML pipelines on Kubernetes</li> <li>MinIO, an S3-compatible object datalake UI</li> </ul>"},{"location":"components/dh_console/","title":"Core UI","text":"<p>The Core console is a front-end application backed by the  Core API. It provides a management interface  for the organization and operations over the Data Science Projects and the associated entities, such as:</p> <ul> <li>functions of various runtimes (see the Functions and Runtimes section for details), as well as their executions (runs) grouped by the corresponding operations (tasks)</li> <li>workflows - composite pipelines combining executions of different functions</li> <li>dataitems - structured Data Items managed by the project</li> <li>artifacts - unstructured files related and maanged by the project</li> <li>models - versioned ML Model artifacts with their metrics and metadata (see ML Models section for details)</li> </ul> <p>When you access the console, you land to the project management page, where you can create or delete projects.</p> <p>Note that all the functionality that is performed via UI console through the Core API can be also performed using the platform management Python SDK reflecting management of the same platform entities.</p>"},{"location":"components/dh_console/#create-a-project","title":"Create a Project","text":"<p>Start by clicking the <code>CREATE A NEW PROJECT</code> button.</p> <p></p> <p>Fill the form's properties. </p> <p>Following the selection of a project, you can get an overview of the associated objects on its dashboard and manage them on their dedicated pages.</p>"},{"location":"components/dh_console/#dashboard","title":"Dashboard","text":"<p>The console dashboard shows the resources that have been created with a series of cards and allows you to quickly access them. You can see the runs performed and their respective status, as well as artifacts, data items and functions. </p>"},{"location":"components/dh_console/#objects","title":"Objects","text":"<p>Through the console it is also possible to manage directly the entities related to the project and perform different operations over those. This amounts not only to CRUD (create, update, delete, and read) operations, but also track relations, view detailed metadata and versions, execute functions and pipelines, etc. </p>"},{"location":"components/dh_console/#functions","title":"Functions","text":"<p>Functions define the executable procedures implemented in various ways that can be run by the platform. In console it is possible to create new functions selecting the corresponding runtime and, based on that, providing its specification, e.g., source code. For each function the console lists the different versions of the function, the specification and the code (if available) of the function, as well as different tasks that can be performed over the function in the corresponding runtime. For example, in case of Python runtime, it is possible to <code>build</code> function (generating the corresponding Docker image and caching in the regsitry), to run the function as <code>job</code> (to be executed on the Kubernetes), or to expose the function as a service, that is to <code>serve</code> the function. </p> <p></p> <p>Within the tab corresponding to the specific task, it is possible to access the list of runs executed over the function, the status of execution, the execution log. It is also possible to create new run of the task, defining the specific parameters and configurations for the run. </p>"},{"location":"components/dh_console/#workflows","title":"Workflows","text":"<p>Workflows represent a composition of function executions that is run over the platform, specifying their dependencies (in terms of data and order). This allows for creating complex pipelines for AI/ML and data operations. Currently, the implementation of the workflow relies on the Kubeflow Pipelines framework, that in turn relies on Kubernetes Argo Workflows so that each step of the workflow is executed as a single Kubernetes Job. </p> <p>From the console it is possible to define a new workflow providing the code of the pipeline and run the <code>pipeline</code> task. As in case of the function runs, the execution of the pipeline is being tracked, as well as the progress of single steps, and the corresponding log. </p>"},{"location":"components/dh_console/#dataitems","title":"Dataitems","text":"<p>Through Dataitems the project may define the relevant structured and semi-structured datasets. Dataset may created manually, starting from a reference to an URL of the file or DB table, or may be produced as a result of some data transformation function execution. As in case of the functions, the dataitems are equipped with the relevant metadata (e.g., creation and changes, tags, ownership, etc). Furthermore, the datasets structured as tables (i.e., <code>table</code> kind datasets) are equipped with the derived schemas and profiling and data preview.</p> <p>Through the console it is possible to manage the datasets, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#artifacts","title":"Artifacts","text":"<p>Similar to dataitems, Artifacts it is possible to explicitly capture the relevant unstructured objects and files. Artifacts may be of arbitrary type, and equipped with a generic metadata properties.  </p> <p>Through the console it is possible to manage the artifacts, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#ml-models","title":"ML Models","text":"<p>ML Models represent a specific type of artifacts, which are produced by the AI tranining activites and represent the datasets used for inference operations. While managed in the same manner as other types of entities, ML Models may have a specific set of metadata and specification attributes, such model kind, metrics, algorithm and framework specification, etc. </p> <p>ML Models are further used by the inference services.</p>"},{"location":"components/dh_console/#secrets","title":"Secrets","text":"<p>When executing operations with the platform, the execution might need access to some sensitive values, for example to access data residing on a data-store that requires credentials (such as a private S3 bucket), access a private repository, or many other similar needs.</p> <p>The platform provides the functionality to manage these values, reffered to as Secrets, both through UI and SDK, where it is possible to associate the key-value pair to the project. The data is managed as Kubernetes secrets and is embedded in the execution of a run that relies on that.</p> <p>The management of secrets allows through the console to create, delete, and read the secret values.</p>"},{"location":"components/dh_console/#versioning","title":"Versioning","text":"<p>All entities operated by Core are versioned. When you view the details of an object, all of its versions are listed and browsable. Moreover, when you view a dataitem, its schema and data preview are available.</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with username <code>admin</code> and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from [Coder, using its template]../tasks/workspaces.md). After launching it from Coder you can access Grafana on Grafana UI (http://nodeipaddress:30110).</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> (should be <code>mlrun</code>)</li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> (should be <code>mlrun</code>)</li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/kubeflow/","title":"KubeFlow Pipelines","text":"<p>Kubeflow Pipelines makes part of the Kubeflow platform and allows for organizing workflows out of single tasks performed as Kubernetes Jobs via Argo Workflows. Kubeflow Pipelines comes with its own DSL specification on top of Python, which is compiled into a workflow definition ready for execution in Kubernetes. In this way wach task, its resources, dependencies, etc may be configured indipendently; the management and tracking is performed by the Kubeflow Pipelines component, equipped also with the Web-based UI for monitoring.  </p> <p>The platform used Kubeflow pipelines to</p> <ul> <li>implement the composite pipelines through its Core orchestrator component and UI</li> <li>support MLRun pipelines. </li> </ul> <p>Currently, version v1 of the Kubeflow Pipelines is used for the compatibility purposes. The definition of the KFP workflows is provided in the corresponding KFP Runtime section.</p> <p>How to access</p> <p>Kubeflow Pipelines UI may be accessed from the dashboard. From its interface, you will be able to monitor the deployed workflows and their executions.</p>"},{"location":"components/kubeflow/#resources","title":"Resources","text":""},{"location":"components/kubeflow/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/mlrun/","title":"MLRun","text":"<p>MLRun is a MLOps framework for building and managing machine-learning applications and automating the workflow.</p> <ul> <li>Ingest and transform data</li> <li>Develop ML models, train and deploy them</li> <li>Track performance and detect problems</li> </ul> <p>How to access</p> <p>MLRun may be accessed from the dashboard. From its interface, you will be able to monitor projects and workflows. After launching it from Coder you can access MLRun on MLRun UI (http://nodeipaddress:30060)</p>"},{"location":"components/mlrun/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/nuclio/","title":"Nuclio","text":"<p>Nuclio is a generic purpose Serverless platform for executing and exposing statelss functions as services, run periodic Jobs etc. Nuclio is bundled with MLRun platform but may be used also indipendently offering the management capabilities both via API, CLI, and Web interfaces. It is possible to create serverless applications (deployed as Kubernetes services) using Python, Java, NodeJS, .NET Core, or shell scripts. Nuclio can therefore  be used to</p> <ul> <li>Create and expose arbitrary data services or microservices</li> <li>Expose ML Models as services through MLRun platform</li> <li>Define processors that are triggered periodically (on Cron expression) or with explicit events (e.g., attached to event-based infrastructure, like Kafka).</li> </ul> <p>How to access</p> <p>Nuclio may be accessed from the dashboard or from within MLRun. From its interface, you will be able to monitor and manage serverless functions grouped by projects.</p>"},{"location":"components/nuclio/#resources","title":"Resources","text":""},{"location":"components/nuclio/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/resourcemanager/","title":"Kubernetes Resource Manager","text":"<p>Kubernetes Resource Manager (KRM) is an application to manage several types of Kubernetes resources:</p> <ul> <li>Custom resources</li> <li>Services</li> <li>Deployments</li> <li>Volumes</li> <li>Jobs</li> </ul> <p>It consists in a back-end, written in Java, which connects to the Kubernetes API to perform actions on resources, and a front-end, written in React and based on React-admin.</p> <p>Instructions on how to install and start an instance can be found on the repository.</p>"},{"location":"components/resourcemanager/#standard-kubernetes-resources","title":"Standard Kubernetes Resources","text":"<p>With KRM you can control the main Kubernetes resources (e.g., services, deployments), manage Persistent Volume Claims, and access secrets. Click the corresponding button in the left menu, and view the details of one item by clicking its Show button.</p>"},{"location":"components/resourcemanager/#custom-resources","title":"Custom resources","text":"<p>Custom resources can be viewed, created, edited and deleted through the use of the interface. </p> <p>If you don't see a specific kind of custom resource listed to the left, it means neither Kubernetes nor KRM contain a schema for it. A schema is required so that the application may understand and describe the related resources.</p> <p>If some resources already exist, they will immediately be visible.</p>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"runtimes/container/","title":"Container runtime","text":"<p>The Container runtime allows you to create deployments, jobs and services on Kubernetes.</p>"},{"location":"runtimes/container/#prerequisites","title":"Prerequisites","text":"<p>Python version and libraries:</p> <ul> <li><code>python &gt;= 3.9</code></li> <li><code>digitalhub-runtime-container</code></li> </ul> <p>The package is available on PyPI:</p> <pre><code>python -m pip install digitalhub-runtime-container\n</code></pre>"},{"location":"runtimes/container/#how-to","title":"HOW TO","text":"<p>With the Container runtime you can launch pods and services on Kubernetes. It is built having remote online execution capabilities.</p>"},{"location":"runtimes/container/#function","title":"Function","text":"<p>The Container runtime introduces a function of kind <code>container</code> that allows you to deploy deployments, jobs and services on Kubernetes.</p> <p>he syntax for creating a <code>Function</code> is the same as the new_function method.</p>"},{"location":"runtimes/container/#function-parameters","title":"Function parameters","text":"Name Type Description Default project str Project name required (if creating from library) name str Name that identifies the object required kind str Kind of the object required (must be <code>python</code>) uuid str ID of the object in form of UUID None description str Description of the object None git_source str Remote git source for object None labels list[str] List of labels None embedded bool Flag to determine if object must be embedded in project True base_image str The base container image None (required if task is <code>build</code>) command str The command to run inside the container None args list[str] The arguments to pass to the command None <p>For example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project('my_project')\nfunction = dh.new_function(\n    kind='dbt',\n    name='my_function',\n    image=\"hello-world:latest\"\n)\n</code></pre>"},{"location":"runtimes/container/#task","title":"Task","text":"<p>The Container runtime introduces three task's kinds:</p> <ul> <li><code>job</code>: to deploy a job</li> <li><code>deploy</code>: to deploy a deployment</li> <li><code>serve</code>: to deploy a service</li> <li><code>build</code>: to build a docker image</li> </ul>"},{"location":"runtimes/container/#task-parameters","title":"Task parameters","text":"Name Type Description Default Kind specific action str Task action. Must be one of: <li><code>job</code></li><li><code>serve</code></li><li><code>build</code></li> required node_selector list[dict] Node selector None volumes list[dict] List of volumes None resources dict Resources restrictions None affinity dict Affinity None tolerations list[dict] Tolerations None envs list[dict] Env variables None secrets list[str] List of secret names None backoff_limit int Backoff limit None <code>job</code> schedule str Backoff limit None <code>job</code> instructions list[str] Build instructions to be executed as RUN instructions in Dockerfile.Example: <code>apt install git -y</code> None <code>build</code> replicas int Number of replicas None <code>deploy</code>, <code>serve</code> service_port list[dict] Service port where to expose the service. Must be: [{port: port, target_port: target_port}, ...] <code>NodePort</code> <code>serve</code> service_type str Service type. Must be one of: <li><code>ClusterIP</code></li><li><code>LoadBalancer</code></li><li><code>NodePort</code></li> <code>NodePort</code> <code>serve</code>"},{"location":"runtimes/container/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\nproj = dh.get_or_create_project(\"project-container\")\n\n# Run container\nfunc_cont = proj.new_function(name=\"function-container\",\n                              kind=\"container\",\n                              image=\"hello-world:latest\")\nrun_cont = func_cont.run(\"job\")\n\n\n# Serve stremlit service\nfunc_serve = proj.new_function(name=\"function-serve\",\n                               kind=\"container\",\n                               image=\"ghcr.io/scc-digitalhub/digitalhub-core-streamlit:latest\")\nrun_serve = func_serve.run(\"serve\",\n                           service_ports= [{\"port\": 8085, \"target_port\": 8501}],\n                           service_type=\"NodePort\")\n</code></pre>"},{"location":"runtimes/dbt/","title":"DBT runtime","text":"<p>The DBT runtime allows you to run DBT transformations on your data. It is a wrapper around the DBT CLI tool. The runtime introduces a function of kind <code>dbt</code> and a task of kind <code>transform</code>.</p>"},{"location":"runtimes/dbt/#prerequisites","title":"Prerequisites","text":"<p>Python version and libraries:</p> <ul> <li><code>python &gt;= 3.9</code></li> <li><code>digitalhub-runtime-dbt</code></li> </ul> <p>The package is available on PyPI:</p> <pre><code>python -m pip install digitalhub-runtime-dbt # for remote execution only\npython -m pip install digitalhub-runtime-dbt[local] # for local execution\n</code></pre>"},{"location":"runtimes/dbt/#how-to","title":"HOW TO","text":"<p>With the DBT runtime you can use the function's <code>run()</code> method to execute a DBT query you have defined. The DBT runtime execution workflow follows roughly these steps:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally. The runtime tries to get the file from the <code>path</code> attribute in the dataitem specification. At the moment, we support the following path types:<ul> <li><code>http(s)://&lt;url&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>sql://&lt;database&gt;(/&lt;schema-optional&gt;)/&lt;table&gt;</code></li> <li><code>&lt;local-path&gt;</code></li> </ul> </li> <li>The runtime inserts the data into a temporary versioned table in the default postgres database. These tables are named <code>&lt;dataitem-name&gt;_v&lt;dataitem-id&gt;</code>, and will be deleted at the end of the execution.</li> <li>The runtime collect the source code of the DBT query and creates all the necessary DBT artifacts (profiles.yml, dbt_project.yml, etc.) and runs the DBT transformation.</li> <li>The runtime stores the output table into the default postgres database as result of the DBT execution. The table name is built from the <code>outputs</code> parameter. Then, the runtime creates a dataitem with the <code>outputs</code> name parameter and saves it into the Core backend. You can retrieve the dataitem with the <code>run.outputs()</code> method. In general, the output table versioned is named <code>&lt;dataitem-output-name&gt;_v&lt;dataitem-output-id&gt;</code> and is stored in the default postgres database passed to the runtime via env variable.</li> </ol>"},{"location":"runtimes/dbt/#source-code","title":"Source code","text":"<p>You can define a DBT query as usual, and store it in several ways. You will then refernce the source code according to a specific <code>Function</code> parameter as shown in the following table:</p> Format Parameter Type Example remote git repository source <code>str</code> \"git+https://github.com/some-user/some-repo\" zip s3 archive source <code>str</code> \"zip+s3://some-bucket/some-key.zip\" base64 encoded string base64 <code>str</code> \"encodedstring\" string code <code>str</code> \"select * from some_table\""},{"location":"runtimes/dbt/#function","title":"Function","text":"<p>The DBT runtime introduces a function of kind <code>dbt</code> that allows you to execute sql dbt queries on your data.</p>"},{"location":"runtimes/dbt/#function-parameters","title":"Function parameters","text":"Name Type Description Default project str Project name required (if creating from library) name str Name that identifies the object required kind str Kind of the object required (must be <code>python</code>) uuid str ID of the object in form of UUID None description str Description of the object None git_source str Remote git source for object None labels list[str] List of labels None embedded bool Flag to determine if object must be embedded in project True source dict Source code details required"},{"location":"runtimes/dbt/#source","title":"Source","text":"<p>The <code>source</code> parameter must be a dictionary containing reference to the sql query to be executed. The parameter is structured as a dictionary with the following keys:</p> <ul> <li><code>source</code>: the source URI to the code. It accepts the following values:<ol> <li>git+https://repo-host/repo-owner/repo.git#indication-where-to-checkout: the code is fetched from a git repository. The link points to the root of the repository, the fragment is as simple indication of the branch, tag or commit to checkout. The runtime will clone the repository and checkout the indicated branch, tag or commit.</li> <li>zip+s3://path-to-some-code.zip: the code is fetched from a zip file in the minio digitalhub instance. The link points to the path to the zip file. The runtime will download the zip file and extract it. It fails if the zip file is not valid.</li> </ol> </li> <li><code>code</code>: the python string code</li> <li><code>base64</code>: the base64 encoded code</li> <li><code>lang</code>: the language of the code use in the console higlihter</li> </ul>"},{"location":"runtimes/dbt/#function-example","title":"Function example","text":"<pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my_project\")\n\nsql = \"\"\"\nSELECT * FROM {{ ref(\"my_table\") }}\n\"\"\"\n\ndataitem = project.new_dataitem(\"my_dataitem\", kind=\"table\", path=\"path-to-some-data\")\n\nfunction = dh.new_function(\n    kind=\"dbt\",\n    name=\"my_function\",\n    source={\"code\": sql}\n)\n</code></pre>"},{"location":"runtimes/dbt/#task","title":"Task","text":"<p>The DBT runtime introduces a task of kind <code>transform</code> that allows you to run a DBT transformation on your data. A <code>Task</code> is created with the <code>run()</code> method, so it's not managed directly by the user. The parameters for the task creation are passed directly to the <code>run()</code> method, and may vary depending on the kind of task.</p>"},{"location":"runtimes/dbt/#task-parameters","title":"Task parameters","text":"Name Type Description Default Kind specific action str Task action. Must be one of: <li><code>transform</code></li> required node_selector list[dict] Node selector None volumes list[dict] List of volumes None resources dict Resources restrictions None affinity dict Affinity None tolerations list[dict] Tolerations None envs list[dict] Env variables None secrets list[str] List of secret names None"},{"location":"runtimes/dbt/#task-example","title":"Task example","text":"<pre><code>run = function.run(\n    action=\"transform\",\n    inputs={\"my_table\": my_dataitem.key},\n    outputs={\"output_table\": \"my_output_table\"},\n)\n</code></pre>"},{"location":"runtimes/dbt/#run","title":"Run","text":"<p>The <code>Run</code> object is, similar to the <code>Task</code>, created with the <code>run()</code> method. The run's parameters are passed alongside the task's ones.</p>"},{"location":"runtimes/dbt/#run-parameters","title":"Run parameters","text":"Name Type Description Default loacal_execution bool Flag to indicate if the run will be executed locally False inputs dict Input entity key. None outputs dict Outputs mapped. None parameters dict Extra parameters for a function. None"},{"location":"runtimes/dbt/#run-example","title":"Run example","text":"<pre><code>run = function.run(\n    action=\"job\",\n    inputs={\n        \"dataitem\": dataitem.key\n    },\n    outputs={\n        \"dataitem\": \"mapped-name\",\n        \"label\": \"some-label\"\n    }\n)\n</code></pre>"},{"location":"runtimes/dbt/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-dbt\")\n\n# Create new input dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\ndi = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n\n# Create new function\nsql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref(\"employees\") }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = \"60\"\n\"\"\"\nfunction = project.new_function(name=\"function-dbt\",\n                                kind=\"dbt\",\n                                source={\"code\": sql})\n\n# Run function\nrun = function.run(\"transform\",\n                   inputs={\"employees\": di.key},\n                   outputs={\"output_table\": \"department-60\"})\n\n# Refresh run\nrun.refresh()\n</code></pre>"},{"location":"runtimes/kfp_pipelines/","title":"KFP Pipelines Runtime","text":"<p>TODO</p>"},{"location":"runtimes/mlrun/","title":"Mlrun runtime","text":"<p>The mlrun runtime allows you to execute mlrun function. It's a wrapper around mlrun methods. The runtime introduces a function of kind <code>mlrun</code> and a task of kind <code>job</code>.</p>"},{"location":"runtimes/mlrun/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python 3.9 or 3.10</li> <li>digitalhub sdk</li> <li>mlrun</li> </ul> <p>We need first to collect digitalhub mlrun modules:</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/ data/ ml/ ./\npip install -r ml/modules/mlrun/requirements-wrapper.txt\npip install -r ml/modules/mlrun\n</code></pre> <p>If you want to exeute the mlrun runtime only remotely, you can avoid to install the requirement-wrapper.</p>"},{"location":"runtimes/mlrun/#function","title":"Function","text":"<p>The mlrun runtime introduces a function of kind <code>mlrun</code> that allows you to execute sql mlrun queries on your data.</p>"},{"location":"runtimes/mlrun/#mlrun-function-parameters","title":"Mlrun function parameters","text":"<p>When you create a function of kind <code>mlrun</code>, you need to specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>mlrun</code></li> <li><code>source</code>: the source dictionary that contains the code, encoded code or path to code to be executed by mlrun. See section below</li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function (git repository)</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> </ul>"},{"location":"runtimes/mlrun/#source","title":"Source","text":"<p>The <code>source</code> parameter must be a dictionary containing reference to the sql query to be executed. The parameter is structured as a dictionary with the following keys:</p> <ul> <li><code>source</code>: the source URI to the code. It accepts the following values:<ol> <li>git+https://repo-host/repo-owner/repo.git#indication-where-to-checkout: the code is fetched from a git repository. The link points to the root of the repository, the fragment is as simple indication of the branch, tag or commit to checkout. The runtime will clone the repository and checkout the indicated branch, tag or commit.</li> <li>zip+s3://path-to-some-code.zip: the code is fetched from a zip file in the minio digitalhub instance. The link points to the path to the zip file. The runtime will download the zip file and extract it. It fails if the zip file is not valid.</li> </ol> </li> <li><code>code</code>: the python string code</li> <li><code>base64</code>: the base64 encoded code</li> <li><code>lang</code>: the language of the code use in the console higlihter</li> </ul> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project('my_project')\n\npath = 'path-to-some-code.py'\ndataitem = project.new_dataitem(\"my_dataitem\", kind=\"table\", path=\"path-to-some-data\")\n\nfunction = dh.new_function(\n    kind='mlrun',\n    name='my_function',\n    source={\"source\": path}\n)\n</code></pre>"},{"location":"runtimes/mlrun/#task","title":"Task","text":"<p>The mlrun runtime introduces a task of kind <code>job</code> that allows you to execute a mlrun function.</p>"},{"location":"runtimes/mlrun/#job-task-parameters","title":"Job task parameters","text":"<p>When you want to execute a task of kind <code>job</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>job</code>.</li> </ul> <p>The following parameters are optional, but usually you need to pass them:</p> <ul> <li><code>inputs</code>: the dictionary of referenced items used in the mlrun function.</li> <li><code>outputs</code>: a dictionary of referenced items produced by the mlrun function.</li> <li><code>parameters</code>: a dictionary of parameters to pass to the mlrun function <code>mlrun.run_function()</code></li> <li><code>values</code>: a list of output values that are not <code>artifacts</code>, <code>dataitems</code> or <code>models</code></li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>Example:</p> <pre><code>run = function.run(\n    action='job',\n    inputs={\"mlrun-input-param-name\": my_dataitem.key},\n    outputs={\"mlrun-input-param-name\": \"my-output-name\"},\n    parameters={\"inputs\": {\"key\": \"value\"}},\n    values=[\"simple-mlrun-output-value-name\"]\n)\n</code></pre>"},{"location":"runtimes/mlrun/#runtime-workflow","title":"Runtime workflow","text":"<p>The mlrun runtime execution workflow is the following:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally.</li> <li>It creates mlrun project and function.</li> <li>It passes the local fetched data path to the mlrun function referenced by the input key as parameter and the content of <code>parameters</code>.</li> <li>It executes the mlrun function and parses the results. It maps the outputs with the name passed in the <code>outputs</code> parameter. If the outputs are not <code>artifacts</code>, <code>dataitems</code> or <code>models</code>, the output is mapped with the <code>values</code>.</li> <li>You can retrieve the outputs with the <code>run.outputs()</code> method.</li> </ol>"},{"location":"runtimes/mlrun/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-mlrun\")\n\n# Create new input dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n\n# Create new dataitem\ndataitem = project.new_dataitem(name=\"url-dataitem\",\n                                kind=\"table\",\n                                path=url)\n\n# Create new function\ndownloader_function = project.new_function(name=\"mlrun-downloader\",\n                                           kind=\"mlrun\",\n                                           source={\"source\":\"pipeline.py\"},\n                                           handler=\"downloader\",\n                                           image=\"mlrun/mlrun\")\n\n# Run function\ndownloader_run = downloader_function.run(\"job\",\n                                         inputs={\"url\": dataitem.key},\n                                         outputs={\"dataset\": \"dataset\"})\n\n# Run refresh\ndownloader_run.refresh()\n</code></pre> <p>pipeline.py file:</p> <pre><code>import mlrun\nimport pandas as pd\n\n@mlrun.handler(outputs=[\"dataset\"])\ndef downloader(context, url: mlrun.DataItem):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(format='parquet')\n    return df\n</code></pre>"},{"location":"runtimes/nefertem/","title":"Nefertem runtime","text":"<p>The Nefertem runtime allows you to run Nefertem validation, profiling or inference on your data. It is a wrapper around the Nefertem library. The runtime introduces a function of kind <code>neferetm</code> and four task of kind <code>validate</code>, <code>profile</code>, <code>infer</code> and <code>metric</code>.</p>"},{"location":"runtimes/nefertem/#prerequisites","title":"Prerequisites","text":"<p>Python libraries:</p> <ul> <li>python &gt;= 3.9</li> <li>digitalhub</li> <li>digitalhub-data-nefertem</li> <li>Nefertem plugins available in the Nefertem repository</li> </ul> <p>If you want to execute Nefertem tasks locally, you need to install digitalhub-core-nefertem package with <code>local</code> flag:</p> <pre><code>git clone https://github.com/scc-digitalhub/digitalhub-sdk.git\ncd digitalhub-sdk\npip install core/ data/ ./\npip install data/modules/nefertem[local]\n</code></pre>"},{"location":"runtimes/nefertem/#function","title":"Function","text":"<p>The Nefertem runtime introduces a function of kind <code>neferetm</code> that allows you to execute various tasks on your data.</p>"},{"location":"runtimes/nefertem/#nefertem-function-parameters","title":"Nefertem function parameters","text":"<p>When you create a function of kind <code>neferetm</code>, you need to specify the following mandatory parameters:</p> <ul> <li><code>project</code>: the project name with which the function is associated. Only if you do not use the project context to create the function, e.g. <code>project.new_function()</code>.</li> <li><code>name</code>: the name of the function</li> <li><code>kind</code>: the kind of the function, must be <code>neferetm</code></li> </ul> <p>Optionally, you can specify the following parameters:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the function</li> <li><code>labels</code>: the labels of the function</li> <li><code>git_source</code>: the remote source of the function (git repository)</li> <li><code>source_code</code>: pointer to the source code of the function</li> <li><code>constraints</code>: the constraints of the function to be applied on the data. Valid only for <code>validate</code> tasks</li> <li><code>error_report</code>: the error report output format. Valid only for <code>validate</code> tasks</li> <li><code>embedded</code>: whether the function is embedded or not. If <code>True</code>, the function is embedded (all the details are expressed) in the project. If <code>False</code>, the function is not embedded in the project.</li> </ul> <p>For example:</p> <pre><code>import digitalhub_core as dh\n\nconstraint = {\n  'constraint': 'type',\n  'field': 'field-name',\n  'field_type': 'string',\n  'name': 'check_country_string',\n  'resources': ['ref-source'],\n  'title': '',\n  'type': 'const-type',\n  'value': 'string',\n  'weight': 5\n}\nfunction = dh.new_function(name=\"nefertem-function\",\n                           kind=\"nefertem\",\n                           constraints=[constraint])\n</code></pre>"},{"location":"runtimes/nefertem/#task","title":"Task","text":"<p>The Nefertem runtime introduces three tasks of kind <code>validate</code>, <code>profile</code> and <code>infer</code> that allows you to run a Nefertem validation, profiling or inference on your data.</p>"},{"location":"runtimes/nefertem/#validate-task-parameters","title":"Validate task parameters","text":"<p>When you want to execute a task of kind <code>validate</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>validate</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the dictionary of nefertem resources referenced in the constraint mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs={\"employees\": di.key})\n</code></pre>"},{"location":"runtimes/nefertem/#profile-task-parameters","title":"Profile task parameters","text":"<p>When you want to execute a task of kind <code>profile</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>profile</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the dictionary of nefertem resources referenced mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"profile\",\n                   framework=\"frictionless\",\n                   inputs={\"employees\": di.key})\n</code></pre>"},{"location":"runtimes/nefertem/#infer-task-parameters","title":"Infer task parameters","text":"<p>When you want to execute a task of kind <code>infer</code>, you need to pass the following mandatory parameters to the function method <code>run()</code>:</p> <ul> <li><code>action</code>: the action to perform. This must be <code>infer</code>.</li> <li><code>framework</code>: the Nefertem framework to be used.</li> <li><code>inputs</code>: the dictionary of nefertem resources referenced mapped to some dataitem keys. The corresponding dataitem objects must be present in the backend, whether it's local or Core backend.</li> </ul> <p>As optional, you can pass the following task parameters specific for remote execution:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject in the container</li> <li><code>secrets</code>: list of secrets to inject in the container</li> <li><code>backoff_limit</code>: the number of retries when a job fails.</li> <li><code>schedule</code>: the schedule of the job as a cron expression</li> <li><code>replicas</code>: the number of replicas of the deployment</li> </ul> <p>For example:</p> <pre><code>run = function.run(\"infer\",\n                   framework=\"frictionless\",\n                   inputs={\"employees\": di.key})\n</code></pre>"},{"location":"runtimes/nefertem/#runtime-workflow","title":"Runtime workflow","text":"<p>The Nefertem runtime execution workflow is the following:</p> <ol> <li>The runtime fetches the input dataitems by downloading them locally. The runtime tries to get the file from the <code>path</code> attribute. At the moment, we support the following path types:<ul> <li><code>http(s)://&lt;url&gt;</code></li> <li><code>s3://&lt;bucket&gt;/&lt;path&gt;</code></li> <li><code>sql://&lt;database&gt;(/&lt;schema-optional&gt;)/&lt;table&gt;</code></li> <li><code>&lt;local-path&gt;</code></li> </ul> </li> <li>The runtime creates a Nefertem <code>DataResource</code> from the input dataitem. The <code>DataResource</code> is a Nefertem object that represents the data to be validated, profiled, inferred or measured.</li> <li>The runtime then create a Nefertem <code>run</code> and execute it. The Nefertem <code>run</code> executes three methods based on the task, and produces a <code>run_metadata</code> report file:</li> <li>If the task is <code>validate</code>:<ul> <li><code>run.validate()</code></li> <li><code>run.log_report()</code> -&gt; produces a <code>NefertemReport</code></li> <li><code>run.persist_report()</code> -&gt; produces one or more validation framework reports</li> </ul> </li> <li>If the task is <code>profile</code>:<ul> <li><code>run.profile()</code></li> <li><code>run.log_profile()</code> -&gt; produces a <code>NefertemProfile</code></li> <li><code>run.persist_profile()</code> -&gt; produces one or more profiling framework reports</li> </ul> </li> <li>If the task is <code>infer</code>:<ul> <li><code>run.infer()</code></li> <li><code>run.log_schema()</code> -&gt; produces a <code>NefertemSchema</code></li> <li><code>run.persist_schema()</code> -&gt; produces one or more inference framework reports</li> </ul> </li> <li>The runtime then creates an <code>Artifact</code> object for each file produced by Nefertem and saves it into the Core backend. It then uploads all the files to the default s3 storage provided. You can collect the artifacts with the <code>run.outputs()</code> method. In general, the saving path is <code>s3://&lt;bucket-from-env&gt;/&lt;project-name&gt;/artifacts/ntruns/&lt;nefertem-run-uuid&gt;/&lt;file&gt;</code>.</li> </ol>"},{"location":"runtimes/nefertem/#snippet-example","title":"Snippet example","text":"<pre><code>import digitalhub as dh\n\n# Get or create project\nproject = dh.get_or_create_project(\"project-nefertem\")\n\n# Create dataitem\nurl = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\ndi = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n\n# Create function\nconstraint = {\n  'constraint': 'type',\n  'field': 'SALARY',\n  'field_type': 'number',\n  'name': 'check_value_integer',\n  'title': '',\n  'resources': ['employees'],\n  'type': 'frictionless',\n  'value': 'number',\n  'weight': 5\n}\nfunction = project.new_function(name=\"function-nefertem\",\n                                kind=\"nefertem\",\n                                constraints=[constraint])\n\n# Run validate task\nrun = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs={\"employees\": di.key})\n\n# Refresh run\nrun.refresh()\n</code></pre>"},{"location":"runtimes/python/","title":"Python","text":"<p>The python runtime allows you to run generic python function within the platform. The runtime introduces a function of kind <code>python</code> and three task of kind <code>job</code>, <code>serve</code> and <code>build</code>.</p>"},{"location":"runtimes/python/#prerequisites","title":"Prerequisites","text":"<p>Python version and libraries:</p> <ul> <li><code>python &gt;= 3.9</code></li> <li><code>digitalhub-runtime-python</code></li> </ul> <p>The package is available on PyPI:</p> <pre><code>python -m pip install digitalhub-runtime-python\n</code></pre>"},{"location":"runtimes/python/#how-to","title":"HOW TO","text":"<p>With the python runtime you can use the function's <code>run()</code> method to execute a python function you have defined. The python runtime execution workflow follows roughly these steps:</p> <ol> <li>Define somewhere a python function.</li> <li>Create a <code>Function</code> object in the platform and execute the function's <code>run()</code> method.</li> <li>The runtime collects the inputs specified in the function as SDK objects (<code>Dataitem</code>, <code>Artifact</code>, <code>Model</code>).</li> <li>It fetches the function source code and import the function handler.</li> <li>It composes the parameters for the handler function.</li> <li>It executes the function and map the outputs as SDK objects or as simple results.</li> </ol>"},{"location":"runtimes/python/#python-function-definition","title":"Python function definition","text":"<p>You can declare a generic python function as usual with the <code>def</code> keyword. There are some restriction that must be applied when defining the function:</p> <ol> <li>The argument <code>project</code> is reserved. The runtime overrides the function parameters and assign to the <code>project</code> argument a <code>Project</code> object, used as SDK context. With the <code>Project</code> object you can manipulate entities like <code>Artifact</code>, <code>Dataitem</code>, etc. If you provide a <code>project</code> argument into the function and use it as a non <code>Project</code> object, you will probably get an error. If you define the <code>project</code> argument into your functions signature, you can use the <code>project</code> variable as <code>Project</code> object.</li> <li>The arguments <code>context</code> and <code>events</code> are reserved in remote execution. These arguments are reserved for <code>nuclio</code> <code>context</code> and <code>events</code> function parameters. If you define these arguments into your functions signature, you can use the <code>context</code> and <code>events</code> variables as <code>nuclio</code> <code>context</code> and <code>events</code> objects.</li> <li>If some arguments of the function refer to some SDK objects, they must be mapped inside the run's <code>inputs</code> parameter. Other arguments of the function can be mapped inside the run's <code>parameter</code> parameter. More on that on the Parameters composition section.</li> <li>You may or may not decorate your function with the <code>@handler</code> decorator you can import from the <code>digitalhub_runtime_python</code> package. If you decorate your function and return something, you need to map the outputs in the decorator and in the run's <code>outputs</code> parameter. More on that on the Parameters composition section.</li> </ol>"},{"location":"runtimes/python/#function-definition-example","title":"Function definition example","text":"<pre><code>from digitalhub_runtime_python import handler\n\n# 1. Simple function that returns a string\n\ndef func1():\n   return \"hello world\"\n\n# 2. Decorated function that returns a string\n\n# If you decorate your function and return something, you need to map the outputs\n# in the decorator and in the run's `outputs` parameter\n@handler(outputs=[\"result\"])\ndef func2():\n   return \"hello world\"\n\n\n# 3. Function with project argument\ndef func3(project):\n   # allowed use of project variable\n   project.log_artifact(name=\"example\",\n                        kind=\"artifact\",\n                         source_path=\"/path/to/file\")\n\n   # not allowed use of project variable\n   project.some_method_not_from_sdk() # Probably there will be an error\n\n\n# 4. Function with context and events arguments\ndef func4(context, events):\n   # allowed use of context and events variables in remote execution\n   context.logger.info(\"Some log\")\n\n# 5. Function with mixed input arguments\ndef func5(di: Dataitem, param1: str):\n   # di refers to a Dataitem object, so it must be mapped\n   # into runs inputs paramaters\n   # param1 is a string, it must be mapped into runs input parameters\n</code></pre>"},{"location":"runtimes/python/#parameters-composition","title":"Parameters composition","text":""},{"location":"runtimes/python/#inputs","title":"Inputs","text":"<p>To properly pass the parameters you need to your function, you must map them in the <code>function.run()</code> method. Ther are some rules you need to follow:</p> <ul> <li>If you expect one of your arguments to be a <code>Dataitem</code>/<code>Artifact</code>/<code>Model</code> object, you need to explicit the reference to the object into the run's <code>inputs</code> parameter using the argument name as key and the object key as value.</li> </ul> <pre><code># Define your function and declare di argument as Dataitem\ndef func(di: Dataitem):\n   # do something with di\n\n\n# Create a dataitem\nsdk_dataitem = sdk.new_dataitem(...)\n\n# Reference the di argument as key and the dataitem key as value\nsdk_function.run(inputs={\"di\": sdk_dataitem.key})\n</code></pre> <ul> <li>Other function arguments must be mapped inside the run's <code>parameter</code> parameter.</li> </ul> <pre><code># Define your function and declare di argument as Dataitem\ndef func(di: Dataitem, param1: str):\n   # do something with di\n\n\n# Create a dataitem\nsdk_dataitem = sdk.new_dataitem(...)\n\n# Reference the di argument as key and the dataitem key as value\nsdk_function.run(inputs={\"di\": sdk_dataitem.key},\n                 parameter={\"param1\": \"some value\"})\n</code></pre>"},{"location":"runtimes/python/#outputs","title":"Outputs","text":"<p>The outputs of the function must be mapped inside the run's <code>outputs</code> parameter if you return something and you decorate your function.</p> <pre><code>from digitalhub_runtime_python import handler\n\n@handler(outputs=[\"result\", \"other_result\"])\ndef func(di: Dataitem, param1: str):\n   # do something with di\n   return \"some value\", \"some other value\"\n\n\nsdk_function.run(inputs={\"di\": sdk_dataitem.key},\n                 parameter={\"param1\": \"some value\"},\n                 outputs={\"result\": \"named_result\",\n                          \"other_result\": \"named_other_result\"})\n</code></pre>"},{"location":"runtimes/python/#source-code","title":"Source code","text":"<p>Once you have defined your function, you can store your code in several ways. You will then refernce the source code according to a specific <code>Function</code> parameter as shown in the following table:</p> Format Parameter Type Example local file path code_src <code>str</code> \"path/to/file.py\" remote git repository code_src <code>str</code> \"git+https://github.com/some-user/some-repo\" zip s3 archive code_src <code>str</code> \"zip+s3://some-bucket/some-key.zip\" base64 encoded string base64 <code>str</code> \"encodedstring\" string code <code>str</code> \"def func():\\n   return \\\"hello world\\\"\""},{"location":"runtimes/python/#function","title":"Function","text":"<p>The python runtime introduces a function of kind <code>python</code>.</p>"},{"location":"runtimes/python/#function-parameters","title":"Function parameters","text":"Name Type Description Default project str Project name required (if creating from library) name str Name that identifies the object required kind str Kind of the object required (must be <code>python</code>) uuid str ID of the object in form of UUID None description str Description of the object None git_source str Remote git source for object None labels list[str] List of labels None embedded bool Flag to determine if object must be embedded in project True code_src str Pointer to source code None code str Source code (plain text) None base64 str Source code (base64 encoded) None handler str Function entrypoint None init_function str Init function for remote nuclio execution None lang str Source code language (hint) None source dict Source code details as dictionary None python_version str Python version to use, must be one of: <li><code>PYTHON3_9</code></li><li><code>PYTHON3_10</code></li> None image str Image where the function will be executed None base_image str Base image used to build the image where the function will be executed None (required when using <code>build</code> task) requirements list Requirements list to be installed in the image where the function will be executed None"},{"location":"runtimes/python/#function-example","title":"Function example","text":"<pre><code>import digitalhub_core as dh\n\n# From project ...\n\nfunction = project.new_function(name=\"python-function\",\n                                kind=\"python\",\n                                code_src=\"main.py\",\n                                handler=\"function\",\n                                python_version=\"PYTHON3_9\")\n\n# .. or from sdk\n\nfunction = dh.new_function(project=\"my-project\",\n                           name=\"python-function\",\n                           kind=\"python\",\n                           source={\n                              \"source\": \"main.py\",\n                              \"handler\": \"function\",\n                           }\n                           python_version=\"PYTHON3_9\")\n</code></pre>"},{"location":"runtimes/python/#task","title":"Task","text":"<p>The python runtime introduces three tasks of kind <code>job</code>, <code>serve</code> and <code>build</code> that allows you to run a python function execution, serving a function as a service or build a docker image where the function is executed. A <code>Task</code> is created with the <code>run()</code> method, so it's not managed directly by the user. The parameters for the task creation are passed directly to the <code>run()</code> method, and may vary depending on the kind of task.</p>"},{"location":"runtimes/python/#task-parameters","title":"Task parameters","text":"Name Type Description Default Kind specific action str Task action. Must be one of: <li><code>job</code></li><li><code>serve</code></li><li><code>build</code></li> required node_selector list[dict] Node selector None volumes list[dict] List of volumes None resources dict Resources restrictions None affinity dict Affinity None tolerations list[dict] Tolerations None envs list[dict] Env variables None secrets list[str] List of secret names None backoff_limit int Backoff limit None <code>job</code> instructions list[str] Build instructions to be executed as RUN instructions in Dockerfile.Example: <code>apt install git -y</code> None <code>build</code> replicas int Number of replicas None <code>serve</code> service_type str Service type. Must be one of: <li><code>ClusterIP</code></li><li><code>LoadBalancer</code></li><li><code>NodePort</code></li> <code>NodePort</code> <code>serve</code>"},{"location":"runtimes/python/#task-example","title":"Task example","text":"<pre><code>run = function.run(\n    action=\"job\",\n    backoff_limit=1,\n)\n</code></pre>"},{"location":"runtimes/python/#run","title":"Run","text":"<p>The <code>Run</code> object is, similar to the <code>Task</code>, created with the <code>run()</code> method. The run's parameters are passed alongside the task's ones.</p>"},{"location":"runtimes/python/#run-parameters","title":"Run parameters","text":"Name Type Description Default loacal_execution bool Flag to indicate if the run will be executed locally False inputs dict Input entity key. None outputs dict Outputs mapped. None parameters dict Extra parameters for a function. None"},{"location":"runtimes/python/#run-example","title":"Run example","text":"<pre><code>run = function.run(\n    action=\"job\",\n    inputs={\n        \"dataitem\": dataitem.key\n    },\n    outputs={\n        \"dataitem\": \"mapped-name\",\n        \"label\": \"some-label\"\n    }\n)\n</code></pre>"},{"location":"scenarios/dremio_grafana/scenario/","title":"Data transformation and usage with Dremio and Grafana","text":"<p>In this scenario we will learn how to use Dremio to transform data and create some virtual datasets on top of it. Then we will visualize the transformed data in a dashboard created with Grafana.</p> <p>In order to collect the initial data and make it accessible to Dremio, we will follow the first step of the ETL scenario, in which we download some traffic data and store it in the DigitalHub datalake.</p>"},{"location":"scenarios/dremio_grafana/scenario/#collect-the-data","title":"Collect the data","text":"<p>NOTE: the procedure is only summarized here, as it is already described in depth in the ETL scenario introduction and Collect the data pages.</p> <ol> <li>Access Jupyter from your Coder instance and create a new notebook using the <code>Python 3 (ipykernel)</code> kernel</li> <li>Set up the environment and create a project</li> <li>Set the URL to the data:</li> </ol> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\n</code></pre> <ol> <li>Create the <code>src</code> folder, define the download function and register it</li> <li>Execute it locally and wait for its completion:</li> </ol> <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre>"},{"location":"scenarios/dremio_grafana/scenario/#access-the-data-from-dremio","title":"Access the data from Dremio","text":"<p>Access Dremio from your Coder instance or create a new Dremio workspace. You should see MinIO already configured as an object storage and you should find the downloaded data in a .parquet file at the path <code>minio/datalake/projects/demo-etl/artifacts/download-data-downloader/0/dataset.parquet</code>.</p> <p>Click on the file to open Dataset Settings, verify that the selected format is <code>Parquet</code> and save it as a Dremio dataset, so that it can be queried.</p> <p>Now you can see the data either by clicking again on the dataset or via the SQL runner, by executing a query such as:</p> <pre><code>SELECT *\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\nORDER BY data, \"codice spira\"\n</code></pre> <p>Create a new Dremio space named <code>demo_etl</code>. We will create three virtual datasets and save them here.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-measurement-data","title":"Extract measurement data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic measurements to save them as a separate dataset:</p> <pre><code>SELECT \"dataset.parquet\".data, \"dataset.parquet\".\"codice spira\", \"00:00-01:00\", \"01:00-02:00\", \"02:00-03:00\", \"03:00-04:00\", \"04:00-05:00\", \"05:00-06:00\", \"06:00-07:00\", \"07:00-08:00\", \"08:00-09:00\", \"09:00-10:00\", \"10:00-11:00\", \"11:00-12:00\", \"12:00-13:00\", \"13:00-14:00\", \"14:00-15:00\", \"15:00-16:00\", \"16:00-17:00\", \"17:00-18:00\", \"18:00-19:00\", \"19:00-20:00\", \"20:00-21:00\", \"21:00-22:00\", \"22:00-23:00\", \"23:00-24:00\"\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-traffic-sensors-data","title":"Extract traffic sensors data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic sensors data (e.g. their geographical position) as a separate dataset:</p> <pre><code>SELECT DISTINCT \"dataset.parquet\".\"codice spira\", \"dataset.parquet\".tipologia, \"dataset.parquet\".id_uni, \"dataset.parquet\".codice, \"dataset.parquet\".Livello, \"dataset.parquet\".\"codice arco\", \"dataset.parquet\".\"codice via\", \"dataset.parquet\".\"Nome via\", \"dataset.parquet\".\"Nodo da\", \"dataset.parquet\".\"Nodo a\", \"dataset.parquet\".stato, \"dataset.parquet\".direzione, \"dataset.parquet\".angolo, \"dataset.parquet\".longitudine, \"dataset.parquet\".latitudine, \"dataset.parquet\".geopoint\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>spire</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#transform-hourly-measurements-into-daily-measurements","title":"Transform hourly measurements into daily measurements","text":"<p>Open the SQL runner and execute the following query, which will sum the measurement columns, each corresponding to an hour, to obtain the daily value and save it as a new dataset:</p> <pre><code>SELECT data, \"codice spira\", \"00:00-01:00\"+\"01:00-02:00\"+\"02:00-03:00\"+\"03:00-04:00\"+\"04:00-05:00\"+\"05:00-06:00\"+\"06:00-07:00\"+\"07:00-08:00\"+\"08:00-09:00\"+\"09:00-10:00\"+\"10:00-11:00\"+\"11:00-12:00\"\n+\"12:00-13:00\"+\"13:00-14:00\"+\"14:00-15:00\"+\"15:00-16:00\"+\"16:00-17:00\"+\"17:00-18:00\"+\"18:00-19:00\"+\"19:00-20:00\"+\"20:00-21:00\"+\"21:00-22:00\"+\"22:00-23:00\"+\"23:00-24:00\" AS totale_giornaliero\nFROM (\n  SELECT * FROM \"demo_etl\".misurazioni\n) nested_0;\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni_giornaliere</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#connect-grafana-to-dremio","title":"Connect Grafana to Dremio","text":"<p>Access Grafana from your Coder instance or create a new Grafana workspace. Open the left menu and navigate to Connections - Data Sources. Add a new <code>Dremio</code> data source configured as follows:</p> <ul> <li>Name: <code>Dremio</code></li> <li>URL: the Internal Endpoint you see on Coder for your Dremio workspace</li> <li>User: <code>admin</code></li> <li>Password: <code>&lt;dremio_password_set_on_coder&gt;</code></li> </ul> <p>Now you can create a dashboard to visualize Dremio data.</p> <p>An example dashboard is available as a JSON file at the <code>user/examples/dremio_grafana</code> path within the repository of this documentation. In order to use it, you can import it in Grafana instead of creating a new dashboard. You will need to update the <code>datasource.uid</code> field, which holds a reference to the Dremio data source in your Grafana instance, throughout the JSON model. The easiest way to obtain your ID is by navigating to the data source configuration page and copying it from the URL:</p> <pre><code>https://&lt;grafana_host&gt;/connections/datasources/edit/&lt;YOUR_DATASOURCE_ID&gt;\n</code></pre> <p>The dashboard includes three panels: a map of the traffic sensors, a table with the daily number of vehicles registered by each sensor and a graph of the vehicles registered monthly.</p> <p></p> <p>We can now use the dashboard to explore the data. We can either interact with the map to get the information related to each sensor, or use the dashboard filters to select different time ranges and analyze traffic evolution over time.</p>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in: <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre></p> <p>Define a function for downloading data as-is and persisting it in the data-lake: <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\nimport requests\n\n@handler(outputs=[\"dataset\"])\ndef downloader(project, url):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(file_format='csv',sep=\";\")\n    return df\n</code></pre></p> <p>Register the function in Core: <pre><code>func = project.new_function(\n                         name=\"download-data\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"src/download-data.py\", \"handler\": \"downloader\"})\n</code></pre></p> <p>This code creates a new function definition that uses Python runtime (versione 3.9) pointing to the create file and the handler method that should be called.</p> <p>For the function to be executed, we need to pass it a reference to the data item. Let us create and register the corresponding data item: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi= project.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n</code></pre></p> <p>It is also possible to see the data item directly in the Core UI. </p> <p>Then, execute the function (locally) as a single job. Note that it may take a few minutes. <pre><code>run = func.run(action=\"job\", inputs={'url':di.key}, outputs={\"dataset\": \"dataset\"}, local_execution=True)\n</code></pre></p> <p>Note that the <code>inputs</code> map should contain the references to the project entities (e.g., artifacts, dataitems, etc), while in order to pass literal values to the function execution it is necessary to use <code>parameters</code> map.</p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. The name of the artifact will be defined according to the mapping specified in <code>outputs</code> map: it maps the handler outputs (see the <code>@handler</code> annotation and its output definition) to the expected name.</p> <p>To get the value of the artifact we can refer to it by the output name: <pre><code>dataset_di = project.get_dataitem(entity_name='dataset')\n</code></pre></p> <p>Load the data item and then into a data frame: <pre><code>dataset_df = dataset_di.as_df()\n</code></pre></p> <p>Run <code>dataset_df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API: <pre><code>%%writefile 'src/api.py'\n\nimport pandas as pd\nimport os\n\n\ndef init_context(context):\n    di = context.project.get_dataitem(entity_name='dataset-measures')\n    df = di.as_df()\n    setattr(context, \"df\", df)\n\ndef handler(context, event):\n    df = context.df\n\n    if df is None:\n        return \"\"\n\n    # mock REST api\n    method = event.method\n    path = event.path\n    fields = event.fields\n\n    id = False\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields['page'])\n\n    if \"size\" in fields:\n        pageSize = int(fields['size'])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    res = {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n\n    return res\n</code></pre></p> <p>Register the function: <pre><code>api_func = project.new_function(\n                         name=\"api\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"src/api.py\", \"handler\": \"handler\", \"init_function\": \"init_context\"})\n</code></pre></p> <p>Please note that other than defining the handler method, it is possible to define the <code>init_function</code> to define the preparatory steps.</p> <p>Deploy the function (perform <code>serve</code> action): <pre><code>run_serve_model = api_func.run(action=\"serve\")\n</code></pre></p> <p>Wait till the deployment is complete and the necessary pods and services are up and running. <pre><code>run_serve_model.refresh()\n</code></pre></p> <p>When done, the status of the run contains the <code>service</code> element with the internal service URL to be used.</p> <pre><code>SERVICE_URL = run_serve_model.status['service']['url']\n</code></pre> <p>Invoke the API and print its results: <pre><code>with requests.get(f'{SERVICE_URL}/?page=5&amp;size=10') as r:\n    res = r.json()\nprint(res)\n</code></pre></p> <p>You can also use pandas to load the result in a data frame: <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre></p>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance. </p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a simple yet realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, are available in the <code>documentation/examples/etl</code> path within the repository of this documentation.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries: <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Create a project: <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p> <p>Check that the project has been created successfully: <pre><code>print(project)\n</code></pre></p>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> Download the file and save it locally: <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre></p> <p>Use pandas to read the file into a dataframe: <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together: <pre><code>%%writefile \"src/pipeline.py\"\n\nfrom digitalhub_runtime_kfp.dsl import pipeline_context\n\ndef pipeline(url):\n    with pipeline_context() as pc:\n        downloader = pc.step(\n            name=\"download-data\",\n            function=\"download-data\",\n            action=\"job\",\n            inputs={\"url\": url},\n            outputs={\"dataset\": \"dataset\"},\n        )\n\n        process_spire = pc.step(\n            name=\"process-spire\",\n            function=\"process-spire\",\n            action=\"job\",\n            inputs={\"di\": downloader.outputs[\"dataset\"]}\n        )\n\n        process_measures = pc.step(\n            name=\"process-measures\",\n            function=\"process-measures\",\n            action=\"job\",\n            inputs={\"di\": downloader.outputs[\"dataset\"]}\n        )\n</code></pre></p> <p>Here in the definition we use a simple DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method generates a KFP step that internally makes the remote execution of the corresponding job. Note that the syntax for step is similar to that of function execution.</p> <p>Register the workflow: <pre><code>workflow = project.new_workflow(name=\"pipeline\", kind=\"kfp\", source={\"source\": \"src/pipeline.py\"}, handler=\"pipeline\")\n</code></pre></p> <p>And run it, this time remotely, passing the URL key as a parameter: <pre><code>workflow.run(parameters={\"url\": di.key})\n</code></pre></p> <p>It is possible to monitor the execution in the Core console:</p> <p></p> <p>The next section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store: <pre><code>%%writefile \"src/process-spire.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\n\nKEYS=['codice spira','longitudine','latitudine','Livello','tipologia','codice','codice arco','codice via','Nome via', 'stato','direzione','angolo','geopoint']\n\n@handler(outputs=[\"dataset-spire\"])\ndef process(project, di):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n\n    return sdf\n</code></pre></p> <p>Register the function in Core: <pre><code>process_func = project.new_function(\n                         name=\"process-spire\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"src/process-spire.py\", \"handler\": \"process\"})\n</code></pre></p> <p>Run it locally: <pre><code>process_run = process_func.run(action=\"job\", inputs={'di': dataset_di.key}, outputs={'dataset-spire': 'dataset-spire'}, local_execution=True)\n</code></pre></p> <p>The results has been saved as an artifact in the data store: <pre><code>spire_di = project.get_dataitem(entity_name='dataset-spire')\nspire_df = spire_di.as_df()\n</code></pre></p> <p>Now you can view the results with <code>spire_df.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot: <pre><code>keys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = dataset_df[columns]\n</code></pre></p> <p>Derive dataset for recorded traffic within each time slot for each spire: <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre></p> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function: <pre><code>%%writefile \"src/process-measures.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@handler(outputs=[\"dataset-measures\"])\ndef process(project, di):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre></p> <p>Register it: <pre><code>process_measures_func = project.new_function(\n                         name=\"process-measures\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"src/process-measures.py\", \"handler\": \"process\"})\n</code></pre></p> <p>Run it locally: <pre><code>process_measures_run = process_measures_func.run(action=\"job\", inputs={'di': dataset_di.key}, outputs={'dataset-measures': 'dataset-measures'}, local_execution=True)\n</code></pre></p> <p>Inspect the resulting data artifact: <pre><code>measures_di = project.get_dataitem(entity_name='dataset-measures')\nmeasures_df = measures_di.as_df()\nmeasures_df.head()\n</code></pre></p> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/etl/streamlit/","title":"Visualize data with Streamlit","text":"<p>We can take this one step further and visualize our data in a graph using Streamlit, a library to create web apps and visualize data by writing simple scripts. Let's get familiar with it.</p>"},{"location":"scenarios/etl/streamlit/#setup","title":"Setup","text":"<p>From the Jupyter notebook you've been using, write the result of the API call to a file:</p> <pre><code>with open(\"result.json\", \"w\") as file:\n    file.write(res['data'])\n</code></pre> <p>Create the script that Streamlit will run:</p> <pre><code>%%writefile 'streamlit-app.py'\n\nimport pandas as pd\nimport streamlit as st\n\nrdf = pd.read_json(\"result.json\", orient=\"records\")\n\n# Replace colons in column names as they can cause issues with Streamlit\nrdf.columns = rdf.columns.str.replace(\":\", \"\")\n\nst.write(\"\"\"My data\"\"\")\nst.line_chart(rdf, x=\"codice spira\", y=\"1200-1300\")\n</code></pre>"},{"location":"scenarios/etl/streamlit/#launch-app","title":"Launch app","text":"<p>In a new code cell, run the following to install Streamlit in the workspace. It's actually not code: the <code>!</code> at the beginning tells Jupyter to run the contents as a shell command.</p> <pre><code>!pip install streamlit\n</code></pre> <p>Similarly, run the following command. This will start hosting the Streamlit web app, so the cell will remain running. The <code>browser.gatherUsageStats</code> flag is set to <code>false</code> because, otherwise, Streamlit will automatically gather usage stats and print a warning about it. <pre><code>!streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre></p> <p>Next, go to your Coder instance and access the Jupyter workspace you've been using.</p> <p></p> <p></p> <p>Click on Ports, type <code>8501</code> (Streamlit's default port), then click the button next to it. It will open a tab to the Streamlit app, where you can visualize data!</p> <p></p> <p>The graph we displayed is very simple, but you are welcome to experiment with more Streamlit features. Don't forget to stop the above code cell, to stop the app.</p> <p>Connect to workspace remotely</p> <p>Alternatively to running shell commands from Jupyter and port-forwarding through the Coder interface, you could connect your local shell to the workspace remotely. You do not need to do this if you already used the method above.</p> <p>Login to Coder with the following command. A tab will open in your browser, containing a token you must copy and paste to the shell (it may ask for your credentials, if your browser isn't already logged in).</p> <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> <p>With this, your shell is authenticated to the Coder instance, and the following command will be able to connect your shell to the workspace remotely, while tunneling port 8501:</p> <pre><code>ssh -L 8501:localhost:8501 coder.my-jupyter-workspace\n</code></pre> <p>Install streamlit:</p> <pre><code>pip install streamlit\n</code></pre> <p>Run the app:</p> <pre><code>streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Access <code>localhost:8501</code> on your browser to view the app!</p>"},{"location":"scenarios/etl/streamlit/#as-docker-container","title":"As Docker container","text":"<p>Streamlit apps can be run as Docker containers. For this section, we will run the same application locally as a container, so you will need either Docker or Podman installed on your machine. Instructions refer to Docker, but if you prefer to use Podman, commands are equivalent: simply replace instances of <code>docker</code> with <code>podman</code>.</p> <p>Download the <code>result.json</code> file obtained previously on your machine, as we will need its data for the app. Also download the <code>streamlit-app.py</code> file.</p> <p>Create a file named <code>Dockerfile</code> and paste the following contents in it: <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY streamlit-app.py streamlit-app.py\nCOPY result.json result.json\n\nRUN pip3 install altair pandas streamlit\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit-app.py\", \"--browser.gatherUsageStats=false\"]\n</code></pre></p> <p>The Dockerfile describes how the image for the container will be built. In short, it installs the required libraries, copies the files you downloaded, then launches the Streamlit script.</p> <p>Make sure the three files are in the same directory, then open a shell in it and run the following, which builds the Docker image: <pre><code>docker build -t streamlit .\n</code></pre></p> <p>Once it's finished, you can verify the image exists with: <pre><code>docker images\n</code></pre></p> <p>Now, run a container: <pre><code>docker run -p 8501:8501 --name streamlit-app streamlit\n</code></pre></p> <p>Port already in use</p> <p>If you run into an error, it's likely that you didn't quit the remote session you opened while following the previous section, meaning port 8501 is already in use.</p> <p>Open your browser and visit <code>localhost:8501</code> to view the data!</p> <p>To stop the container, simply press Ctrl+C, then run the following to remove the container: <pre><code>docker rm -f streamlit-app\n</code></pre></p>"},{"location":"scenarios/etl-core/scenario/","title":"ETL with digitalhub-core and DBT scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding organizations, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>digitalhub-core</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, is available in the <code>documentation/examples/etl-core</code> path within the repository of this documentation, or in the path <code>tutorials/08-dbt-demo.ipynb</code> of the Jupyter instance.</p>"},{"location":"scenarios/etl-core/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-dbt\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl-core/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The DBT runtime will use the dataitem specifications to fetch the data and perform the <code>transform</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/etl-core/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a tranformation on data with DBT. Our function will be an SQL query that selects all the employees of department 60.</p> <pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '60'\n\"\"\"\n</code></pre> <p>We create the function from the project object:</p> <pre><code>function = project.new_function(name=\"function-dbt\",\n                                kind=\"dbt\",\n                                source={\"code\": sql})\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>dbt</code>.</li> <li><code>source</code> contains the code that is the SQL we'll execute in the function. Must have key <code>code</code> and value the SQL code.query that</li> </ul>"},{"location":"scenarios/etl-core/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>transform</code>)</li> <li>the inputs map the refereced table in the DBT query (<code>{{ ref('employees') }}</code>) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be <code>department-60</code> and will be the sql query table name result and the output dataitem name.</li> </ul> <pre><code>run = function.run(\"transform\",\n                   inputs={\"employees\": di.key},\n                   outputs={\"output_table\": \"department-60\"})\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling <code>run.refresh()</code> will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/etl-core/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. We can fetch the output table and explore it with <code>pandas</code>.</p> <pre><code>df = run.outputs()[0]['department-60'].as_df()\ndf.head()\n</code></pre>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function, providing the model reference and then deploying.</p> <p>Create a model serving function and provide the model: <pre><code>serving_fn = mlrun.new_function(\"serving\", image=\"mlrun/mlrun\", kind=\"serving\")\nserving_fn.add_model('cancer-classifier',model_path=trainer_run.outputs[\"model\"], class_name='mlrun.frameworks.sklearn.SklearnModelServer')\n</code></pre></p> <p>Deploy (it may take several minutes): <pre><code>project.deploy_function(serving_fn)\n</code></pre></p> <p>You can now test the endpoint: <pre><code>my_data = {\"inputs\"\n           :[[\n               1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n               9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n               3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n               1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n               1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n            ]\n}\nserving_fn.invoke(\"/v2/models/cancer-classifier/infer\", body=my_data)\n</code></pre></p>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>To make the API accessible from outside, we'll need to create an API gateway in Nuclio.</p> <p>Go to your Coder instance, go to the dashboard and access Nuclio. You will notice a <code>demo-ml</code> project, which we created earlier. When you access it, you will see the <code>demo-ml-serving</code> function listed, but click on the API GATEWAYS tab on top instead. Then, click on NEW API GATEWAY.</p> <p>On the left, if you wish, set Authentication to Basic and choose Username and Password.</p> <p>In the middle, set any Name you want. Host must use the same domain as the other components of the platform. For example, if you access Coder at <code>coder.my-digitalhub-instance.it</code>, the Host field should use a value like <code>demo-ml.my-digitalhub-instance.it</code>.</p> <p>On the right, under Primary, you must enter the name of the function, in this case <code>demo-ml-serving</code>.</p> <p>Save and, after a few moments, you will be able to call the API at the address you entered in Host! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"ML scenario introduction","text":"<p>This is a scenario that comes as an official tutorial of MLRun. In fact, its related notebook can be found in your Jupyter instance: <code>/tutorial/01-mlrun-basics.ipynb</code>. However, we skip a number of cells to keep it concise and to the point, while preserving the same functionality.</p> <p>To run this notebook, use the <code>Python 3 (ipykernel)</code> kernel. To do this, select <code>kernel</code> in the top bar and <code>change kernel</code> in the dropdown menu. A new window will open, where you can select the desired kernel.</p> <p>The resulting edited notebook, as well as a file for the function we will create, are available in the <code>documentation/examples/ml</code> path within the repository of this documentation.</p> <p>We will prepare data, train a model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p>"},{"location":"scenarios/ml/intro/#set-up","title":"Set-up","text":"<p>Let's initialize our working environment. Import required libraries: <pre><code>import mlrun\nimport os\n</code></pre></p> <p>Load environment variables for MLRun: <pre><code>ENV_FILE = \".mlrun.env\"\nif os.path.exists(ENV_FILE):\n    mlrun.set_env_from_file(ENV_FILE)\n</code></pre></p> <p>Create a MLRun project: <pre><code>PROJECT = \"demo-ml\"\nproject = mlrun.get_or_create_project(PROJECT, \"./\")\n</code></pre></p>"},{"location":"scenarios/ml/intro/#generate-data","title":"Generate data","text":"<p>Define the following function, which generates the dataset as required by the model: <pre><code>%%writefile data-prep.py\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nimport mlrun\n\n\n@mlrun.handler(outputs=[\"dataset\", \"label_column\"])\ndef breast_cancer_generator():\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(\n        data=breast_cancer.data, columns=breast_cancer.feature_names\n    )\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\n    breast_cancer_dataset = pd.concat(\n        [breast_cancer_dataset, breast_cancer_labels], axis=1\n    )\n\n    return breast_cancer_dataset, \"label\"\n</code></pre></p> <p>Register it: <pre><code>data_gen_fn = project.set_function(\"data-prep.py\", name=\"data-prep\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"breast_cancer_generator\")\nproject.save()\n</code></pre></p> <p>Run it locally: <pre><code>gen_data_run = project.run_function(\"data-prep\", local=True)\n</code></pre></p> <p>You can view the state of the execution with <code>gen_data_run.state()</code> or its output with <code>gen_data_run.outputs</code>. You can see a few records from the output artifact: <pre><code>gen_data_run.artifact(\"dataset\").as_df().head()\n</code></pre></p> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/ml/training/","title":"Training the model","text":"<p>MLRun integrates a set of pre-configured, pre-made functions which support both training and evaluation phases for several frameworks:</p> <ul> <li>SciKit-Learn</li> <li>TensorFlow (and Keras)</li> <li>PyTorch</li> <li>XGBoost</li> <li>LightGBM</li> <li>ONNX</li> </ul> <p>MLRun's auto-trainer can train and evaluate models for supported frameworks, in a fully autonomous and automated way.</p> <p>Import the auto-trainer: <pre><code>trainer = mlrun.import_function('hub://auto_trainer')\n</code></pre></p> <p>Run it on the cluster (it may take a few minutes): <pre><code>trainer_run = project.run_function(trainer,\n    inputs={\"dataset\": gen_data_run.outputs[\"dataset\"]},\n    params = {\n        \"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n        \"train_test_split_size\": 0.2,\n        \"label_columns\": \"label\",\n        \"model_name\": 'cancer',\n    }, \n    handler='train',\n)\n</code></pre></p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/postgrest/data/","title":"Insert data into the database","text":"<p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We will now insert some data into the database we created earlier. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file is available in the <code>documentation/examples/postgrest</code> path within the repository of this documentation.</p> <p>Import required libraries: <pre><code>import os\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport requests\n</code></pre></p> <p>Connect to the database. You will need the value of POSTGRES_URL you got from the owner's secret in the first stage of the scenario. <pre><code>engine = create_engine('postgresql://owner-UrN9ct:88aX8tLFJ95qYU7@database-postgres-cluster/mydb')\n</code></pre></p> <p>Download a CSV file and parse it (may take a few minutes): <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n\nwith requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n\ndf = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>The following will create a table and insert the dataframe into it. If it fails, resources allocated to the Jupyter workspace may be insufficient. The table will be created automatically, or replaced if it already exists. <pre><code>df.to_sql(\"readings\", engine, if_exists=\"replace\")\n</code></pre></p> <p>Run a test select query to check data has been successfully inserted: <pre><code>select = \"SELECT * FROM readings LIMIT 3\"\nselect_df = pd.read_sql(select,con=engine)\nselect_df.head()\n</code></pre></p> <p>If everything went right, a few rows are returned. We will now create a PostgREST service to expose this data via a REST API.</p>"},{"location":"scenarios/postgrest/intro/","title":"PostgREST scenario introduction","text":"<p>In this scenario, we download some data into a Postgres database, then use PostgREST - a tool to make Postgres tables accessible via REST API - to expose this data and run a simple request to view the results.</p>"},{"location":"scenarios/postgrest/intro/#database-set-up","title":"Database set-up","text":"<p>Let's start by setting up the database. Access your KRM instance and Postgres DBs on the left menu, then click Create.</p> <ul> <li><code>Name</code>: This is just an identifier for Kubernetes. Type <code>my-db</code>.</li> <li><code>Database</code>: The actual name on the database. Type <code>mydb</code>.</li> <li>Toggle on <code>Drop on delete</code>, which conveniently deletes the database when you delete the custom resource.</li> </ul> <p></p> <p>Click Save. You should now see your database listed.</p>"},{"location":"scenarios/postgrest/intro/#add-users-to-database","title":"Add users to database","text":"<p>Click Show on your database's entry and then Add user on the bottom. We will create two users: PostgREST needs one to authenticate and another to assume its role when the API is called.</p> <p>Create the first one as follows:</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-owner</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>owner</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Owner</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>owner</code>.</li> </ul> <p></p> <p>Add a second user. This one will only be able to read data.</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-reader</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>reader</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Reader</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>reader</code>.</li> </ul>"},{"location":"scenarios/postgrest/intro/#retrieve-postgres_url","title":"Retrieve POSTGRES_URL","text":"<p>Together with the users, two secrets have also been created. Go to Secrets on the left menu; the list should contain two secrets with names referring the users you created.</p> <p>Look for the owner's secret, click Show and then Decode on the <code>POSTGRES_URL</code> entry. It will automatically copy the connection string to the clipboard. Write this down somewhere, as we will use it to log into the database and insert some data.</p> <p></p>"},{"location":"scenarios/postgrest/postgrest/","title":"Expose data with PostgREST","text":"<p>We will go back to KRM to create a PostgREST service and expose the database's table via API.</p>"},{"location":"scenarios/postgrest/postgrest/#inspect-users-secrets","title":"Inspect users' secrets","text":"<p>There are a number of parameters we need to configure PostgREST. Similarly to what you did in the first stage of the scenario, go to Secrets on the left and look for the two secrets, belonging to the owner and reader users you created.</p> <p>You will need the following properties, so write them down somewhere for convenience:</p> <p>For the owner:</p> <ul> <li>The Name of the secret.</li> <li>DATABASE_NAME</li> <li>HOST</li> </ul> <p>For the reader:</p> <ul> <li>ROLE</li> </ul>"},{"location":"scenarios/postgrest/postgrest/#creating-the-postgrest-service","title":"Creating the PostgREST service","text":"<p>Click PostgREST Data Services on the left and then Create.</p> <p>Fill the first few fields as follows:</p> <ul> <li><code>Name</code>: Anything you'd like, it's once again an identifier for Kubernetes.</li> <li><code>Schema</code>: <code>public</code></li> <li>Toggle on <code>With existing DB user</code>.</li> <li><code>Existing DB user name</code>: Value of the reader's secret ROLE.</li> </ul> <p>Under Connection, fill the fields as follows:</p> <ul> <li><code>DB Host</code>: Value of the owner's secret HOST.</li> <li><code>DB Port</code>: <code>5432</code>. In case the host's value contains the port, remove it from there (delete the <code>:</code> symbol) and put it here.</li> <li><code>Database name</code>: Value of the owner's secret DATABASE_NAME.</li> <li>Toggle on <code>With existing secret</code>.</li> <li><code>Secret name</code>: The Name of the owner's secret.</li> </ul> <p>When you hit Save, the PostgREST instance will be launched!</p> <p></p>"},{"location":"scenarios/validation/scenario/","title":"Validation with digitalhub-core and Nefertem scenario introduction","text":"<p>This scenario demonstrates how to use digitalhub-core and Nefertem to validate data.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>digitalhub-core</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, are available in the <code>documentation/examples/validation</code>path within the repository of this documentation.</p>"},{"location":"scenarios/validation/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-nefertem\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/validation/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The Nefertem runtime will use the dataitem specifications to fetch the data and perform the <code>validate</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/validation/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a validation task on the dataitem. First we define the constraint that we want to validate. A constaint is a rule that we wanto to check against the data. In this case, we want to check that the <code>SALARY</code> column is of type <code>int</code>. We define the constraint as a dictionary:</p> <pre><code>constraint = {\n  'constraint': 'type',\n  'field': 'SALARY',\n  'field_type': 'number',\n  'name': 'check_value_integer',\n  'title': '',\n  'resources': ['employees'],\n  'type': 'frictionless',\n  'value': 'number',\n  'weight': 5\n}\n</code></pre> <p>With the constraint defined, we can now create the function from the project object. We pass the following parameters:</p> <pre><code>function = project.new_function(name=\"function-nefertem\",\n                                kind=\"nefertem\",\n                                constraints=[constraint])\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>nefertem</code>.</li> <li><code>constraints</code> is the list of constraints that we want to validate.</li> </ul>"},{"location":"scenarios/validation/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>validate</code>)</li> <li>framework we want to use (in this case, <code>frictionless</code>)</li> <li>the inputs map the resource defined in the constraint with our dataitem key. The runtime will collect the data referenced in the dataitem path an treat that data as Nefertem <code>DataResource</code>.</li> </ul> <pre><code>run = function.run(\"validate\",\n                   framework=\"frictionless\",\n                   inputs={\"employees\": di.key})\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERRROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling run.refresh() will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/validation/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. A Neferetem run produces various artifacts, like reports produced by Nefertem and the framework used for validation (in our case, a Frictionless report). We can get the artifact list from the run:</p> <pre><code>artifacts = run.outputs()\n\n\nprint(artifacts)\n</code></pre> <p>If we want to get the report, we can get it from the artifact and read it:</p> <pre><code>path = artifacts[0][\"run-metadata\"].download()\n\nwith open(path) as f:\n    print(f.read())\n</code></pre>"},{"location":"tasks/data/","title":"Data and transformations","text":"<p>The platform supports data of different types to be stored and operated by the underlying storage subsystems.</p> <p>Specifically, the platform natively supports two types of storages:</p> <ul> <li>persistence object storage (datalake S3 Minio), which manages immutable data in the form of files.</li> <li>operational relational data storage (PostgreSQL database), which is used for efficient querying of mutable data. Postgres is rich with extensions, most notably for geo-spatial and time-series data.</li> </ul> <p>The data is represented in the platform as entities of different types, depending on its usage and format. More specifically, we distinguish:</p> <ul> <li>data items, which represent immutable data sets resulting from different transformation operations and are ready for use in differerent types of analysis. Data items are enriched with metadata (versions, lineage, stats, profiling, schema, ...) and unique keys and managed and persisted to the datalake directly by the platform in the form of Parquet files. It is possible to treat tabular data (items of <code>table</code> kind) as, for example, DataFrames, using conventional libraries.</li> <li>artifacts, which represent arbitrary files, not limited to tabular format, stored to the datalake with some extra metadata.</li> </ul> <p>Each data entity may be accessed and manipulated by the platform via UI or using the API, for example with SDK.</p>"},{"location":"tasks/data/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/data/#artifacts","title":"Artifacts","text":"<p>Artifacts can be managed as entities from the UI. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new artifact</li> <li><code>filter</code> artifacts by name and kind</li> <li><code>expand</code> an artifact to see its 5 latest versions</li> <li><code>show</code> the details of an artifact</li> <li><code>edit</code> an artifact</li> <li><code>delete</code> an artifact</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete artifacts using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the artifact</li> <li><code>Kind</code>: kind of the artifact</li> <li>(Spec) <code>Path</code>: remote path where the artifact is stored. If you instead upload the artifact at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later.</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description of the artifact</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the artifact</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path to the artifact, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view an artifact's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update","title":"Update","text":"<p>You can update an artifact by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete","title":"Delete","text":"<p>You can delete an artifact from either its detail page or the list of artifacts, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#data-items","title":"Data items","text":"<p>Data items can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new data item</li> <li><code>expand</code> a data item and see its 5 latest versions</li> <li><code>show</code> the details of a data item</li> <li><code>edit</code> a data item</li> <li><code>delete</code> a data item</li> <li><code>filter</code> data items by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete data items using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create_1","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name of the dataitem</li> <li><code>Kind</code>: kind of the dataitem</li> <li>(Spec) <code>Path</code>: remote path where the data item is stored. If you instead upload the data item at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later:</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the data item</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path of the data item, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#kind","title":"Kind","text":"<p>There are 2 possible kinds for dataitems:</p> <ul> <li><code>Dataitem</code>: indicates it is a generic data item. There are no specific attributes in the creation page.</li> <li><code>table</code>: indicates that the data item points to a table. The optional parameter is the schema of the table in table_schema format.</li> </ul>"},{"location":"tasks/data/#read_1","title":"Read","text":"<p>Click <code>SHOW</code> to view a data item's details.</p> <p></p> <p>Based on the <code>kind</code>, there may be a <code>schema</code>, indicating that the dataitem point to a table.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update_1","title":"Update","text":"<p>You can update a data item by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete_1","title":"Delete","text":"<p>You can delete a data item from either its detail page or the list of data items, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#management-via-sdk","title":"Management via SDK","text":""},{"location":"tasks/data/#artifacts_1","title":"Artifacts","text":"<p>Artifacts (ARTIFACT) are (binary) objects stored in one of the artifact stores of the platform, and available to every process, module and component as files (or data streams). Artifacts can be created and managed as entities with the SDK CRUD methods. This can be done directly from the package or through the <code>Project</code> object. To manage artifacts, you need to have the <code>digitalhub_core</code> layer installed.</p> <p>In the first section, we will see how to create, read, update and delete artifacts. In the second section, we will see what can be done with the <code>Artifact</code> object.</p>"},{"location":"tasks/data/#artifact-management-via-sdk","title":"Artifact management via SDK","text":"<p>An <code>artifact</code> can be managed with the following methods.</p> <ul> <li><code>new_artifact</code>: create a new artifact</li> <li><code>get_artifact</code>: get an artifact</li> <li><code>update_artifact</code>: update an artifact</li> <li><code>delete_artifact</code>: delete an artifact</li> <li><code>list_artifacts</code>: list all artifacts</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Artifact</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\nartifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\n## From project\nartifact = project.new_artifact(name=\"my-artifact\",\n                                kind=\"artifact\",\n                                path=\"s3://my-bucket/my-artifact.ext\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete an artifact, focusing on managing artifacts through the library. If you want to manage artifacts from the project, you can use the <code>Project</code> object and avoid having to specify the <code>project</code> parameter.</p>"},{"location":"tasks/data/#create_2","title":"Create","text":"<p>To create an artifact you can use the <code>new_artifact()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> <li><code>name</code>: the name of the artifact</li> <li><code>kind</code>: the kind of the artifact</li> <li><code>path</code>: the remote path where the artifact is stored</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>uuid</code>: the uuid of the artifact (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: the description of the artifact</li> <li><code>source</code>: the remote source of the artifact (git repository)</li> <li><code>labels</code>: the labels of the artifact</li> <li><code>embedded</code>: whether the artifact is embedded or not. If <code>True</code>, the artifact is embedded (all the spec details are expressed) in the project. If <code>False</code>, the artifact is not embedded in the project</li> <li><code>src_path</code>: local path of the artifact, used in case of upload into remote storage</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n</code></pre>"},{"location":"tasks/data/#read_2","title":"Read","text":"<p>To read an artifact you can use the <code>get_artifact()</code> or <code>import_artifact()</code> methods. The first one searches for the artifact into the backend, the second one loads it from a local yaml.</p>"},{"location":"tasks/data/#get","title":"Get","text":"<p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the artifact as identifier. It returns the latest version of the artifact</li> <li><code>entity_id</code>: to use the uuid of the artifact as identifier. It returns the specified version of the artifact</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.get_artifact(project=\"my-project\",\n                           entity_name=\"my-artifact\")\n\nartifact = dh.get_artifact(project=\"my-project\",\n                           entity_id=\"uuid-of-my-artifact\")\n</code></pre>"},{"location":"tasks/data/#import","title":"Import","text":"<p>Mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the artifact yaml</li> </ul> <p>Example:</p> <pre><code>artifact = dh.import_artifact(file=\"./some-path/my-artifact.yaml\")\n</code></pre>"},{"location":"tasks/data/#update_2","title":"Update","text":"<p>To update an artifact you can use the <code>update_artifact()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>artifact</code>: artifact object to be updated</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\nartifact.metadata.description = \"My new description\"\n\nartifact = dh.update_artifact(artifact=artifact)\n</code></pre>"},{"location":"tasks/data/#delete_2","title":"Delete","text":"<p>To delete an artifact you can use the <code>delete_artifact()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the artifact exists</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the artifact as identifier</li> <li><code>entity_id</code>: to use the uuid of the artifact as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the artifact will be deleted. Mutually exclusive with the <code>entity_id</code> parameter.</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>artifact = dh.new_artifact(project=\"my-project\",\n                           name=\"my-artifact\",\n                           kind=\"artifact\",\n                           path=\"s3://my-bucket/my-artifact.ext\")\n\ndh.delete_artifact(project=\"my-project\",\n                   entity_id=artifact.id)\n</code></pre>"},{"location":"tasks/data/#list","title":"List","text":"<p>To list all artifacts you can use the <code>list_artifacts()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project containing the artifacts</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>artifacts = dh.list_artifacts(project=\"my-project\")\n</code></pre>"},{"location":"tasks/data/#artifact-object","title":"Artifact object","text":"<p>The <code>Artifact</code> object is built using the <code>new_artifact()</code> method. There are several variations of the <code>Artifact</code> object based on the <code>kind</code> of the artifact. The SDK supports the following kinds:</p> <ul> <li><code>artifact</code>: represents a generic artifact</li> </ul> <p>For each different kind, the <code>Artifact</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>. All the <code>Artifact</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity artifact into backend or locally as yaml.</p> <p>To create a specific artifact, you must use the desired <code>kind</code> in the <code>new_artifact()</code> method.</p>"},{"location":"tasks/data/#kind-artifact","title":"Kind: artifact","text":"<p>The <code>artifact</code> kind indicates that the artifact is a generic artifact. There are no specific <code>spec</code> parameters.</p> <p>The <code>artifact</code> kind has the following methods:</p> <ul> <li><code>as_file()</code>: collects the artifact into a local temporary file</li> <li><code>download()</code>: downloads the artifact into a specified path</li> <li><code>upload()</code>: uploads the artifact to a specified path</li> </ul>"},{"location":"tasks/data/#as-file","title":"As file","text":"<p>The <code>as_file()</code> method returns the artifact as a temporary file. The file is not automatically deleted when the program ends. The method returns the path of the downloaded artifact.</p>"},{"location":"tasks/data/#download","title":"Download","text":"<p>The <code>download()</code> method downloads the artifact into a specified path. The method returns the path of the downloaded artifact. The method accepts the following parameters:</p> <ul> <li><code>target</code>: remote path of the artifact to be downloaded (eg. <code>s3://my-bucket/my-artifact.ext</code>). By default, uses the <code>spec</code> <code>path</code>.</li> <li><code>dst</code>: local path where the artifact will be downloaded. By default, it is in the current working directory</li> <li><code>overwrite</code>: if <code>True</code>, the target path will be overwritten if it already exists</li> </ul>"},{"location":"tasks/data/#upload","title":"Upload","text":"<p>The <code>upload()</code> method uploads the artifact to a specified path. The method returns the path of the uploaded artifact. The method accepts the following parameters:</p> <ul> <li><code>source</code>: local path of the artifact to be uploaded</li> <li><code>target</code>: remote path of the artifact to be uploaded (eg. <code>s3://my-bucket/my-artifact.ext</code>). By default, uses the <code>spec</code> <code>path</code>.</li> </ul>"},{"location":"tasks/data/#data-items_1","title":"Data items","text":"<p>Data items (DATAITEM) are data objects which contain a dataset of a given type, stored in an addressable repository and accessible to every component able to understand the type (kind) and the source (path). Do note that data items could be stored in the artifact store as artifacts, but that is not a dependency or a requirement. Dataitems can be created and managed as entities with the SDK CRUD methods. This can be done directly from the package or through the <code>Project</code> object. To manage dataitems, you need to have the <code>digitalhub_data</code> layer installed.</p> <p>In the first section, we will see how to create, read, update and delete dataitems. In the second section, we will see what can be done with the <code>Dataitem</code> object.</p>"},{"location":"tasks/data/#data-item-management-via-sdk","title":"Data item management via SDK","text":"<p>A <code>data item</code> can be managed with the following methods.</p> <ul> <li><code>new_dataitem</code>: create a new data item</li> <li><code>get_dataitem</code>: get a data item</li> <li><code>update_dataitem</code>: update a data item</li> <li><code>delete_dataitem</code>: delete a data item</li> <li><code>list_dataitems</code>: list all data items</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Dataitem</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\ndataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\n## From project\ndataitem = project.new_dataitem(name=\"my-dataitem\",\n                                kind=\"dataitem\",\n                                path=\"s3://my-bucket/my-dataitem.ext\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete a data item, focusing on managing data items through the library. If you want to manage data items from the project, you can use the <code>Project</code> object and avoid having to specify the <code>project</code> parameter.</p>"},{"location":"tasks/data/#create_3","title":"Create","text":"<p>To create a data item you can use the <code>new_dataitem()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: project in which the data item will be created</li> <li><code>name</code>: name of the data item</li> <li><code>kind</code>: kind of the data item</li> <li><code>path</code>: remote path where the data item is stored</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>uuid</code>: uuid of the data item (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: description of the data item</li> <li><code>source</code>: remote source of the data item (git repository)</li> <li><code>labels</code>: list of labels</li> <li><code>embedded</code>: whether the data item is embedded or not. If <code>True</code>, the data item is embedded (all the spec details are expressed) in the project. If <code>False</code>, the data item is not embedded in the project.</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n</code></pre>"},{"location":"tasks/data/#read_3","title":"Read","text":"<p>To read a data item you can use the <code>get_dataitem()</code> or <code>import_dataitem()</code> methods. The first one searches for the data item into the backend, the second one load it from a local yaml.</p>"},{"location":"tasks/data/#get_1","title":"Get","text":"<p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the data item will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the data item as identifier. It returns the latest version of the data item.</li> <li><code>entity_id</code>: to use the uuid of the data item as identifier. It returns the specified version of the data item.</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.get_dataitem(project=\"my-project\",\n                           entity_name=\"my-dataitem\")\n\ndataitem = dh.get_dataitem(project=\"my-project\",\n                           entity_id=\"uuid-of-my-dataitem\")\n</code></pre>"},{"location":"tasks/data/#import_1","title":"Import","text":"<p>Mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the dataitem yaml</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.import_dataitem(file=\"./some-path/my-dataitem.yaml\")\n</code></pre>"},{"location":"tasks/data/#update_3","title":"Update","text":"<p>To update a data item you can use the <code>update_dataitem()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>dataitem</code>: data item object to be updated</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\ndataitem.metadata.description = \"My new description\"\n\ndataitem = dh.update_dataitem(dataitem=dataitem)\n</code></pre>"},{"location":"tasks/data/#delete_3","title":"Delete","text":"<p>To delete a data item you can use the <code>delete_dataitem()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the data item exists</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the data item as identifier</li> <li><code>entity_id</code>: to use the uuid of the data item as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the data item will be deleted. Its mutually exclusive with the <code>entity_id</code> parameter</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>dataitem = dh.new_dataitem(project=\"my-project\",\n                           name=\"my-dataitem\",\n                           kind=\"dataitem\",\n                           path=\"s3://my-bucket/my-dataitem.ext\")\n\ndh.delete_dataitem(project=\"my-project\",\n                   entity_id=dataitem.id)\n</code></pre>"},{"location":"tasks/data/#list_1","title":"List","text":"<p>To list all data items you can use the <code>list_dataitems()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project containing the data items</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>dataitems = dh.list_dataitems(project=\"my-project\")\n</code></pre>"},{"location":"tasks/data/#dataitem-object","title":"Dataitem object","text":"<p>The <code>Dataitem</code> object is built using the <code>new_dataitem()</code> method. There are several variations of the <code>Dataitem</code> object based on the <code>kind</code> of the data item. The SDK supports the following kinds:</p> <ul> <li><code>dataitem</code>: represents a generic data item</li> <li><code>table</code>: represents a table data item</li> </ul> <p>For each different kind, the <code>Dataitem</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>.</p> <p>To create a specific data item, you must use the desired <code>kind</code> in the <code>new_dataitem()</code> method. All the <code>Dataitem</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity data item into backend or locally as yaml.</p>"},{"location":"tasks/data/#kind-dataitem","title":"Kind: dataitem","text":"<p>The <code>dataitem</code> kind indicates that the data item is a generic data item. There are no specific <code>spec</code> parameters nor specific method exposed. It acts as a generic data item.</p>"},{"location":"tasks/data/#kind-table","title":"Kind: table","text":"<p>The <code>table</code> kind indicates that the data item point to a table. The optional <code>spec</code> parameters are:</p> <ul> <li><code>schema</code>: the schema of the table in table_schema format</li> </ul> <p>The <code>table</code> kind also has the following methods:</p> <ul> <li><code>as_df()</code>: to collect the data in a pandas dataframe</li> <li><code>write_df()</code>: to write the data item as parquet</li> </ul>"},{"location":"tasks/data/#read-table","title":"Read table","text":"<p>The <code>as_df()</code> method returns the data in a pandas dataframe. The method accepts the following parameters:</p> <ul> <li><code>format</code>: the format of the data. If not provided, the format will be inferred from the file extension. We support ONLY parquet or csv.</li> <li><code>kwargs</code>: keyword arguments passed to the pandas <code>read_parquet</code> or <code>read_csv</code> method</li> </ul>"},{"location":"tasks/data/#write-table","title":"Write table","text":"<p>The <code>write_df()</code> method writes the data item as parquet. The method accepts the following parameters:</p> <ul> <li><code>target_path</code>: the path of the target parquet file. If not provided, the target path will created by the SDK and the data item will be stored in the default store</li> <li><code>kwargs</code>: keyword arguments passed to the pandas <code>to_parquet</code> method</li> </ul>"},{"location":"tasks/functions/","title":"Functions and Runtimes","text":"<p>Functions are the logical description of something that the platform may execute and track for you. A function may represent code to run as a job, an ML model inference to be used as batch procedure or as a service, a data validation, etc.</p> <p>In the platform we perform actions over functions (also referred to as \"tasks\"), such as job execution, deploy, container image build. A single action execution is called run, and the platform keeps track of these runs, with metadata about function version, operation parameters, and runtime parameters for a single execution. </p> <p>They are associated with a given runtime, which implements the actual execution and determines which actions are available. Examples are DBT, Nefertem, Python, etc. Runtimes  are highly specialized components which can translate the representation of a given execution, as expressed in the run, into an actual execution operation performed via libraries, code, external tools etc.</p> <p>Runtimes define the key point of extension of the platform: new runtimes may be added in order to implement the low-level logic of \"translating\" the high level operation definition into an executable run. For example, DBT runtime allows for defining the transformation as a task that, given the input table reference,  produces a datastt appyling the function defined as SQL code. The runtime in this case is responsible for converting the specification and the references to a dedicated Kubernetes Job that runs DBT transformation and stores the corresponding dataset.</p> <p>The set of the supported runtimes is documented in Runtimes References section. Independently of the specific runtime implementation, the flow of actions  with respect to the function definition and execution is the following:</p> <ul> <li>define a new function providing its name, runtime, definition (e.g., source code), and configuration (e.g., dependencies). The function definition is saved by the project. Each change to the function spec creates a new function version so that the executions of different function versions are independently tracked.</li> <li>execute a task over the function providing the configuration of the task (e.g., the K8S resources needed for the execution), the execution parameters and inputs (if any). This creates a new task specification and a new run instance tracked by the platform. </li> </ul> <p>The definition and execution of the functions may be performed either via UI or via Python SDK.</p>"},{"location":"tasks/functions/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/functions/#functions","title":"Functions","text":"<p>Functions can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new function</li> <li><code>expand</code> a function to see its 5 latest versions</li> <li><code>show</code> the details of a function</li> <li><code>edit</code> a function</li> <li><code>delete</code> a function</li> <li><code>filter</code> functions by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete functions using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/functions/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the function</li> <li><code>Kind</code>: kind of function</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p><code>Spec</code> fields will change depending on the function's kind.</p>"},{"location":"tasks/functions/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a function's details.</p> <p></p> <p>Tabs next to <code>SUMMARY</code> will change depending on the function's <code>kind</code>. Some of them allow you to create runs, but we will see this in a later section.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/functions/#update","title":"Update","text":"<p>You can update a function by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/functions/#delete","title":"Delete","text":"<p>You can delete a function from either its detail page or the list of functions, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/functions/#runs","title":"Runs","text":""},{"location":"tasks/functions/#create_1","title":"Create","text":"<p>A run represents the execution of a task through a function. As such, the starting point to create a run is the function it is based on. Select one of the functions you created. You will notice multiple tabs at the top, next to <code>SUMMARY</code>. These tabs may differ depending on the function's <code>kind</code>.</p> <p></p> <p>Click <code>CREATE</code> to create a new run. You will start a 3-steps process to create a run.</p> <p>The first step will ask for parameters that depend on the function's <code>kind</code> and the task you are creating the run for, but will generally also ask if you wish to configure resources to allocate, environment variables, secrets, volumes and node modules.</p> <p>The second step will ask, if applicable, to specify inputs, outputs and parameters.</p> <p>The third step will simply present a recap.</p> <p></p>"},{"location":"tasks/functions/#view-and-manage","title":"View and manage","text":"<p>By going through a function's tabs, you can access the corresponding runs, but you may also access all runs from the Runs section in the left menu (also available as Jobs and runs in the dashboard).</p> <p>You can filter runs by name, kind and status.</p> <p></p> <p>Click on a run to view its details.</p> <p></p> <p>From here, click on <code>LOGS</code> to view its logs.</p> <p></p>"},{"location":"tasks/functions/#management-via-sdk","title":"Management via SDK","text":"<p>In the following sections, we will see how to create, read, update and delete functions and what can be done with the <code>Function</code> object through the SDK, including how to create <code>Run</code> objects.</p> <p>You can manage the <code>Function</code> entity with the following methods:</p> <ul> <li><code>new_function</code>: create a new function</li> <li><code>get_function</code>: get a function</li> <li><code>update_function</code>: update a function</li> <li><code>delete_function</code>: delete a function</li> <li><code>list_functions</code>: list all functions</li> </ul> <p>This can be done through the SDK, or through the <code>Project</code> object.</p> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\nfunction = dh.new_function(project=\"my-project\",\n    name=\"my-function\",\n    kind=\"function-kind\",\n    **kwargs)\n\n## From project\nfunction = project.new_function(name=\"my-function\",\n    kind=\"function-kind\",\n    **kwargs)\n</code></pre>"},{"location":"tasks/functions/#create_2","title":"Create","text":"<p>To create a function you can use the <code>new_function()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> <li><code>name</code>: name of the function</li> <li><code>kind</code>: kind of the function</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>uuid</code>: the uuid of the function (this is automatically generated if not provided). If provided, must be a valid uuid v4.</li> <li><code>description</code>: description of the function</li> <li><code>labels</code>: labels for the function</li> <li><code>git_source</code>: remote source of the function</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>function = dh.new_function(project=\"my-project\",\n                           name=\"my-function\",\n                           kind=\"function-kind\",\n                           **kwargs)\n</code></pre>"},{"location":"tasks/functions/#read_1","title":"Read","text":"<p>To read a function you can use the <code>get_function()</code> or <code>import_function()</code> methods. The first one searches for the function in the back-end, the second one loads it from a local yaml file.</p>"},{"location":"tasks/functions/#get","title":"Get","text":"<p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the function as identifier. It returns the latest version of the function.</li> <li><code>entity_id</code>: to use the uuid of the function as identifier. It returns the specified version of the function.</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the back-end</li> </ul> <p>Examples:</p> <pre><code>function = dh.get_function(project=\"my-project\",\n                           entity_name=\"my-function\")\n\nfunction = dh.get_function(project=\"my-project\",\n                           entity_id=\"uuid-of-my-function\")\n</code></pre>"},{"location":"tasks/functions/#import","title":"Import","text":"<p>Mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the function yaml</li> </ul> <p>Example:</p> <pre><code>function = dh.import_function(file=\"my-function.yaml\")\n</code></pre>"},{"location":"tasks/functions/#update_1","title":"Update","text":"<p>To update a function, use the <code>update_function()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>function</code>: the function object to update</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the back-end</li> </ul> <p>Example:</p> <pre><code>function = dh.update_function(function=function,\n                              **kwargs)\n</code></pre>"},{"location":"tasks/functions/#delete_1","title":"Delete","text":"<p>To delete a function, use the <code>delete_function()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the function as identifier</li> <li><code>entity_id</code>: to use the uuid of the function as identifier. Mutually exclusive with <code>delete_all_versions</code>.</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the function will be deleted. Mutually exclusive with <code>entity_id</code>.</li> <li><code>cascade</code>: if <code>True</code>, all <code>Task</code> and <code>Run</code> objects associated with the function will be deleted</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the back-end</li> </ul> <p>Example:</p> <pre><code>function = dh.delete_function(project=\"my-project\",\n                              entity_name=\"my-function\")\n</code></pre>"},{"location":"tasks/functions/#list","title":"List","text":"<p>To list all functions, use the <code>list_functions()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the function will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the back-end</li> </ul> <p>Example:</p> <pre><code>functions = dh.list_functions(project=\"my-project\")\n</code></pre>"},{"location":"tasks/functions/#function-object","title":"Function object","text":"<p>The <code>Function</code> object represents an executable function. The object exposes methods for saving and exporting the entity function into backend or locally as yaml and to execute it.</p>"},{"location":"tasks/functions/#save","title":"Save","text":"<p>To save a function in the back-end, use the <code>save()</code> method.</p> <p>The method accepts the following optional parameters:</p> <ul> <li><code>update</code>: a boolean value, if <code>True</code> the function will be updated on the back-end</li> </ul> <p>Example:</p> <pre><code>function.save()\n</code></pre>"},{"location":"tasks/functions/#export","title":"Export","text":"<p>To export a function as yaml, use the <code>export()</code> method.</p> <p>The method accepts the following optional parameters:</p> <ul> <li><code>filename</code>: the name of the file to export</li> </ul> <p>Example:</p> <pre><code>function.export(filename=\"my-function.yaml\")\n</code></pre>"},{"location":"tasks/functions/#run","title":"Run","text":"<p>To run a function, use the <code>run()</code> method. This method is a shortcut for:</p> <ul> <li>creating a <code>Task</code> object</li> <li>creating a <code>Run</code> object</li> <li>executing the <code>Run</code> object</li> </ul> <p>The method accepts the following mandatory parameters:</p> <ul> <li><code>action</code>: the action to be executed. Possible values for this parameter depend on the <code>kind</code> of the function. See the runtimes section for more information.</li> </ul> <p>The optional task parameters are as follows. For Kubernetes:</p> <ul> <li><code>node_selector</code>: a list of node selectors. The runtime will select the nodes to which the task will be scheduled.</li> <li><code>volumes</code>: a list of volumes</li> <li><code>resources</code>: a map of resources (CPU, memory, GPU)</li> <li><code>affinity</code>: node affinity</li> <li><code>tolerations</code>: tolerations</li> <li><code>env</code>: environment variables to inject into the container</li> <li><code>secrets</code>: list of secrets to inject into the container</li> <li><code>backoff_limit</code>: number of retries when a job fails.</li> <li><code>schedule</code>: schedule of the job as a cron expression</li> <li><code>replicas</code>: number of replicas of the deployment</li> </ul> <p>For runtime-specific task parameters, see the runtime documentation.</p> <p>The optional run parameters are:</p> <ul> <li><code>inputs</code>: a map of inputs</li> <li><code>outputs</code>: a map of outputs</li> <li><code>parameters</code>: a map of parameters</li> <li><code>values</code>: a list of values</li> <li><code>local_execution</code>: if <code>True</code>, the function will be executed locally</li> </ul> <p>Example:</p> <pre><code>run = function.run(\n    action=\"job\",\n    inputs={\"input-param\": \"input-value\"},\n    outputs={\"output-param\": \"output-value\"}\n)\n</code></pre>"},{"location":"tasks/kubernetes-resources/","title":"Using Kubernetes Resources for Runs","text":"<p>With SDK you can manage Kubernetes resources for your tasks. When you run a function you can require some Kubernetes resources for the task. Resources and data are specified in the <code>function.run()</code> method.</p> Name Type Description Default node_selector list[dict] Node selector None volumes list[dict] List of volumes None resources dict Resources restrictions None affinity dict Affinity None tolerations list[dict] Tolerations None envs list[dict] Env variables None secrets list[str] List of secret names None"},{"location":"tasks/kubernetes-resources/#node_selector","title":"Node_selector","text":"<p>You can request a node selector for the container being launched by the task by passing the selector as a dictionary with the <code>node_selector</code> task parameters.</p> <pre><code>node_selector = {\n    \"key\": \"Node selector key.\",\n    \"value\": \"Node selector value.\"\n}\n</code></pre>"},{"location":"tasks/kubernetes-resources/#volumes","title":"Volumes","text":"<p>With SDK you can request three types of volumes:</p> <ul> <li>Persistent volume claims (PVC)</li> <li>ConfigMap</li> <li>Secret</li> </ul>"},{"location":"tasks/kubernetes-resources/#persistent-volume-claims-pvc","title":"Persistent volume claims (PVC)","text":"<p>You can ask for a persistent volume claim (pvc) to be mounted on the container being launched by the task. You need to declare the volume type as <code>persistent_volume_claim</code>, a name for the PVC for the user (e.g., <code>my-pvc</code>), the mount path on the container and a spec with the name of the PVC on Kubernetes (e.g., <code>pvc-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"persistent_volume_claim\",\n        \"name\": \"my-pvc\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"claim_name\": \"pvc-name-on-k8s\",\n            }\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#configmap","title":"ConfigMap","text":"<p>You can ask for a configmap to be mounted on the container being launched by the task. You need to declare the volume type as <code>config_map</code>, a name for the ConfigMap for the user (e.g., <code>my-config-map</code>), the mount path on the container and a spec with the name of the ConfigMap on Kubernetes (e.g., <code>config-map-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"config_map\",\n        \"name\": \"my-config-map\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"name\": \"config-map-name-on-k8s\"\n        }\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#secret","title":"Secret","text":"<p>You can ask for a secret to be mounted on the container being launched by the task. You need to declare the volume type as <code>secret</code>, a name for the Secret for the user (e.g., <code>my-secret</code>), the mount path on the container and a spec with the name of the Secret on Kubernetes (e.g., <code>secret-name-on-k8s</code>).</p> <pre><code>volumes = [{\n        \"volume_type\": \"secret\",\n        \"name\": \"my-secret\",\n        \"mount_path\": \"/data\",\n        \"spec\": {\n            \"secret_name\": \"secret-name-on-k8s\"\n        }\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#resources","title":"Resources","text":"<p>You can request a specific amount of hardware resources (cpu, memory, gpu) for the task, declared thorugh the <code>resources</code> task parameter; <code>resources</code> must be a map of Resource objects represented as a dictionary. At the moment Digitalhub SDK supports:</p> <ul> <li>CPU</li> <li>RAM memory</li> <li>GPU</li> </ul>"},{"location":"tasks/kubernetes-resources/#cpu","title":"CPU","text":"<p>You can request a specific amount of CPU for the task. You need to declare the resource type as <code>cpu</code>, request and/or limit specifications.</p> <pre><code>resources = {\n    \"cpu\": {\n        \"requests\": \"12\",\n        \"limits\": \"16\"\n    }\n}\n</code></pre>"},{"location":"tasks/kubernetes-resources/#ram-memory","title":"RAM memory","text":"<p>You can request a specific amount of RAM memory for the task. You need to declare the resource type as <code>mem</code>, request and/or limit specifications.</p> <pre><code>resources = {\n    \"mem\"{\n        \"requests\": \"64Gi\",\n    }\n}\n</code></pre>"},{"location":"tasks/kubernetes-resources/#gpu","title":"GPU","text":"<p>You can request a specific amount of GPU for the task. You need to declare the resource type as <code>gpu</code>, request and/or limit specifications. There could be administation-specific requirements for requesting a GPU. You may need to use <code>tolerations</code> or <code>affinity</code> parameters to request the GPU. Both of these parameters are described in the Kubernetes documentation. Other times you may need to specify a list of labels with the <code>labels</code> parameter.</p> <p>Here is an example for the digitahub in FBK that uses the <code>tolerations</code> parameter:</p> <pre><code>resources = {\n    \"gpu\": {\n        \"limits\": \"1\"\n    }\n}\ntoleration = [{\n    \"key\": \"nvidia.com/gpu\",\n    \"operator\": \"Equal\",\n    \"value\": \"v100\",\n    \"effect\": \"NoSchedule\"\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#secrets","title":"Secrets","text":"<p>You can request a secret injection into the container being launched by the task by passing the reference to the backend with the <code>secrets</code> task parameters.</p> <pre><code>secrets = [\"my-secret\"]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#envs","title":"Envs","text":"<p>You can request an environment variable injection into the container being launched by the task by passing the reference to the backend with the <code>envs</code> task parameters.</p> <pre><code>env = [{\n    \"name\": \"env-name\",\n    \"value\": \"value\"\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#tolerations","title":"Tolerations","text":"<p>Kubernetes requires you to specify tolerations if you want to use GPU.</p> <pre><code>toleration = [{\n    \"key\": \"nvidia.com/gpu\",\n    \"operator\": \"Equal\",\n    \"value\": \"v100\",\n    \"effect\": \"NoSchedule\",\n    \"toleration_seconds\": 300\n}]\n</code></pre>"},{"location":"tasks/kubernetes-resources/#affinity","title":"Affinity","text":"<p>Please see Kubernetes documentation.</p>"},{"location":"tasks/models/","title":"ML Models","text":""},{"location":"tasks/models/#management-via-ui","title":"Management via UI","text":"<p>Models can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new model</li> <li><code>expand</code> a model to see its 5 latest versions</li> <li><code>show</code> the details of a model</li> <li><code>edit</code> a model</li> <li><code>delete</code> a model</li> <li><code>filter</code> models by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete models using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/models/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the model</li> <li><code>Kind</code>: kind of the model</li> <li>(Spec) <code>Path</code>: remote path where the model is stored</li> </ul>"},{"location":"tasks/models/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a model's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/models/#update","title":"Update","text":"<p>You can update a model by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/models/#delete","title":"Delete","text":"<p>You can delete a model from either its detail page or the list of models, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/models/#management-via-sdk","title":"Management via SDK","text":"<p>A <code>model</code> can be managed with the following methods.</p> <ul> <li><code>new_model</code>: create a new model</li> <li><code>get_model</code>: get a model</li> <li><code>update_model</code>: update a model</li> <li><code>delete_model</code>: delete a model</li> <li><code>list_models</code>: list all models</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Model</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\n## From library\nmodel = dh.new_model(project=\"my-project\",\n                           name=\"my-model\",\n                           kind=\"model\",\n                           path=\"s3://my-bucket/my-model.ext\")\n\n## From project\nmodel = project.new_model(name=\"my-model\",\n                                kind=\"model\",\n                                path=\"s3://my-bucket/my-model.ext\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete a model, focusing on managing models through the library. If you want to manage models from the project, you can use the <code>Project</code> object and avoid having to specify the <code>project</code> parameter.</p>"},{"location":"tasks/models/#create_1","title":"Create","text":"<p>To create a model you can use the <code>new_model()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the model will be created</li> <li><code>name</code>: name of the model</li> <li><code>kind</code>: kind of the model</li> <li><code>path</code>: remote path where the model is stored. If you instead upload the model at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>uuid</code>: uuid of the model (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: description of the model</li> <li><code>source</code>: remote source of the model (git repository)</li> <li><code>labels</code>: labels of the model</li> <li><code>embedded</code>: whether the model is embedded or not. If <code>True</code>, the model is embedded (all the spec details are expressed) in the project. If <code>False</code>, the model is not embedded in the project</li> <li><code>src_path</code>: local path of the model, used in case of upload into remote storage</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>model = dh.new_model(project=\"my-project\",\n                           name=\"my-model\",\n                           kind=\"model\",\n                           path=\"s3://my-bucket/my-model.ext\")\n</code></pre>"},{"location":"tasks/models/#read_1","title":"Read","text":"<p>To read a model you can use the <code>get_model()</code> or <code>import_model()</code> methods. The first one searches for the model into the backend, the second one loads it from a local yaml.</p>"},{"location":"tasks/models/#get","title":"Get","text":"<p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the model will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the model as identifier. It returns the latest version of the model</li> <li><code>entity_id</code>: to use the uuid of the model as identifier. It returns the specified version of the model</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>model = dh.get_model(project=\"my-project\",\n                           entity_name=\"my-model\")\n\nmodel = dh.get_model(project=\"my-project\",\n                           entity_id=\"uuid-of-my-model\")\n</code></pre>"},{"location":"tasks/models/#import","title":"Import","text":"<p>Mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the model yaml</li> </ul> <p>Example:</p> <pre><code>model = dh.import_model(file=\"./some-path/my-model.yaml\")\n</code></pre>"},{"location":"tasks/models/#update_1","title":"Update","text":"<p>To update a model you can use the <code>update_model()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>model</code>: model object to be updated</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>model = dh.new_model(project=\"my-project\",\n                           name=\"my-model\",\n                           kind=\"model\",\n                           path=\"s3://my-bucket/my-model.ext\")\n\nmodel.metadata.description = \"My new description\"\n\nmodel = dh.update_model(model=model)\n</code></pre>"},{"location":"tasks/models/#delete_1","title":"Delete","text":"<p>To delete a model you can use the <code>delete_model()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the model exists</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the model as identifier</li> <li><code>entity_id</code>: to use the uuid of the model as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the model will be deleted. Mutually exclusive with the <code>entity_id</code> parameter.</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>model = dh.new_model(project=\"my-project\",\n                           name=\"my-model\",\n                           kind=\"model\",\n                           path=\"s3://my-bucket/my-model.ext\")\n\ndh.delete_model(project=\"my-project\",\n                   entity_id=model.id)\n</code></pre>"},{"location":"tasks/models/#list","title":"List","text":"<p>To list all models you can use the <code>list_models()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project containing the models</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>models = dh.list_models(project=\"my-project\")\n</code></pre>"},{"location":"tasks/models/#model-object","title":"Model object","text":"<p>The <code>Model</code> object is built using the <code>new_model()</code> method. There are several variations of the <code>Model</code> object based on the <code>kind</code> of the model. The SDK supports the following kinds:</p> <ul> <li><code>model</code>: represents a generic model</li> </ul> <p>For each different kind, the <code>Model</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>. All the <code>Model</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity model into backend or locally as yaml.</p> <p>To create a specific model, you must use the desired <code>kind</code> in the <code>new_model()</code> method.</p>"},{"location":"tasks/models/#kind-model","title":"Kind: model","text":"<p>The <code>model</code> kind indicates that the model is a generic model. There are no specific <code>spec</code> parameters.</p> <p>The <code>model</code> kind has the following methods:</p> <ul> <li><code>as_file()</code>: collects the model into a local temporary file</li> <li><code>download()</code>: downloads the model into a specified path</li> <li><code>upload()</code>: uploads the model to a specified path</li> </ul>"},{"location":"tasks/models/#as-file","title":"As file","text":"<p>The <code>as_file()</code> method returns the model as a temporary file. The file is not automatically deleted when the program ends. The method returns the path of the downloaded model.</p>"},{"location":"tasks/models/#download","title":"Download","text":"<p>The <code>download()</code> method downloads the model into a specified path. The method returns the path of the downloaded model. The method accepts the following parameters:</p> <ul> <li><code>target</code>: remote path of the model to be downloaded (eg. <code>s3://my-bucket/my-model.ext</code>). By default, uses the <code>spec</code> <code>path</code>.</li> <li><code>dst</code>: local path where the model will be downloaded. By default, it is in the current working directory</li> <li><code>overwrite</code>: if <code>True</code>, the target path will be overwritten if it already exists</li> </ul>"},{"location":"tasks/models/#upload","title":"Upload","text":"<p>The <code>upload()</code> method uploads the model to a specified path. The method returns the path of the uploaded model. The method accepts the following parameters:</p> <ul> <li><code>source</code>: local path of the model to be uploaded</li> <li><code>target</code>: remote path of the model to be uploaded (eg. <code>s3://my-bucket/my-model.ext</code>). By default, uses the <code>spec</code> <code>path</code>.</li> </ul>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project represents a data and AI application and is a container for different entities (code, assets, configuration, ...) that form the application. It is the context in which you can run functions and manage models, data, and artifacts. Projects may be created and managed from the UI, but also by using DH Core's API, for example via Python SDK.</p>"},{"location":"tasks/projects/#management-via-ui","title":"Management via UI","text":"<p>In the following sections we document project management via the <code>Core Console</code> UI.</p> <p>Here we detail how to create, read, update and delete projects using the UI, similarly to SDK usage.</p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A project is created by clicking <code>CREATE A NEW PROJECT</code> in the console's home page.</p> <p></p> <p>A form asking for the project's details is then shown:</p> <p></p> <p>The following parameters are mandatory:</p> <ul> <li><code>name</code>: name of the project, also acts as identifier of the project</li> </ul> <p><code>Metadata</code> parameters are optional and may be changed later:</p> <ul> <li><code>name</code>: name of the project</li> <li><code>description</code>: a human-readable description of the project</li> <li><code>labels</code>: list of labels</li> </ul> <p><code>Save</code> and the project will appear in the home page.</p>"},{"location":"tasks/projects/#read","title":"Read","text":"<p>All projects present in the database are listed in the home page. Each tile shows:</p> <ul> <li>Identifier of the project</li> <li>Name of the project (hidden if same as identifier)</li> <li>Description</li> <li>Date of creation</li> <li>Date of last modification</li> </ul> <p></p> <p>Click on the tile to access the project's dashboard:</p> <p></p> <p>This dashboard shows a summary of the resources associated with the project and allows you to access the management of these resources.</p> <ul> <li><code>Jobs and runs</code>: list and status of performed runs</li> <li><code>Models</code>: number and list of latest models</li> <li><code>Functions and code</code>: number and list of latest functions</li> <li><code>Data items</code>: number and list of latest data items</li> <li><code>Artifacts</code>: number and list of latest artifacts</li> </ul> <p>You can return to the list of projects at any time by clicking Projects at the bottom of the left menu, or switch directly to a specific project by using the drop-down menu in the upper left of the interface.</p> <p></p>"},{"location":"tasks/projects/#update","title":"Update","text":"<p>To update a project's <code>Metadata</code>, first click <code>Configuration</code> in the left menu.</p> <p></p> <p>Click <code>Edit</code> in the top right and the edit form for <code>Metadata</code> properties will be shown. In the example below, a label was added.</p> <p></p> <p>When you're done updating the project, click Save.</p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>You can delete a project from the <code>Configuration</code> page, by clicking <code>Delete</code>. You will be asked to confirm by entering the project's identifier.</p> <p></p>"},{"location":"tasks/projects/#management-via-sdk","title":"Management via SDK","text":"<p>In the following sections we document the project CRUD methods available in the SDK and the methods exposed by the <code>Project</code> entity.</p>"},{"location":"tasks/projects/#basic-operations","title":"Basic Operations","text":"<p>Here we analyze how to create, read, update and delete projects using the SDK.</p>"},{"location":"tasks/projects/#create_1","title":"Create","text":"<p>A project is created with <code>new_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project, it is also the identifier of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>context</code>: path where project can export yaml files locally</li> <li><code>description</code>: a human readable description of the project</li> <li><code>source</code>: a Git repository URL where lies the project code</li> <li><code>labels</code>: list of labels</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.new_project(\"my-project\", context=\"./\", description=\"my new project\")\n</code></pre>"},{"location":"tasks/projects/#config","title":"Config","text":"<p>The <code>config</code> parameter can be used to provide a dictionary containing the project configuration like user and password for basic auth or a bearer token. The format of the dictionary for basic auth must be as this:</p> <pre><code>{\n    \"user\": \"user\",\n    \"password\": \"password\"\n}\n</code></pre> <p>The format of the dictionary for bearer token must be as this:</p> <pre><code>{\n    \"access_token\": \"token\"\n}\n</code></pre> <p>In case you try to get a project without from the backend with invalid credentials, an exception will be raised. Because the backend client is a Singleton object, it will autoconfigure credentials at startup, so the only way to setup proper credentials once it fails to connect is to use the SDK method <code>set_dhub_env()</code>. The method accepts the following optional parameters:</p> <ul> <li><code>endpoint</code>: the endpoint of the backend</li> <li><code>user</code>: the user for basic auth</li> <li><code>password</code>: the password for basic auth</li> <li><code>token</code>: the auth token</li> </ul> <p>Example:</p> <pre><code>dh.set_dhub_env(\n    endpoint=\"https://some-digitalhub:8080\",\n    token=\"token\"\n)\n</code></pre> <p>Note that the <code>set_dhub_env()</code> method ovverrides the environment variables and (if already instantiated) the credentials attributes of the backend client.</p>"},{"location":"tasks/projects/#setup-kwargs","title":"Setup kwargs","text":"<p>The <code>setup_kwargs</code> parameter can be used to provide a dictionary containing the project hook setup arguments. The concept behind this parameter is that at the beginning of the project lifecycle, the project can be configured with an hook script that will be executed when the project is created / got. First of all, the configuration script MUST comply with the following format:</p> <ul> <li>It must be a Python script named <code>setup_project.py</code> inside the project context directory.</li> <li>It must contain an handler (a python function) named <code>setup</code> as entrypoint.</li> <li>The <code>setup</code> function must accept a <code>Project</code> instance as the only positional argument.</li> <li><code>setup_kwargs</code> must be passed as keyword arguments to the <code>setup</code> function.</li> </ul> <p>The project setup will create a <code>.CHECK</code> file at the end of the <code>setup</code> function execution. This sentinel file is used to indicate that the project is set up and new executions will be ready.</p> <p>A use case scenario can be the instantiation of entities used by the user like artifacts or functions.</p> <p>Example:</p> <pre><code>setup_kwargs = {\n    \"some_arg1\": \"arg1\",\n    \"some_arg2\": \"arg2\"\n}\n\n# Setup script\n\ndef setup(project, some_arg1=None, some_arg2=None):\n    # Do something with project and args\n</code></pre>"},{"location":"tasks/projects/#read_1","title":"Read","text":"<p>You can read a project with three methods, from remote backend with <code>get_project()</code>, from local directory with <code>import_project()</code> or from either with <code>load_project()</code>.</p>"},{"location":"tasks/projects/#get-project","title":"Get project","text":"<p>With <code>get_project()</code> you can load a project from the backend. The method requires the following manadatory parameters:</p> <ul> <li><code>name</code>: the name of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.get_project(\"my-project\")\n</code></pre>"},{"location":"tasks/projects/#import-project","title":"Import project","text":"<p>With <code>import_project()</code> you can load a project from a local directory. The method requires the following manadatory parameters:</p> <ul> <li><code>file</code>: path to the yaml project file</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.import_project(\"./my-project.yaml\")\n</code></pre>"},{"location":"tasks/projects/#load-project","title":"Load project","text":"<p>With <code>load_project()</code> you can load a project from the backend or from a local directory. The method requires either one of the following parameters:</p> <ul> <li><code>name</code>: the name of the project</li> <li><code>file</code>: path to the yaml project file</li> </ul> <p>If you pass the name, the method will try to load the project from the backend, otherwise it will try to load it from the local directory. Note that both parameters are mutually exclusive and keyword arguments.</p> <p>The other parameters are optional:</p> <ul> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.load_project(name=\"my-project\")\nproject = dh.load_project(file=\"./my-project.yaml\")\n</code></pre>"},{"location":"tasks/projects/#create-or-read","title":"Create or read","text":"<p>The <code>digitalhub</code> SDK provides a method <code>get_or_create_project()</code> that allows you to create a project if it does not exist. The method has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project.</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>context</code>: path where project can export yaml files locally</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>config</code>: a dictionary containing the project configuration like user and password for basic auth or a bearer token</li> <li><code>setup_kwargs</code>: a dictionary containing the project hook setup arguments</li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.get_or_create_project(\"my-project\", context=\"./\")\n</code></pre>"},{"location":"tasks/projects/#update_1","title":"Update","text":"<p>You can update a project with <code>update_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>project</code>: the project entity that will be updated</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.update_project(project)\n</code></pre>"},{"location":"tasks/projects/#delete_1","title":"Delete","text":"<p>You can delete a project with <code>delete_project()</code> method. It has the following mandatory parameters:</p> <ul> <li><code>name</code>: the name of the project</li> </ul> <p>The other parameters are optional:</p> <ul> <li><code>cascade</code>: a boolean value, if <code>True</code> the project and all the related resources will be deleted from the backend. Defaults to <code>True</code>. It is only available in the Core backend.</li> <li><code>clean_context</code>: a boolean value, if <code>True</code> the project context will be deleted (no more object can be created locally under the project).</li> <li><code>local</code>: a boolean value, if <code>True</code> the project will be managed without Core backend. Defaults to <code>False</code></li> <li><code>kwargs</code>: a list of keyword arguments passed to the client that comunicate with the backend</li> </ul> <p>Example:</p> <pre><code>project = dh.delete_project(\"my-project\")\n</code></pre>"},{"location":"tasks/projects/#project-methods","title":"Project methods","text":"<p>The <code>Project</code> class exposes two basic methods that allow you to save remotely or export locally the project. Furthermore, according to the SDK digitalhub layer installed, the <code>Project</code> class exposes CRUD methods for a variety of entities.</p>"},{"location":"tasks/projects/#save-and-export","title":"Save and export","text":"<p>The methods <code>save()</code> and <code>export()</code> are used to save the project on the backend or export the project locally.</p>"},{"location":"tasks/projects/#save","title":"Save","text":"<p>The <code>save()</code> method is used to save the project on the backend and it accepts the following optional parameters:</p> <ul> <li><code>update</code>: a boolean value, if <code>True</code> the project will be updated on the backend</li> </ul> <p>Please note that the save method will usually raise an exception if called without the <code>update</code> parameter on runtime. This is because the project (managed with CRUD SDK methods) should already exists on the backend if exists as object.</p>"},{"location":"tasks/projects/#export","title":"Export","text":"<p>The <code>export()</code> method is used to export the project locally as yaml file and it accepts the following optional parameters:</p> <ul> <li><code>filename</code>: the name of the file to export</li> </ul> <p>Note that the filename must have the <code>.yaml</code> extension. The project will be exported as a yaml file inside the context directory. If no filename is passed, the project will be exported as a yaml file named <code>project_{project-name}.yaml</code>.</p> <p>According to the SDK digitalhub layer installed, the <code>Project</code> class exposes CRUD methods for a variety of entities.</p>"},{"location":"tasks/projects/#entity-management-operations","title":"Entity Management Operations","text":"<p>The project acts as context for other entities as mentioned in the introduction. With a <code>Project</code> object, you can create, read, update and delete these entities. The methods exposed are basically five for all entities, and are:</p> <ul> <li><code>new</code>: create a new entity</li> <li><code>get</code>: get an entity from backend</li> <li><code>update</code>: update an entity</li> <li><code>delete</code>: delete an entity</li> <li><code>list</code>: list entities related to the project</li> </ul> <p>Each digitalhub layer exposes CRUD handles different aspects of data and ml entities. Here follows a list of the methods exposed by the <code>Project</code> class for each layer. Please refer to the specific entity documentation for more information.</p>"},{"location":"tasks/projects/#core-layer","title":"Core layer","text":"<p>The entity handled by the <code>Project</code> class in the core layer (<code>digitalhub_core</code>) are:</p> <ul> <li><code>functions</code></li> <li><code>artifacts</code></li> <li><code>workflows</code></li> <li><code>secrets</code></li> </ul>"},{"location":"tasks/projects/#data-layer","title":"Data layer","text":"<p>The entity handled by the <code>Project</code> class in the data layer (<code>digitalhub_data</code>) are:</p> <ul> <li><code>dataitems</code></li> </ul>"},{"location":"tasks/projects/#ml-layer","title":"Ml layer","text":"<p>The entity handled by the <code>Project</code> class in the ml layer (<code>digitalhub_ml</code>) are:</p> <ul> <li><code>models</code></li> </ul>"},{"location":"tasks/resources/","title":"Resource Management with KRM","text":"<p>Different platform entities are associated with and represented as Kubernetes resources: they are deployed as services, user volumes and secrets, captured as Custom Resources, etc. Kubernetes Resource Manager (KRM) component allows for performing various operations over these resources depending on their kind.</p> <p></p> <p>KRM navigation menu provides access to different types of resources. This includes both standard resources (Services, Deployments, Persistent Volume Claims, Secrets) and custom resources based on Custom Resource Definitions currently installed on the platform. Some custom resources are managed with the customized UI (e.g., PostgreSQL instances, PostgREST Data services o Dremio Data service), while the others may be managed with the standard UI based on their JSON schema.</p>"},{"location":"tasks/resources/#management-of-standard-kubernetes-resources","title":"Management of Standard Kubernetes Resources","text":"<p>KRM allows for accessing and managing the standard K8S resources relevant for the DigitalHub platform: space (through Persistent Volume Claims), services and deployments, and secrets.</p>"},{"location":"tasks/resources/#listing-k8s-services","title":"Listing K8S Services","text":"<p>Accessing the <code>Services</code> menu of the KRM, it is possible to list the (subset of) services deployed on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each service KRM shows its name, type (e.g., Coder workspace type), exposed port type and value. In the service details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#listing-k8s-deployments","title":"Listing K8S Deployments","text":"<p>Accessing the <code>Deployments</code> menu of the KRM, it is possible to list the (subset of) deployments on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each deployment KRM shows its name and availability of instances. In the deployment details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#managing-persistent-volume-claims","title":"Managing Persistent Volume Claims","text":"<p>In certain cases, the operations developed with the platform may require more substantial disk space, e.g., for training / producing significant amounts of data. In this case, it is possible to attach to the tasks the corresponding Persistent Volume Claim (PVC) references. To create a new PVC for the use of the pipeline or Job, KRM provides the corresponding interface.</p> <p>Accessing <code>Persistent Volume Claims</code> menu, it is possible to list and manage the PVCs of the platform. </p> <p></p> <p>For each PVC, you can see the status (Pending or Bound) of the PVC, the name of the volume (if specified), the storage class and the size in Gi. The details view provides further metadata regarding the PVC.</p> <p>It is also possible to delete the PVC and create new ones.</p> <p>Deleting PVC</p> <p>Please note that deleting a PVC bound to a Pod or a Job may affect negatively their execution.</p> <p>To create a new PVC, provide the following:</p> <ul> <li>name of the resource</li> <li>Disk space requeste</li> <li>Storage class name (select one of the available in your deployment)</li> <li>(Optional) name of the volume</li> <li>Access modes (standard K8S values)</li> <li>PVC mode (Filesystem or Block)</li> </ul> <p></p>"},{"location":"tasks/resources/#listing-k8s-secrets","title":"Listing K8S Secrets","text":"<p>Accessing the <code>Secrets</code> menu of the KRM, it is possible to list the (subset of) secrets on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each secret KRM shows its name, type, and number of elements. In the secret details view it is possible to access other metadata and also a list of secret elements. The values are not available directly; to retrieve the actual value of the secret element, use <code>Decode</code> button that will copy the decoded content of the secret into Clipboard.</p> <p></p>"},{"location":"tasks/resources/#managing-custom-resources","title":"Managing Custom Resources","text":"<p>KRM allows for the management of generic CRs as well as for the management of some predefined ones, such as PostgreSQL instances, PostgREST and Dremio Data services.</p>"},{"location":"tasks/resources/#managing-postgresql-instances-with-krm","title":"Managing PostgreSQL instances with KRM","text":"<p>Using PostgreSQL operator (https://github.com/movetokube/postgres-operator) it is possible to create new DB instances and the DB users to organize the data storage.</p> <p>Accessing <code>Postgres DBs</code> menu of the KRM, it is possible to list, create, and delete PostgreSQL databases.</p> <p></p> <p>To create a new Database, provide the following:</p> <ul> <li>name of the database to create</li> <li>whether to drop the DB on resource deletion</li> <li>Comma-separated list of PostgreSQL extensions to enable (e.g., timescale and/or postgis) as supported by the platform deployment (optional). </li> <li>Comma-separated list of schemas to create in DB (optional) </li> <li>Name of the master role for the DB access management (optional) </li> </ul> <p></p> <p>In the Database details view it is possible also to configure the DB users that can access and operate the Database (create, edit, view, delete). To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the user to be created</li> <li>access privileges (e.g., Owner, Read, or Write)</li> <li>name of the secret to be create to store the user credentials and DB access information. This results in creating a secret  <code>&lt;user-cr-name&gt;-&lt;secret-name&gt;</code> that can be accessed in the Secrets section of KRM.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-s3-resources-with-krm","title":"Managing S3 resources with KRM","text":"<p>If supported by the deployment, using Minio S3 operator (http://github.com/scc-digitalhub/minio-operator/) it is possible to create new S3 buckets, policies, and create/associate users to them.</p> <p>Accessing <code>S3 Buckets</code> menu of the KRM, it is possible to list, create, and delete S3 buckets, policies, and users.</p> <p></p> <p>To create a new Bucket, provide the following:</p> <ul> <li>name of the bucket to create</li> <li>optional quota for the bucket</li> </ul> <p></p> <p>In the S3 Policies tab view it is possible also to configure the S3 policies. To create a new policy, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the policy to be created</li> <li>standard S3 policy spec in JSON format.</li> </ul> <p></p> <p>In the S3 Users tab view it is possible also to configure the S3 users. To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>access key for the user to be created</li> <li>secret key for the user</li> <li>list of policies to associate to the user.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-postgrest-data-services-with-krm","title":"Managing PostgREST Data Services with KRM","text":"<p>Using KRM it is possible to instantiate and deploy new PostgREST Data services. A PostgREST service exposes a set of PostgreSQL tables as API, allowing for querying the information and even modifying it.</p> <p>Accessing <code>PostgREST Data Services</code> menu it is possible to list, create, and delete data service instances.</p> <p></p> <p>To create a new data service, it is necessary to provide the information about the exposed schema and tables, the DB access information, and the role with which the service operates. This latter may be specified either as an existing users with the appropriate permissions or may be created if the DB access information is sufficient for  that operation. More specifically, it is necessary to provide the following information:</p> <ul> <li>name of the resource.</li> <li>name of the DB schema to expose.</li> <li>the existing DB user (role) on behalf of which the service will operate OR the list of DB permissions to enable for this service and list of exposed DB tables. In this case the user will be created (if the connection information allows for it).</li> <li>Connection information with cluster DB host and port (optional), name of the database, DB username / password OR, alternatively, DB secret to use in order to extract the connection credentials. In this later case the secret should contain elements <code>USERNAME</code> and <code>PASSWORD</code>, or alternatively <code>POSTGRES_URL</code> with the full connection information.</li> </ul> <p>Connection information</p> <p>Please note that in order to create a new role that will be used by the service to access the data, the user specified with the connection information should have sufficient privileges to perform the operation. By default, the owner/writer/readers users created by the Postgres operator do not have this permission.</p> <p>Schema exposure</p> <p>PostgREST exposes all the tables and views in the schema specified in the configuration. In order to have a better control over the exposed data, it is  recommended to create a separate schema (e.g., 'api') and provide the access to the data via views / stored procedures. To accomplish this, it is possible to use SQLPad to create schemas and views.</p> <p></p> <p>This will result in a deployment of PostgREST microservice connected to the specified database and exposing PostgREST API over the specified schema and tables.  See here for further details.</p>"},{"location":"tasks/resources/#managing-dremio-data-services-with-krm","title":"Managing Dremio Data Services with KRM","text":"<p>Using KRM it is possible to instantiate and deploy new Dremio Data services that expose the data presented in Dremio views as API.</p> <p>Accessing <code>Dremio Data Services</code> menu it is possible to list, create, and delete data service instances.</p> <p></p> <p>To create a new data service, provide the following:</p> <ul> <li>name of the resource</li> <li>list of exposed virtual datasets</li> <li>Connection information with dremio host and port (optional), Dremio username / password OR, alternatively, a secret to use in order to extract the connection credentials. In this later case the secret should contain elements <code>USER</code> and <code>PASSWORD</code>.</li> </ul> <p></p> <p>This will result in a deployment of Dremio REST microservice connected to the specified database and exposing a simple REST API over the specified datasets. </p>"},{"location":"tasks/resources/#exposing-services-externally","title":"Exposing services externally","text":"<p>Various APIs and services (e.g., PostgREST or Dremio data services, Nuclio serverless functions) may be exposed externally, outside of the platform, on a public domain of the platform. Using KRM, the operation amounts to defining a new API gateway resource that will be transformed into the corresponding ingress routing specification. </p> <p></p> <p>To create a new API gateway, provide the following:</p> <ul> <li>name of the gateway</li> <li>Kubernetes service to be exposed (select it from the dropdown list and the port will automatically be provided)</li> <li>host and relative path to be exposed. The host defines the full domain name to be exposed. By default it refers to the 'services' subdomain, e.g., <code>myservice.services.example.com</code> where <code>example.com</code> corresponds to the platform domain.</li> <li>authentication information. Currently, services may be unprotected (<code>None</code>) or protected with <code>Basic</code> authentication, specifying username and password.</li> </ul>"},{"location":"tasks/resources/#defining-and-managing-crd-schemas","title":"Defining and Managing CRD Schemas","text":"<p>To have a valid representation of the CRs in the system, it is necessary to have a JSON specification schema for each CRDs. Normally, such schema is provided with the CRD definition and is used by KRM to manage the resources. However, in certain cases a CRD may have no structured schema definition attached. To allow for managing such resources, it is possible to provide a custom schema for the CRD.</p> <p>Creating a schema is fairly simple. Access the Settings section from the left menu and click Create.</p> <p>The CRD drop-down menu will list all Custom Resource Definitions available on the Kubernetes instance; when you pick one, the Version field will automatically be filled with the version of the currently active schema.</p> <p>Provide the Schema definition and save it in KRM for future CR management.</p>"},{"location":"tasks/secrets/","title":"Secret Management","text":"<p>Working with different operations may implu the usage of a sensitive values, such as external API credentials, storage credentials, etc. </p> <p>In order to avoid embedding the credentials in the code of functions, the platform supports an explicit management of credentials as secrets. This operation exploits the underlying secret management subsystem, such as Kubernetes Secret Manager.</p> <p>Besides the secrets managed natively by the platform to integrate e.g., default storage credentials, it is possible to  define custom secrets at the level of a single project. The project secrets are managed as any other project-related entities, such as functions, dataitems, etc.</p> <p>At the level of the project the secrets are represented as key-value pairs. The management of secrets is delegated to a secret provider, and currently only Kubernetes Secret Manager is supported. Each project has its own Kubernetes secret, where  all the key-value pairs are stored.</p> <p>To create a new secret value it is possible to use the Core UI console or directly via API, e.g., using the SDK. </p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-ui","title":"Creating and Managing Secrets via UI","text":"<p>Core console can be used to manage project secrets. To create a new one, it is necessary to provide  a secret key and a value to be stored. </p> <p></p> <p>The entries may be then deleted and updated, as well as their metadata.</p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-sdk","title":"Creating and Managing Secrets via SDK","text":"<p>The secrets may be operated by the DigitalHub SDK. More specifically, to create a new secret in the project</p> <pre><code>project = dhcore.get_or_create_project(\"project-secrets\")\n\nsecret0 = project.new_secret(name=\"somesecret\", secret_value=\"value1\")\nprint(secret0)  \n</code></pre> <p>To read the value of an existing secret <pre><code>secret0 = project.get_secret(entity_name=\"somesecret\")\nprint(secret0.read_secret_value())\n</code></pre></p> <p>To update an existing secret <pre><code>secret0.set_secret_value(value=\"value1\")\nprint(secret0.read_secret_value())\n</code></pre></p> <p>To read the project secret entities <pre><code>secrets = project.list_secrets()\nprint(secrets)\n</code></pre></p> <p>To delete a secret <pre><code>project.delete_secret(entity_name=\"somesecret\")\nsecrets = project.list_secrets()\nprint(secrets)\n</code></pre></p>"},{"location":"tasks/workflows/","title":"Workflows","text":"<p>Workflows allow for organizing the single operations in a advanced management pipelines, to perform a series operation of data processing, ML model training and serving, etc. Workflows represent long-running procedures defined as Directed Acyclic Graphs (DAGs) where each node is a single unit of work performed by the platform (e.g., as a Kubernetes Job). </p> <p>As in case of functions, it is possible for the platform to have different workflow runtimes. Currently, the only workflow runtime implemented is the one based on Kubeflow Pipelines infrastructure. See KFP Runtime for further details about how the workflow is defined and executed with the Kubeflow Pipelines component of the platform. </p> <p>Similarly, to functions the workflows may be managed via console UI or via Python SDK. </p>"},{"location":"tasks/workflows/#management-via-ui","title":"Management via UI","text":"<p>Workflows can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new workflow</li> <li><code>expand</code> a workflow to see its 5 latest versions</li> <li><code>show</code> the details of a workflow</li> <li><code>edit</code> a workflow</li> <li><code>delete</code> a workflow</li> <li><code>filter</code> workflows by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete workflows using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/workflows/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the workflow</li> <li><code>Kind</code>: kind of workflow</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p>In case of a <code>kfp</code> workflow, the source code and handler fields are required as well.</p>"},{"location":"tasks/workflows/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a workflow's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p> <p>In case of <code>kfp</code> workflows, the executions of the workflow instances can be monitored with the corresponding DAG viewer.</p> <p></p>"},{"location":"tasks/workflows/#update","title":"Update","text":"<p>You can update a workflow by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/workflows/#delete","title":"Delete","text":"<p>You can delete a workflow from either its detail page or the list of workflows, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/workflows/#management-via-sdk","title":"Management via SDK","text":"<p>A <code>workflow</code> can be managed with the following methods.</p> <ul> <li><code>new_workflow</code>: create a new workflow</li> <li><code>get_workflow</code>: get a workflow</li> <li><code>update_workflow</code>: update a workflow</li> <li><code>delete_workflow</code>: delete a workflow</li> <li><code>list_workflows</code>: list all workflows</li> </ul> <p>This is done in two ways. The first is through the SDK and the second is through the <code>Workflow</code> object. Example:</p> <pre><code>import digitalhub as dh\n\nproject = dh.get_or_create_project(\"my-project\")\n\nworkflow = project.new_workflow(name=\"my-workflow\",\n                           kind=\"kfp\",\n                           source={\"source\": \"src/pipeline.py\"}, \n                           handler=\"pipeline\")\n</code></pre> <p>The syntax is the same for all CRUD methods. The following sections describe how to create, read, update and delete a workflow, focusing on managing workflows through the library. If you want to manage workflows from the project, you can use the <code>Project</code> object and avoid having to specify the <code>project</code> parameter.</p>"},{"location":"tasks/workflows/#create_1","title":"Create","text":"<p>To create a workflow you can use the <code>new_workflow()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>name</code>: name of the workflow</li> <li><code>kind</code>: kind of the workflow runtime (e.g., <code>kfp</code>)</li> <li>source: source code specification (e.g., file reference)</li> <li>handler: name of the pipeline method</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>uuid</code>: uuid of the workflow (this is automatically generated if not provided). Must be a valid uuid v4.</li> <li><code>description</code>: description of the workflow</li> <li><code>labels</code>: labels of the workflow</li> <li><code>embedded</code>: whether the workflow is embedded or not. If <code>True</code>, the workflow is embedded (all the spec details are expressed) in the project. If <code>False</code>, the workflow is not embedded in the project</li> <li><code>kwargs</code>: keyword arguments passed to the spec constructor</li> </ul> <p>Example:</p> <pre><code>workflow = project.new_workflow(name=\"my-workflow\",\n                           kind=\"kfp\",\n                           source={\"source\": \"src/pipeline.py\"}, \n                           handler=\"pipeline\")\n</code></pre>"},{"location":"tasks/workflows/#read_1","title":"Read","text":"<p>To read a workflow you can use the <code>get_workflow()</code> or <code>import_workflow()</code> methods. The first one searches for the workflow into the backend, the second one loads it from a local yaml.</p>"},{"location":"tasks/workflows/#get","title":"Get","text":"<p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the workflow will be created</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the workflow as identifier. It returns the latest version of the workflow</li> <li><code>entity_id</code>: to use the uuid of the workflow as identifier. It returns the specified version of the workflow</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>workflow = dh.get_workflow(project=\"my-project\",\n                           entity_name=\"my-workflow\")\n\nworkflow = dh.get_workflow(project=\"my-project\",\n                           entity_id=\"uuid-of-my-workflow\")\n</code></pre>"},{"location":"tasks/workflows/#import","title":"Import","text":"<p>Mandatory parameters are:</p> <ul> <li><code>file</code>: file path to the workflow yaml</li> </ul> <p>Example:</p> <pre><code>workflow = dh.import_workflow(file=\"./some-path/my-workflow.yaml\")\n</code></pre>"},{"location":"tasks/workflows/#update_1","title":"Update","text":"<p>To update a workflow you can use the <code>update_workflow()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>workflow</code>: workflow object to be updated</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>workflow = dh.new_workflow(project=\"my-project\",\n                           name=\"my-workflow\",\n                           kind=\"kfp\",\n                           source={\"source\": \"src/pipeline.py\"}, \n                           handler=\"pipeline\")\n\nworkflow.metadata.description = \"My new description\"\n\nworkflow = dh.update_workflow(workflow=workflow)\n</code></pre>"},{"location":"tasks/workflows/#delete_1","title":"Delete","text":"<p>To delete a workflow you can use the <code>delete_workflow()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project in which the workflow exists</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>entity_name</code>: to use the name of the workflow as identifier</li> <li><code>entity_id</code>: to use the uuid of the workflow as identifier</li> <li><code>delete_all_versions</code>: if <code>True</code>, all versions of the workflow will be deleted. Mutually exclusive with the <code>entity_id</code> parameter.</li> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>dh.delete_workflow(project=\"my-project\",\n                   entity_id=workflow.id)\n</code></pre>"},{"location":"tasks/workflows/#list","title":"List","text":"<p>To list all workflows you can use the <code>list_workflows()</code> method.</p> <p>Mandatory parameters are:</p> <ul> <li><code>project</code>: the project containing the workflows</li> </ul> <p>Optional parameters are:</p> <ul> <li><code>kwargs</code>: keyword arguments passed to the client that communicates with the backend</li> </ul> <p>Example:</p> <pre><code>workflows = dh.list_workflows(project=\"my-project\")\n</code></pre>"},{"location":"tasks/workflows/#workflow-object","title":"Workflow object","text":"<p>The <code>Workflow</code> object is built using the <code>new_workflow()</code> method. There are several variations of the <code>Workflow</code> object based on the <code>kind</code> of the workflow. The SDK supports the following kinds:</p> <ul> <li><code>kfp</code>: represents a workflow implemented with thr Kbeflow Pipleines runtime.</li> </ul> <p>For each different kind, the <code>Workflow</code> object has a different set of methods and different <code>spec</code>, <code>status</code> and <code>metadata</code>. All the <code>Workflow</code> kinds have a <code>save()</code> and an <code>export()</code> method to save and export the entity workflow into backend or locally as yaml.</p> <p>To create a specific workflow, you must use the desired <code>kind</code> in the <code>new_workflow()</code> method.</p>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p> <p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>Let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}