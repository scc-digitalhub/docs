{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Digital Hub is an Open-Source platform for building and managing Data and AI applications and services. Bringing the principles of DataOps, MLOps, DevOps and GitOps, integrating the state-of-art open source technologies and modern standards, DigitalHub extends your  development process and operations to the whole application life-cycle, from exploration to deployment, monitoring, and evolution.</p> <p></p> <ul> <li>Explore. Use on-demand interactive scalable workspaces of your choice to code, explore, experiment and analyze the data and ML. Workspace provide a secure and isolated execution environments natively connected to the platform, such as Jupyter Notebooks or VS Code, Dremio distributed query engine, etc. Use extensibility mechanisms provided by the underlying workspace manager to bring your own workspace templates into the platform.</li> <li>Process. Use persistent storages (Datalake and Relational DBs) to manage structured and non-structured data on top of the data abstraction layer. Elaborate  data, perform data analysis activites and train AI models using frameworks and libraries of your choice (e.g., from python-based to DBT, to arbitrary containers). Manage the supporting computational and storage resources in a declarative and transparent manner.</li> <li>Execute. Delegate the code execution, image preparation, run-time operations and services to the underlying Kubernetes-based execution infrastructure, Serverless platform, and pipeline execution automation environment.</li> <li>Integrate. Build new AI services and expose your data in a standard and interoperable manner, to facilitate the integration within different applications, systems, business intelligence tools and visualizations. </li> </ul> <p>To support this functionality, the platform relies on scalable Kubernetes platform and its extensions (operators) as well as on the modular architecture and functinality model that allows for dealing with arbitrary jobs, functions, frameworks and solutions without affecting your development workflow. The underlying methodology and management approach aim at facilitating the re-use, reproducability, and portability of the solution among different contexts and settings. </p>"},{"location":"#interested","title":"Interested?","text":"<ul> <li>Quick Start. Bring the platform up and explore it in few minutes!</li> <li>Installation. Learn how to install, configure, and manage the platform in different settings.</li> <li>Overview. Deep dive into the platform functionality, architecture, components, and functionality.</li> </ul>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<ul> <li>Native support for the integration of new functionality</li> <li>Standard infrastructures and frameworks</li> <li>Common workflow and abstractions</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform adopts Nuclio as the serverless platform, and supports Python, Java and Go as programming languages.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with ML a range of frameworks, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (via MLRun), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-on-minikube","title":"Installation on minikube","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm</li> <li>Kubectl</li> <li>Minikube</li> </ul>"},{"location":"installation/#installation","title":"Installation","text":"<p>1) Start minikube (change 192.168.49.0 if your network setup is different): <pre><code>    minikube start --insecure-registry \"192.168.49.0/24\" --memory 12288 --cpus 4\n</code></pre></p> <p>2) Add Digitalhub repository: <pre><code>helm repo add digitalhub https://scc-digitalhub.github.io/digitalhub/\n</code></pre></p> <p>3) Get minikube external IP: <pre><code>    minikube ip\n</code></pre></p> <p>4) Install DigitalHub with Helm.</p> <p>Replace the two placeholders called <code>MINIKUBE_IP_ADDRESS</code> in the command below with the output of the previous command, <code>minikube ip</code> <pre><code>helm upgrade digitalhub digitalhub/digitalhub -n digitalhub --install --create-namespace --set global.registry.url=\"MINIKUBE_IP_ADDRESS\" --set global.externalHostAddress=\"MINIKUBE_IP_ADDRESS\" --timeout 45m0s\n</code></pre></p> <p>5) Wait until all pods are in Running or Completed state <pre><code>kubectl --namespace digitalhub get pods\n</code></pre></p> <p>Once installed, you should see the references (URLs) for the different tools of the platform, similar to the example below: <pre><code>##########################################################\n#   _____   _       _           _ _     _       _        #\n#  (____ \\ (_)     (_)_        | | |   | |     | |       #\n#   _   \\ \\ _  ____ _| |_  ____| | |__ | |_   _| | _     #\n#  | |   | | |/ _  | |  _)/ _  | |  __)| | | | | || \\    #\n#  | |__/ /| ( ( | | | |_( ( | | | |   | | |_| | |_) )   #\n#  |_____/ |_|\\_|| |_|\\___)_||_|_|_|   |_|\\____|____/    #\n#            (_____|                                     #\n#                                                        #\n##########################################################\n\nDigitalhub has been installed. Check its status by running:\n  kubectl --namespace digitalhub get pods\n\nDigitalhub componet URLs:\n  - Dashboard: http://192.168.76.2:30110\n  - Jupyter: http://192.168.76.2:30040 (Create jupyter workspace from template in the coder dashboard before use)\n  - Dremio: http://192.168.76.2:30120 (Create dremio workspace from template in the coder dashboard before use)\n  - Sqlpad: http://192.168.76.2:30140 (Create sqlpad workspace from template in the coder dashboard before use)\n  - Grafana: http://192.168.76.2:30130 (Create grafana workspace from template in the coder dashboard before use)\n  - Vscode: http://192.168.76.2:30190 (Create vscode workspace from template in the coder dashboard before use)\n  - Docker Registry: http://192.168.76.2:30150\n  - Nuclio: http://192.168.76.2:30050\n  - MLRun API: http://192.168.76.2:30070\n  - MLRun UI: http://192.168.76.2:30060\n  - Minio API: http://192.168.76.2:30080 (Username: minio Password: minio123)\n  - Minio UI: http://192.168.76.2:30090 (Username: minio Password: minio123)\n  - KubeFlow: http://192.168.76.2:30100\n  - Coder: http://192.168.76.2:30170 (Username: test@digitalhub.test Password: Test12456@!)\n  - Core: http://192.168.76.2:30180\n  - Kubernetes Resource Manager: http://192.168.76.2:30160\n</code></pre></p> <p>A note for Windows, Darwin and WSL users </p> <p>As of now, due to the limitations of Minikube it is not possible to access your applications directly while using one of the OS mentioned above.  </p> <p>You can still access your apps from browser, but you will have to use the <code>kubectl port-forward</code> command.  </p> <p>For example, if you wish to expose the core service, you can use: <pre><code>kubectl -n digitalhub port-forward service/digitalhub-core 30180:8080\n</code></pre> This will allow you to access core by typing <code>localhost:30180</code> in your browser.</p> <p>The full list of services can be checked using this command: <pre><code>kubectl -n digitalhub get services\n</code></pre></p> <p>Please consult the official Kubernetes documentation for more details.</p>"},{"location":"installation/#install-with-ms-azure","title":"Install with MS Azure","text":"<p>Documentation in progress...</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>To start with DigitalHub, the first step is to install the platform and all its components. For its functionality, DigitalHub relies on Kubernetes, a state-of-art Open-Source containerized application deployment, orchestration and execution platform. While it is possible to run DigitalHub on any Kubernetes installation, the quickest way is to deploy it on Minikube, a local Kubernetes environment with minimal settings. See here instruction on how to set up DigitalHub on Minikube.</p> <p>Once installed, you can access different platform components and perform different operations, ranging from exlorative data science with Jupyter Notebooks, creating projects for data processing or ML tasks, managing necessary resources (e.g., databases or datalake buckets), creating and running different functions, etc.</p>"},{"location":"quickstart/#platform-components-and-functionality","title":"Platform Components and Functionality","text":"<p>To access the different components of the platform start from the landing page, where the components are linked:</p> <ul> <li>Use Coder to create interactive workspaces, such as Jupyter Notebooks, to perform explorative tasks, access and manage the data. See how to use Workspaces for these type of activities.</li> <li>Use DH Core UI to manage your data science and ML project and start with management activities, such as creating data items, defining and executing different functions and operations. Please note that these tasks may be done directly with the DH Core Python SDK from your interactive environment. See how to use DH Console for the management operations.</li> <li>To see and manage the relevant Kubernetes resources (e.g., services, jobs, secrets), as well as custom resources of the platform (e.g., databases, S3 buckets, data services), use Kubernetes Resource Manager. The operations and the functionality of the tool are described in the Resource Management with KRM section of the documentation.</li> <li>Use Minio browser to navigate your datalake, upload and manage the files. The datalake is based on S3 protocol and can be used also programmatically. See the Data and Transformations section on how the data abstraction layer is defined and implemented.</li> <li>If you perform ML task with the Python runtime, you can prepare data, create and log ML Models using DH Core (see, e.g., Python Runtime if you want to use MLRun operations through DHCore).  Alternatively, it is possible to use MLRun subsystem of the platform, MLRun UI provides you the information of the data, models, jobs and services operated with MLRun. See MLRun documentation on how to use MLRun directly.</li> <li>Use Nuclio Serverless platform to deploy and expose functions as services within the platform. Nuclio is also used by MLRun to serve its ML Models as run-time operations. See Nuclio documentation on how to use Nuclio in different scenarios.</li> <li>It is possible to organize the data and ML operations in complex pipelines. Currently the platform relies on Kubeflow Pipelines component for this purpose, orchestrating the activities as single Kubernetes Jobs. See more on this in the corresponding Pipelines section.</li> </ul>"},{"location":"quickstart/#tutorials","title":"Tutorials","text":"<p>Start exploring the platform through a series of tutorials aiming at explaining the key usage scenarios for DigitalHub platform. Specifically</p> <ul> <li>Create your first data management pipeline, from data exploration to automated data ETL procedure running on the platform.</li> <li>Perform DBT data transformation and store the data in a database.</li> <li>Train a scikit-learn ML Model and deploy it as an inference server.</li> <li>Train a MLFLow-compatible Model and deploy it as an inference server.</li> <li>Train a custom ML Model and deploy it as a service with the serverless platform.</li> <li>Work with LLM Model and deploy it as a service with the serverless platform.</li> <li>Use Dremio distributed query engine to organize data and visualize with Grafana.</li> <li>Store data in DB to perform efficient and complex queries and expose the data as REST API.</li> </ul>"},{"location":"components/dashboard/","title":"Landing Page","text":"<p>The landing page is a central access point to reach a number of tools that are automatically run when the platform is installed. It provides access to the platform components and to the monitoring subsystem of the platform.</p> <p></p> <p>Components</p> <ul> <li>Coder, Tool for managing interactive workspaces</li> <li>DH Core Console, UI for the platform management</li> <li>KRM, or Kubernetes Resource Manager, is the tool for organizing and managing standard and custom Kubernetes resources</li> <li>MLRun, a framework for MLOps</li> <li>Nuclio, a platform for serverless functions</li> <li>Kubeflow, a tool for ML pipelines on Kubernetes</li> <li>MinIO, an S3-compatible object datalake UI</li> </ul>"},{"location":"components/dh_console/","title":"Core UI","text":"<p>The Core console is a front-end application backed by the  Core API. It provides a management interface  for the organization and operations over the Data Science Projects and the associated entities, such as:</p> <ul> <li>functions of various runtimes (see the Functions and Runtimes section for details), as well as their executions (runs) grouped by the corresponding operations (tasks)</li> <li>workflows - composite pipelines combining executions of different functions</li> <li>dataitems - structured Data Items managed by the project</li> <li>artifacts - unstructured files related and maanged by the project</li> <li>models - versioned ML Model artifacts with their metrics and metadata (see ML Models section for details)</li> </ul> <p>When you access the console, you land to the project management page, where you can create or delete projects.</p> <p>Note that all the functionality that is performed via UI console through the Core API can be also performed using the platform management Python SDK reflecting management of the same platform entities.</p>"},{"location":"components/dh_console/#create-a-project","title":"Create a Project","text":"<p>Start by clicking the <code>CREATE A NEW PROJECT</code> button.</p> <p></p> <p>Fill the form's properties. </p> <p>Following the selection of a project, you can get an overview of the associated objects on its dashboard and manage them on their dedicated pages.</p>"},{"location":"components/dh_console/#dashboard","title":"Dashboard","text":"<p>The console dashboard shows the resources that have been created with a series of cards and allows you to quickly access them. You can see the runs performed and their respective status, as well as artifacts, data items and functions. </p>"},{"location":"components/dh_console/#objects","title":"Objects","text":"<p>Through the console it is also possible to manage directly the entities related to the project and perform different operations over those. This amounts not only to CRUD (create, update, delete, and read) operations, but also track relations, view detailed metadata and versions, execute functions and pipelines, etc. </p>"},{"location":"components/dh_console/#functions","title":"Functions","text":"<p>Functions define the executable procedures implemented in various ways that can be run by the platform. In console it is possible to create new functions selecting the corresponding runtime and, based on that, providing its specification, e.g., source code. For each function the console lists the different versions of the function, the specification and the code (if available) of the function, as well as different tasks that can be performed over the function in the corresponding runtime. For example, in case of Python runtime, it is possible to <code>build</code> function (generating the corresponding Docker image and caching in the regsitry), to run the function as <code>job</code> (to be executed on the Kubernetes), or to expose the function as a service, that is to <code>serve</code> the function. </p> <p></p> <p>Within the tab corresponding to the specific task, it is possible to access the list of runs executed over the function, the status of execution, the execution log. It is also possible to create new run of the task, defining the specific parameters and configurations for the run. </p>"},{"location":"components/dh_console/#workflows","title":"Workflows","text":"<p>Workflows represent a composition of function executions that is run over the platform, specifying their dependencies (in terms of data and order). This allows for creating complex pipelines for AI/ML and data operations. Currently, the implementation of the workflow relies on the Kubeflow Pipelines framework, that in turn relies on Kubernetes Argo Workflows so that each step of the workflow is executed as a single Kubernetes Job. </p> <p>From the console it is possible to define a new workflow providing the code of the pipeline and run the <code>pipeline</code> task. As in case of the function runs, the execution of the pipeline is being tracked, as well as the progress of single steps, and the corresponding log. </p>"},{"location":"components/dh_console/#dataitems","title":"Dataitems","text":"<p>Through Dataitems the project may define the relevant structured and semi-structured datasets. Dataset may created manually, starting from a reference to an URL of the file or DB table, or may be produced as a result of some data transformation function execution. As in case of the functions, the dataitems are equipped with the relevant metadata (e.g., creation and changes, tags, ownership, etc). Furthermore, the datasets structured as tables (i.e., <code>table</code> kind datasets) are equipped with the derived schemas and profiling and data preview.</p> <p>Through the console it is possible to manage the datasets, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#artifacts","title":"Artifacts","text":"<p>Similar to dataitems, Artifacts it is possible to explicitly capture the relevant unstructured objects and files. Artifacts may be of arbitrary type, and equipped with a generic metadata properties.  </p> <p>Through the console it is possible to manage the artifacts, i.e., create (from a URL or from file), update and delete them.</p>"},{"location":"components/dh_console/#ml-models","title":"ML Models","text":"<p>ML Models represent a specific type of artifacts, which are produced by the AI tranining activites and represent the datasets used for inference operations. While managed in the same manner as other types of entities, ML Models may have a specific set of metadata and specification attributes, such model kind, metrics, algorithm and framework specification, etc. </p> <p>ML Models are further used by the inference services.</p>"},{"location":"components/dh_console/#secrets","title":"Secrets","text":"<p>When executing operations with the platform, the execution might need access to some sensitive values, for example to access data residing on a data-store that requires credentials (such as a private S3 bucket), access a private repository, or many other similar needs.</p> <p>The platform provides the functionality to manage these values, reffered to as Secrets, both through UI and SDK, where it is possible to associate the key-value pair to the project. The data is managed as Kubernetes secrets and is embedded in the execution of a run that relies on that.</p> <p>The management of secrets allows through the console to create, delete, and read the secret values.</p>"},{"location":"components/dh_console/#versioning","title":"Versioning","text":"<p>All entities operated by Core are versioned. When you view the details of an object, all of its versions are listed and browsable. Moreover, when you view a dataitem, its schema and data preview are available.</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with username <code>admin</code> and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from [Coder, using its template]../tasks/workspaces.md). After launching it from Coder you can access Grafana on Grafana UI (http://nodeipaddress:30110).</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> (should be <code>mlrun</code>)</li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> (should be <code>mlrun</code>)</li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running. After launching it from Coder you can access Jupyter Notebook on jupyter-notebook UI (http://nodeipaddress:30040).</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/kubeflow/","title":"KubeFlow Pipelines","text":"<p>Kubeflow Pipelines makes part of the Kubeflow platform and allows for organizing workflows out of single tasks performed as Kubernetes Jobs via Argo Workflows. Kubeflow Pipelines comes with its own DSL specification on top of Python, which is compiled into a workflow definition ready for execution in Kubernetes. In this way wach task, its resources, dependencies, etc may be configured indipendently; the management and tracking is performed by the Kubeflow Pipelines component, equipped also with the Web-based UI for monitoring.  </p> <p>The platform used Kubeflow pipelines to</p> <ul> <li>implement the composite pipelines through its Core orchestrator component and UI</li> <li>support MLRun pipelines. </li> </ul> <p>Currently, version v1 of the Kubeflow Pipelines is used for the compatibility purposes. The definition of the KFP workflows is provided in the corresponding KFP Runtime section.</p> <p>How to access</p> <p>Kubeflow Pipelines UI may be accessed from the dashboard. From its interface, you will be able to monitor the deployed workflows and their executions.</p>"},{"location":"components/kubeflow/#resources","title":"Resources","text":""},{"location":"components/kubeflow/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/mlrun/","title":"MLRun","text":"<p>MLRun is a MLOps framework for building and managing machine-learning applications and automating the workflow.</p> <ul> <li>Ingest and transform data</li> <li>Develop ML models, train and deploy them</li> <li>Track performance and detect problems</li> </ul> <p>Currently, the version 1.6.1 is supported by the platform.</p> <p>How to access</p> <p>MLRun may be accessed from the dashboard. From its interface, you will be able to monitor projects and workflows. After launching it from Coder you can access MLRun on MLRun UI (http://nodeipaddress:30060)</p>"},{"location":"components/mlrun/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/nuclio/","title":"Nuclio","text":"<p>Nuclio is a generic purpose Serverless platform for executing and exposing statelss functions as services, run periodic Jobs etc. Nuclio is bundled with MLRun platform but may be used also indipendently offering the management capabilities both via API, CLI, and Web interfaces. It is possible to create serverless applications (deployed as Kubernetes services) using Python, Java, NodeJS, .NET Core, or shell scripts. Nuclio can therefore  be used to</p> <ul> <li>Create and expose arbitrary data services or microservices</li> <li>Expose ML Models as services through MLRun platform</li> <li>Define processors that are triggered periodically (on Cron expression) or with explicit events (e.g., attached to event-based infrastructure, like Kafka).</li> </ul> <p>How to access</p> <p>Nuclio may be accessed from the dashboard or from within MLRun. From its interface, you will be able to monitor and manage serverless functions grouped by projects.</p>"},{"location":"components/nuclio/#resources","title":"Resources","text":""},{"location":"components/nuclio/#-official-documentation","title":"- Official documentation","text":""},{"location":"components/resourcemanager/","title":"Kubernetes Resource Manager","text":"<p>Kubernetes Resource Manager (KRM) is an application to manage several types of Kubernetes resources:</p> <ul> <li>Custom resources</li> <li>Services</li> <li>Deployments</li> <li>Volumes</li> <li>Jobs</li> </ul> <p>It consists in a back-end, written in Java, which connects to the Kubernetes API to perform actions on resources, and a front-end, written in React and based on React-admin.</p> <p>Instructions on how to install and start an instance can be found on the repository.</p>"},{"location":"components/resourcemanager/#standard-kubernetes-resources","title":"Standard Kubernetes Resources","text":"<p>With KRM you can control the main Kubernetes resources (e.g., services, deployments), manage Persistent Volume Claims, and access secrets. Click the corresponding button in the left menu, and view the details of one item by clicking its Show button.</p>"},{"location":"components/resourcemanager/#custom-resources","title":"Custom resources","text":"<p>Custom resources can be viewed, created, edited and deleted through the use of the interface. </p> <p>If you don't see a specific kind of custom resource listed to the left, it means neither Kubernetes nor KRM contain a schema for it. A schema is required so that the application may understand and describe the related resources.</p> <p>If some resources already exist, they will immediately be visible.</p>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"runtimes/container/","title":"Container runtime","text":"<p>The Container runtime allows you to create deployments, jobs and services on Kubernetes.</p>"},{"location":"runtimes/container/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK container runtime documentation for more information.</p>"},{"location":"runtimes/dbt/","title":"DBT runtime","text":"<p>The DBT runtime allows you to run DBT transformations on your data. It is a wrapper around the DBT CLI tool.</p>"},{"location":"runtimes/dbt/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK DBT runtime documentation for more information.</p>"},{"location":"runtimes/kfp_pipelines/","title":"KFP Pipelines Runtime","text":"<p>The kfp runtime allows you to run workflows within the platform.</p>"},{"location":"runtimes/kfp_pipelines/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK KFP runtime documentation for more information.</p>"},{"location":"runtimes/modelserve/","title":"Modelserve runtime","text":"<p>The Modelserve runtime allows you to deploy ML models on the platform.</p>"},{"location":"runtimes/modelserve/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK modelserve runtime documentation for more information.</p>"},{"location":"runtimes/python/","title":"Python","text":"<p>The python runtime allows you to run generic python function within the platform.</p>"},{"location":"runtimes/python/#management-with-sdk","title":"Management with SDK","text":"<p>Check the SDK python runtime documentation for more information.</p>"},{"location":"scenarios/dremio_grafana/scenario/","title":"Data transformation and usage with Dremio and Grafana","text":"<p>In this scenario we will learn how to use Dremio to transform data and create some virtual datasets on top of it. Then we will visualize the transformed data in a dashboard created with Grafana.</p> <p>In order to collect the initial data and make it accessible to Dremio, we will follow the first step of the ETL scenario, in which we download some traffic data and store it in the DigitalHub datalake.</p>"},{"location":"scenarios/dremio_grafana/scenario/#collect-the-data","title":"Collect the data","text":"<p>NOTE: the procedure is only summarized here, as it is already described in depth in the ETL scenario introduction and Collect the data pages.</p> <ol> <li>Access Jupyter from your Coder instance and create a new notebook using the <code>Python 3 (ipykernel)</code> kernel</li> <li>Set up the environment and create a project</li> <li>Set the URL to the data:</li> </ol> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\n</code></pre> <ol> <li>Create the <code>src</code> folder, define the download function and register it</li> <li>Execute it locally and wait for its completion:</li> </ol> <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre>"},{"location":"scenarios/dremio_grafana/scenario/#access-the-data-from-dremio","title":"Access the data from Dremio","text":"<p>Access Dremio from your Coder instance or create a new Dremio workspace. You should see MinIO already configured as an object storage and you should find the downloaded data in a .parquet file at the path <code>minio/datalake/projects/demo-etl/artifacts/download-data-downloader/0/dataset.parquet</code>.</p> <p>Click on the file to open Dataset Settings, verify that the selected format is <code>Parquet</code> and save it as a Dremio dataset, so that it can be queried.</p> <p>Now you can see the data either by clicking again on the dataset or via the SQL runner, by executing a query such as:</p> <pre><code>SELECT *\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\nORDER BY data, \"codice spira\"\n</code></pre> <p>Create a new Dremio space named <code>demo_etl</code>. We will create three virtual datasets and save them here.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-measurement-data","title":"Extract measurement data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic measurements to save them as a separate dataset:</p> <pre><code>SELECT \"dataset.parquet\".data, \"dataset.parquet\".\"codice spira\", \"00:00-01:00\", \"01:00-02:00\", \"02:00-03:00\", \"03:00-04:00\", \"04:00-05:00\", \"05:00-06:00\", \"06:00-07:00\", \"07:00-08:00\", \"08:00-09:00\", \"09:00-10:00\", \"10:00-11:00\", \"11:00-12:00\", \"12:00-13:00\", \"13:00-14:00\", \"14:00-15:00\", \"15:00-16:00\", \"16:00-17:00\", \"17:00-18:00\", \"18:00-19:00\", \"19:00-20:00\", \"20:00-21:00\", \"21:00-22:00\", \"22:00-23:00\", \"23:00-24:00\"\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#extract-traffic-sensors-data","title":"Extract traffic sensors data","text":"<p>Open the SQL runner and execute the following query, which will extract the traffic sensors data (e.g. their geographical position) as a separate dataset:</p> <pre><code>SELECT DISTINCT \"dataset.parquet\".\"codice spira\", \"dataset.parquet\".tipologia, \"dataset.parquet\".id_uni, \"dataset.parquet\".codice, \"dataset.parquet\".Livello, \"dataset.parquet\".\"codice arco\", \"dataset.parquet\".\"codice via\", \"dataset.parquet\".\"Nome via\", \"dataset.parquet\".\"Nodo da\", \"dataset.parquet\".\"Nodo a\", \"dataset.parquet\".stato, \"dataset.parquet\".direzione, \"dataset.parquet\".angolo, \"dataset.parquet\".longitudine, \"dataset.parquet\".latitudine, \"dataset.parquet\".geopoint\nFROM minio.datalake.projects.\"demo-etl\".artifacts.\"download-data-downloader\".\"0\".\"dataset.parquet\"\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>spire</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#transform-hourly-measurements-into-daily-measurements","title":"Transform hourly measurements into daily measurements","text":"<p>Open the SQL runner and execute the following query, which will sum the measurement columns, each corresponding to an hour, to obtain the daily value and save it as a new dataset:</p> <pre><code>SELECT data, \"codice spira\", \"00:00-01:00\"+\"01:00-02:00\"+\"02:00-03:00\"+\"03:00-04:00\"+\"04:00-05:00\"+\"05:00-06:00\"+\"06:00-07:00\"+\"07:00-08:00\"+\"08:00-09:00\"+\"09:00-10:00\"+\"10:00-11:00\"+\"11:00-12:00\"\n+\"12:00-13:00\"+\"13:00-14:00\"+\"14:00-15:00\"+\"15:00-16:00\"+\"16:00-17:00\"+\"17:00-18:00\"+\"18:00-19:00\"+\"19:00-20:00\"+\"20:00-21:00\"+\"21:00-22:00\"+\"22:00-23:00\"+\"23:00-24:00\" AS totale_giornaliero\nFROM (\n  SELECT * FROM \"demo_etl\".misurazioni\n) nested_0;\n</code></pre> <p>Click on the arrow next to Save Script as, select Save View as..., name the new dataset <code>misurazioni_giornaliere</code> and save it in the space <code>demo_etl</code>.</p>"},{"location":"scenarios/dremio_grafana/scenario/#connect-grafana-to-dremio","title":"Connect Grafana to Dremio","text":"<p>Access Grafana from your Coder instance or create a new Grafana workspace. Open the left menu and navigate to Connections - Data Sources. Add a new <code>Dremio</code> data source configured as follows:</p> <ul> <li>Name: <code>Dremio</code></li> <li>URL: the Internal Endpoint you see on Coder for your Dremio workspace</li> <li>User: <code>admin</code></li> <li>Password: <code>&lt;dremio_password_set_on_coder&gt;</code></li> </ul> <p>Now you can create a dashboard to visualize Dremio data.</p> <p>An example dashboard is available as a JSON file at the <code>user/examples/dremio_grafana</code> path within the repository of this documentation. In order to use it, you can import it in Grafana instead of creating a new dashboard. You will need to update the <code>datasource.uid</code> field, which holds a reference to the Dremio data source in your Grafana instance, throughout the JSON model. The easiest way to obtain your ID is by navigating to the data source configuration page and copying it from the URL:</p> <pre><code>https://&lt;grafana_host&gt;/connections/datasources/edit/&lt;YOUR_DATASOURCE_ID&gt;\n</code></pre> <p>The dashboard includes three panels: a map of the traffic sensors, a table with the daily number of vehicles registered by each sensor and a graph of the vehicles registered monthly.</p> <p></p> <p>We can now use the dashboard to explore the data. We can either interact with the map to get the information related to each sensor, or use the dashboard filters to select different time ranges and analyze traffic evolution over time.</p>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in:</p> <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre> <p>Define a function for downloading data as-is and persisting it in the data-lake:</p> <pre><code>%%writefile \"src/download-data.py\"\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef downloader(url):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(file_format='csv',sep=\";\")\n    return df\n</code></pre> <p>Register the function in Core:</p> <pre><code>func = project.new_function(\n                         name=\"download-data\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         code_src=\"src/download-data.py\",\n                         handler=\"downloader\")\n</code></pre> <p>This code creates a new function definition that uses Python runtime (versione 3.9) pointing to the create file and the handler method that should be called.</p> <p>For the function to be executed, we need to pass it a reference to the data item. Let us create and register the corresponding data item:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\ndi= project.new_dataitem(name=\"url_data_item\",kind=\"table\",path=URL)\n</code></pre> <p>It is also possible to see the data item directly in the Core UI.</p> <p>Then, execute the function (locally) as a single job. Note that it may take a few minutes.</p> <pre><code>run = func.run(action=\"job\", inputs={'url':di.key}, outputs={\"dataset\": \"dataset\"}, local_execution=True)\n</code></pre> <p>Note that the <code>inputs</code> map should contain the references to the project entities (e.g., artifacts, dataitems, etc), while in order to pass literal values to the function execution it is necessary to use <code>parameters</code> map.</p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. The name of the artifact will be defined according to the mapping specified in <code>outputs</code> map: it maps the handler outputs (see the <code>@handler</code> annotation and its output definition) to the expected name.</p> <p>To get the value of the artifact we can refer to it by the output name:</p> <pre><code>dataset_di = project.get_dataitem('dataset')\n</code></pre> <p>Load the data item and then into a data frame:</p> <pre><code>dataset_df = dataset_di.as_df()\n</code></pre> <p>Run <code>dataset_df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API:</p> <pre><code>%%writefile 'src/api.py'\n\ndef init_context(context):\n    di = context.project.get_dataitem('dataset-measures')\n    df = di.as_df()\n    setattr(context, \"df\", df)\n\ndef handler(context, event):\n    df = context.df\n\n    if df is None:\n        return \"\"\n\n    # mock REST api\n    method = event.method\n    path = event.path\n    fields = event.fields\n\n    id = False\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields['page'])\n\n    if \"size\" in fields:\n        pageSize = int(fields['size'])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    res = {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n\n    return res\n</code></pre> <p>Register the function:</p> <pre><code>api_func = project.new_function(\n                         name=\"api\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         code_src=\"src/api.py\",\n                         handler=\"handler\",\n                         init_function=\"init_context\")\n</code></pre> <p>Please note that other than defining the handler method, it is possible to define the <code>init_function</code> to define the preparatory steps.</p> <p>Deploy the function (perform <code>serve</code> action):</p> <pre><code>run_serve_model = api_func.run(action=\"serve\")\n</code></pre> <p>Wait till the deployment is complete and the necessary pods and services are up and running.</p> <pre><code>run_serve_model.refresh()\n</code></pre> <p>When done, the status of the run contains the <code>service</code> element with the internal service URL to be used.</p> <pre><code>SERVICE_URL = run_serve_model.status['service']['url']\n</code></pre> <p>Invoke the API and print its results:</p> <pre><code>with requests.get(f'{SERVICE_URL}/?page=5&amp;size=10') as r:\n    res = r.json()\nprint(res)\n</code></pre> <p>You can also use pandas to load the result in a data frame:</p> <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a simple yet realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, are available in the <code>documentation/examples/etl</code> path within the repository of this documentation.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries:</p> <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre> <p>Create a project:</p> <pre><code>PROJECT = \"demo-etl\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name:</p> <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> <p>Download the file and save it locally:</p> <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre> <p>Use pandas to read the file into a dataframe:</p> <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together:</p> <pre><code>%%writefile \"src/pipeline.py\"\n\nfrom digitalhub_runtime_kfp.dsl import pipeline_context\n\ndef pipeline(url):\n    with pipeline_context() as pc:\n        downloader = pc.step(\n            name=\"download-data\",\n            function=\"download-data\",\n            action=\"job\",\n            inputs={\"url\": url},\n            outputs={\"dataset\": \"dataset\"},\n        )\n\n        process_spire = pc.step(\n            name=\"process-spire\",\n            function=\"process-spire\",\n            action=\"job\",\n            inputs={\"di\": downloader.outputs[\"dataset\"]}\n        )\n\n        process_measures = pc.step(\n            name=\"process-measures\",\n            function=\"process-measures\",\n            action=\"job\",\n            inputs={\"di\": downloader.outputs[\"dataset\"]}\n        )\n</code></pre> <p>Here in the definition we use a simple DSL to represent the execution of our functions as steps of the workflow. The DSL <code>step</code> method generates a KFP step that internally makes the remote execution of the corresponding job. Note that the syntax for step is similar to that of function execution.</p> <p>Register the workflow:</p> <pre><code>workflow = project.new_workflow(name=\"pipeline\", kind=\"kfp\", code_src=\"src/pipeline.py\", handler=\"pipeline\")\n</code></pre> <p>And run it, this time remotely, passing the URL key as a parameter:</p> <pre><code>workflow.run(parameters={\"url\": di.key})\n</code></pre> <p>It is possible to monitor the execution in the Core console:</p> <p></p> <p>The next section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store:</p> <pre><code>%%writefile \"src/process-spire.py\"\n\nfrom digitalhub_runtime_python import handler\n\nKEYS=['codice spira','longitudine','latitudine','Livello','tipologia','codice','codice arco','codice via','Nome via', 'stato','direzione','angolo','geopoint']\n\n@handler(outputs=[\"dataset-spire\"])\ndef process(project, di):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n    return sdf\n</code></pre> <p>Register the function in Core:</p> <pre><code>process_func = project.new_function(\n                         name=\"process-spire\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         code_src=\"src/process-spire.py\",\n                         handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_run = process_func.run(action=\"job\", inputs={'di': dataset_di.key}, outputs={'dataset-spire': 'dataset-spire'}, local_execution=True)\n</code></pre> <p>The results has been saved as an artifact in the data store:</p> <pre><code>spire_di = project.get_dataitem('dataset-spire')\nspire_df = spire_di.as_df()\n</code></pre> <p>Now you can view the results with <code>spire_df.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot:</p> <pre><code>keys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = dataset_df[columns]\n</code></pre> <p>Derive dataset for recorded traffic within each time slot for each spire:</p> <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function:</p> <pre><code>%%writefile \"src/process-measures.py\"\n\nfrom digitalhub_runtime_python import handler\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@handler(outputs=[\"dataset-measures\"])\ndef process(project, di):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre> <p>Register it:</p> <pre><code>process_measures_func = project.new_function(\n                         name=\"process-measures\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         code_src=\"src/process-measures.py\",\n                         handler=\"process\")\n</code></pre> <p>Run it locally:</p> <pre><code>process_measures_run = process_measures_func.run(action=\"job\", inputs={'di': dataset_di.key}, outputs={'dataset-measures': 'dataset-measures'}, local_execution=True)\n</code></pre> <p>Inspect the resulting data artifact:</p> <pre><code>measures_di = project.get_dataitem('dataset-measures')\nmeasures_df = measures_di.as_df()\nmeasures_df.head()\n</code></pre> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/etl/streamlit/","title":"Visualize data with Streamlit","text":"<p>We can take this one step further and visualize our data in a graph using Streamlit, a library to create web apps and visualize data by writing simple scripts. Let's get familiar with it.</p>"},{"location":"scenarios/etl/streamlit/#setup","title":"Setup","text":"<p>From the Jupyter notebook you've been using, write the result of the API call to a file:</p> <pre><code>with open(\"result.json\", \"w\") as file:\n    file.write(res['data'])\n</code></pre> <p>Create the script that Streamlit will run:</p> <pre><code>%%writefile 'streamlit-app.py'\n\nimport pandas as pd\nimport streamlit as st\n\nrdf = pd.read_json(\"result.json\", orient=\"records\")\n\n# Replace colons in column names as they can cause issues with Streamlit\nrdf.columns = rdf.columns.str.replace(\":\", \"\")\n\nst.write(\"\"\"My data\"\"\")\nst.line_chart(rdf, x=\"codice spira\", y=\"1200-1300\")\n</code></pre>"},{"location":"scenarios/etl/streamlit/#launch-app","title":"Launch app","text":"<p>In a new code cell, run the following to install Streamlit in the workspace. Note that if you want to run a shell command from Jupyter cell, prepone it with <code>!</code>. If you want to install a package in the workspace, prepone <code>pip install</code> with <code>%</code>.</p> <pre><code>%pip install streamlit\n</code></pre> <p>Similarly, run the following command. This will start hosting the Streamlit web app, so the cell will remain running. The <code>browser.gatherUsageStats</code> flag is set to <code>false</code> because, otherwise, Streamlit will automatically gather usage stats and print a warning about it.</p> <pre><code>!streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Next, go to your Coder instance and access the Jupyter workspace you've been using.</p> <p></p> <p></p> <p>Click on Ports, type <code>8501</code> (Streamlit's default port), then click the button next to it. It will open a tab to the Streamlit app, where you can visualize data!</p> <p></p> <p>The graph we displayed is very simple, but you are welcome to experiment with more Streamlit features. Don't forget to stop the above code cell, to stop the app.</p> <p>Connect to workspace remotely</p> <p>Alternatively to running shell commands from Jupyter and port-forwarding through the Coder interface, you could connect your local shell to the workspace remotely. You do not need to do this if you already used the method above.</p> <p>Login to Coder with the following command. A tab will open in your browser, containing a token you must copy and paste to the shell (it may ask for your credentials, if your browser isn't already logged in).</p> <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> <p>With this, your shell is authenticated to the Coder instance, and the following command will be able to connect your shell to the workspace remotely, while tunneling port 8501:</p> <pre><code>ssh -L 8501:localhost:8501 coder.my-jupyter-workspace\n</code></pre> <p>Install streamlit:</p> <pre><code>pip install streamlit\n</code></pre> <p>Run the app:</p> <pre><code>streamlit run streamlit-app.py --browser.gatherUsageStats false\n</code></pre> <p>Access <code>localhost:8501</code> on your browser to view the app!</p>"},{"location":"scenarios/etl/streamlit/#as-docker-container","title":"As Docker container","text":"<p>Streamlit apps can be run as Docker containers. For this section, we will run the same application locally as a container, so you will need either Docker or Podman installed on your machine. Instructions refer to Docker, but if you prefer to use Podman, commands are equivalent: simply replace instances of <code>docker</code> with <code>podman</code>.</p> <p>Download the <code>result.json</code> file obtained previously on your machine, as we will need its data for the app. Also download the <code>streamlit-app.py</code> file.</p> <p>Create a file named <code>Dockerfile</code> and paste the following contents in it:</p> <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY streamlit-app.py streamlit-app.py\nCOPY result.json result.json\n\nRUN pip3 install altair pandas streamlit\n\nEXPOSE 8501\n\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit-app.py\", \"--browser.gatherUsageStats=false\"]\n</code></pre> <p>The Dockerfile describes how the image for the container will be built. In short, it installs the required libraries, copies the files you downloaded, then launches the Streamlit script.</p> <p>Make sure the three files are in the same directory, then open a shell in it and run the following, which builds the Docker image:</p> <pre><code>docker build -t streamlit .\n</code></pre> <p>Once it's finished, you can verify the image exists with:</p> <pre><code>docker images\n</code></pre> <p>Now, run a container:</p> <pre><code>docker run -p 8501:8501 --name streamlit-app streamlit\n</code></pre> <p>Port already in use</p> <p>If you run into an error, it's likely that you didn't quit the remote session you opened while following the previous section, meaning port 8501 is already in use.</p> <p>Open your browser and visit <code>localhost:8501</code> to view the data!</p> <p>To stop the container, simply press Ctrl+C, then run the following to remove the container:</p> <pre><code>docker rm -f streamlit-app\n</code></pre>"},{"location":"scenarios/etl-core/scenario/","title":"ETL with digitalhub-core and DBT scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding organizations, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>digitalhub-core</code> kernel.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file covering this scenario, as well as files for individual functions, is available in the <code>documentation/examples/etl-core</code> path within the repository of this documentation, or in the path <code>tutorials/08-dbt-demo.ipynb</code> of the Jupyter instance.</p>"},{"location":"scenarios/etl-core/scenario/#setup","title":"Setup","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required library:</p> <pre><code>import digitalhub as dh\n</code></pre> <p>Create a project:</p> <pre><code>project = dh.get_or_create_project(\"project-dbt\")\n</code></pre> <p>Check that the project has been created successfully:</p> <pre><code>print(project)\n</code></pre>"},{"location":"scenarios/etl-core/scenario/#set-data-source","title":"Set data source","text":"<p>The data we will use is available as a CSV file on GitHub. It is a generic sample table of employees. The URL to the data is:</p> <pre><code>url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n</code></pre> <p>We can now create a dataitem to represent the data source that we want to operate transformation on. The DBT runtime will use the dataitem specifications to fetch the data and perform the <code>transform</code> task on it.</p> <p>To create the dataitem, we call the <code>new_dataitem</code> method of the project object. We pass the following mandatory parameters:</p> <pre><code>di = project.new_dataitem(name=\"employees\",\n                          kind=\"table\",\n                          path=url)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the dataitem.</li> <li><code>kind</code> is the type of the dataitem (In this case, <code>table</code>, because our data is a table).</li> <li><code>path</code> is the path to the data source.</li> </ul> <p>Please note that the dataitem is not the data itself, but contains a reference to the data. The dataitem is a Core object that represents the data source, and it is stored in the Core backend. The data itself are (eventually) present on the path specified in the dataitem.</p>"},{"location":"scenarios/etl-core/scenario/#set-up-the-function","title":"Set up the function","text":"<p>We can now set up the function that operates a tranformation on data with DBT. Our function will be an SQL query that selects all the employees of department 60.</p> <pre><code>sql = \"\"\"\nWITH tab AS (\n    SELECT  *\n    FROM    {{ ref('employees') }}\n)\nSELECT  *\nFROM    tab\nWHERE   tab.\"DEPARTMENT_ID\" = '60'\n\"\"\"\n</code></pre> <p>We create the function from the project object:</p> <pre><code>function = project.new_function(name=\"function-dbt\",\n                                kind=\"dbt\",\n                                code=sql)\n</code></pre> <p>The parameters are:</p> <ul> <li><code>name</code> is the identifier of the function.</li> <li><code>kind</code> is the type of the function. Must be <code>dbt</code>.</li> <li><code>code</code> contains the code that is the SQL we'll execute in the function.</li> </ul>"},{"location":"scenarios/etl-core/scenario/#run-the-function","title":"Run the function","text":"<p>We can now run the function and see the results. To do this we use the <code>run</code> method of the function. To the method, we pass:</p> <ul> <li>the task we want to run (in this case, <code>transform</code>)</li> <li>the inputs map the refereced table in the DBT query (<code>{{ ref('employees') }}</code>) to one of our dataitems key. The Runtime will fetch the data and use dem as reference for the query.</li> <li>the output map the output table name. The name of the output table will be <code>department-60</code> and will be the sql query table name result and the output dataitem name.</li> </ul> <pre><code>run = function.run(\"transform\",\n                   inputs={\"employees\": di.key},\n                   outputs={\"output_table\": \"department-60\"})\n</code></pre> <p>We can check the status of the run:</p> <pre><code>print(run.refresh().status)\n</code></pre> <p>If the status says <code>state: RUNNING</code>, wait some time and relaunch the refresh method. Once the function has finished (<code>state</code> should be <code>COMPLETED</code> or <code>ERROR</code>), we can inspect the results:</p> <pre><code>print(run)\n</code></pre> <p>Note that calling <code>run.refresh()</code> will update the run object with the latest information from the Core backend.</p>"},{"location":"scenarios/etl-core/scenario/#explore-the-results","title":"Explore the results","text":"<p>We can now explore the results of the function. We can fetch the output table and explore it with <code>pandas</code>.</p> <pre><code>df = run.output('department-60').as_df()\ndf.head()\n</code></pre>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function: we should define the inference operation and the initialization operation where the model is loaded.</p> <p>Create a model serving function and provide the model: <pre><code>%%writefile \"serve_darts_model.py\"\n\nfrom darts.models import NBEATSModel\nfrom zipfile import ZipFile\nfrom darts import TimeSeries\nimport json\nimport pandas as pd\n\ndef init(context):\n    model_name = \"darts_model\"\n\n    model = context.project.get_model(model_name)\n    path = model.download()\n    local_path_model = \"extracted_model/\"\n\n    with ZipFile(path, 'r') as zip_ref:\n        zip_ref.extractall(local_path_model)\n\n    input_chunk_length = 24\n    output_chunk_length = 12\n    name_model_local = local_path_model +\"predictor_model.pt\"\n    mm = NBEATSModel(\n            input_chunk_length,\n            output_chunk_length\n    ).load(name_model_local)\n\n    setattr(context, \"model\", mm)\n\ndef serve(context, event):\n\n    if isinstance(event.body, bytes):\n        body = json.loads(event.body)\n    else:\n        body = event.body\n    context.logger.info(f\"Received event: {body}\")\n    inference_input = body[\"inference_input\"]\n\n    pdf = pd.DataFrame(inference_input)\n    pdf['date'] = pd.to_datetime(pdf['date'], unit='ms')\n\n    ts = TimeSeries.from_dataframe(\n        pdf,\n        time_col=\"date\",\n        value_cols=\"value\"\n    )\n\n    output_chunk_length = 12\n    result = context.model.predict(n=output_chunk_length*2, series=ts)\n    # Convert the result to a pandas DataFrame, reset the index, and convert to a list\n    jsonstr = result.pd_dataframe().reset_index().to_json(orient='records')\n    return json.loads(jsonstr)\n</code></pre></p> <p>Register it: <pre><code>func = project.new_function(name=\"serve_darts_model\",\n                            kind=\"python\",\n                            python_version=\"PYTHON3_9\",\n                            base_image = \"python:3.9\",\n                            source={\n                                 \"source\": \"serve_darts_model.py\",\n                                 \"handler\": \"serve\",\n                                 \"init_function\": \"init\"},\n                           requirements=[\"darts==0.30.0\"])\n</code></pre></p> <p>Given the dependencies, it is better to have the image ready, using <code>build</code> action of the function: <pre><code>run_build_model_serve = func.run(action=\"build\", instructions=[\"pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\",\"pip3 install darts==0.30.0\"])\n</code></pre></p> <p>Now we can deploy the function: <pre><code>run_serve = func.run(action=\"serve\")\n</code></pre></p> <p>You can now test the endpoint: <pre><code>import requests\nimport json \nfrom datetime import datetime\n\nseries = AirPassengersDataset().load()\nval = series[-24:]\njson_value = json.loads(val.to_json())\n\ndata = map(lambda x, y: {\"value\": x[0], \"date\": datetime.timestamp(datetime.strptime(y, \"%Y-%m-%dT%H:%M:%S.%f\"))*1000}, json_value[\"data\"], json_value[\"index\"])\ninference_input = list(data)\n\nSERVICE_URL = run_serve.refresh().status.to_dict()[\"service\"][\"url\"]\n\nwith requests.post(f'http://{SERVICE_URL}', json={\"inference_input\":inference_input}) as r:\n    res = r.json()\nprint(res)\n</code></pre></p>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance. </p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"Custom ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying generic machine learning applications using the functionalities of the platform. For this purpose, we use ML algorithms for the time series management provided by the Darts framework.</p> <p>The resulting edited notebook, as well as a file for the function we will create, are available in the <code>documentation/examples/ml</code> path within the repository of this documentation.</p> <p>We will train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p>"},{"location":"scenarios/ml/intro/#set-up","title":"Set-up","text":"<p>Let's initialize our working environment. Import required libraries: <pre><code>import digitalhub as dh\nimport pandas as pd\nimport os\n</code></pre></p> <p>Create a project: <pre><code>PROJECT = \"demo-ml\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p>"},{"location":"scenarios/ml/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function. For the sake of simplicity, we use predefined \"Air Passengers\" dataset of Darts.</p> <pre><code>%%writefile \"train-model.py\"\n\n\nfrom digitalhub_runtime_python import handler\n\nimport pandas as pd\nimport numpy as np\n\nfrom darts import TimeSeries\nfrom darts.datasets import AirPassengersDataset\nfrom darts.models import NBEATSModel\nfrom darts.metrics import mape, smape, mae\n\nfrom zipfile import ZipFile\n\n@handler()\ndef train_model(project):\n    series = AirPassengersDataset().load()\n    train, val = series[:-36], series[-36:]\n\n    model = NBEATSModel(\n        input_chunk_length=24,\n        output_chunk_length=12,\n        n_epochs=200,\n        random_state=0\n    )\n    model.fit(train)\n    pred = model.predict(n=36)\n\n    model.save(\"predictor_model.pt\")\n    with ZipFile(\"predictor_model.pt.zip\", \"w\") as z:\n        z.write(\"predictor_model.pt\")\n        z.write(\"predictor_model.pt.ckpt\")\n    metrics = {\n        \"mape\": mape(series, pred),\n        \"smape\": smape(series, pred),\n        \"mae\": mae(series, pred)\n    }\n\n    project.log_model(\n        name=\"darts_model\", \n        kind=\"model\", \n        source=\"predictor_model.pt.zip\", \n        algorithm=\"darts.models.NBEATSModel\",\n        framework=\"darts\",\n        metrics=metrics\n    )\n</code></pre> <p>In this code we create a NBEATS DL model, store it locally zipping the content, extract some metrics, and log the model to the platform with a generic <code>model</code> kind.</p> <p>Let us register it: <pre><code>train_fn = project.new_function(\n     name=\"train-darts\",\n     kind=\"python\",\n     python_version=\"PYTHON3_9\",\n     source={\"source\": \"train-model.py\", \"handler\": \"train_model\"},\n     requirements=[\"darts==0.30.0\"])\n</code></pre></p> <p>and run it locally: <pre><code>train_run = train_fn.run(action=\"job\", local_execution=True)\n</code></pre></p> <p>If we want to run the function on Kubernetes, it is better to build it first as there are specific custom dependencies. <pre><code>build_run = train_fn.run(action=\"build\", local_execution=False)\n</code></pre></p> <p>In this way the function image will be created and associated with the function.</p> <p>As a result of train execution, a new model is registered in the Core and may be used by different inference operations.</p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlllm/llm/","title":"Managing LLM Models","text":"<p>With the platform it is possible to create and serve LLM HuggingFace-compatible-models. Specifically, it is possible to serve directly the LLM models from the HuggingFace catalogue provided the id of the model or to serve the fine-tuned model from the specified path, such as S3.</p> <p>LLM implementation relies on the KServe LLM runtime and therefore supports one of the corresponding LLM tasks:</p> <ul> <li>Text Generation</li> <li>Text2Text Generation </li> <li>Fill Mask</li> <li>Text (Sequence) Classification</li> <li>Token Classification</li> </ul> <p>Based on the type of the task the API of the exposed service may differ. Generative models (text generation and text2text generation) use OpenAI's Completion and Chat Completion API. </p> <p>The other types of tasks like token classification, sequence classification, fill mask are served using KServe's Open Inference Protocol v2 API.</p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-classification-models","title":"Exposing Predefined Text Classification Models","text":"<p>In case of predefined HuggingFace non-generative model it is possible to use <code>huggingfaceserve</code> runtime to expose the corresponding inference  API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english</code>. </p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Create a project to host the functions and executions</p> <pre><code>PROJECT = \"llm\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm_classification\", \n                                   kind=\"huggingfaceserve\",\n                                   model_name=\"mymodel\",\n                                   path=\"huggingface://distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n                                  )\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"template-a100\")\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls:</p> <pre><code>SERVICE_URL = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nMODEL_NAME = \"mymodel\"\n\nwith requests.post(f'http://{SERVICE_URL}/v2/models/{MODEL_NAME}/infer', json={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [2],\n        \"datatype\": \"BYTES\",\n        \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"]\n        }\n    ]\n}) as r:\n    res = r.json()\nprint(res)\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    'model_name': 'mymodel', \n    'model_version': None, \n    'id': 'cab30aa5-c10f-4233-94e2-14e4bc8fbf6f', \n    'parameters': None, \n    'outputs': [\n        {'name': 'output-0', 'shape': [2], 'datatype': 'INT64', 'parameters': None, 'data': [1, 0]}\n        ]}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality. </p>"},{"location":"scenarios/mlllm/llm/#exposing-predefined-text-generation-models","title":"Exposing Predefined Text Generation Models","text":"<p>In case of predefined HuggingFace ngenerative model it is possible to use <code>huggingfaceserve</code> runtime to expose the OpenAI compatible API. For this purpose it is necessary to define the <code>huggingfaceserve</code> function definition (via UI or SDK) providing the name of the exposed model and the URI of the model in the following form</p> <p><code>huggingface://&lt;id of the huggingface model&gt;</code></p> <p>For example <code>huggingface://meta-llama/meta-llama-3-8b-instruct</code>. </p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Create a project to host the functions and executions</p> <pre><code>PROJECT = \"llm\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm_generation\", \n                                   kind=\"huggingfaceserve\",\n                                   model_name=\"mymodel\",\n                                   path=\"huggingface://meta-llama/meta-llama-3-8b-instruct\"\n                                  )\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"template-a100\")\n</code></pre> <p>Please note that in case of protected models (like, e.g., llama models) it is necessary to path the HuggingFace token. For example,</p> <pre><code>llm_run = llm_function.run(action=\"serve\", \n                           profile=\"template-a100\", \n                           envs = [{\n                                \"name\": \"HF_TOKEN\",\n                                \"value\": \"&lt;HUGGINGFACE TOKEN&gt;\"\n                            }]\n                          )\n</code></pre> <p>As in case of classification models, the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls. For example, for the completion requests:</p> <pre><code>SERVICE_URL = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nMODEL_NAME = \"mymodel\"\n\nwith requests.post(f'http://{SERVICE_URL}/openai/v1/completions', json={\n    \"model\": MODEL_NAME, \"prompt\": \"Hello! How are you?\", \"stream\": False, \"max_tokens\": 30\n    }) as r:\n    res = r.json()\nprint(res)\n</code></pre> <p>Here the expected output should have the following form:</p> <pre><code>{\n  \"id\": \"cmpl-625a9240f25e463487a9b6c53cbed080\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"text\": \" and how they make you feel\\nColors, oh colors, so vibrant and bright\\nA world of emotions, a kaleidoscope in sight\\nRed\"\n    }\n  ],\n  \"created\": 1718620153,\n  \"model\": \"mymodel\",\n  \"system_fingerprint\": null,\n  \"object\": \"text_completion\",\n  \"usage\": {\n    \"completion_tokens\": 30,\n    \"prompt_tokens\": 6,\n    \"total_tokens\": 36\n  }\n}\n</code></pre> <p>In case of chat requests:</p> <pre><code>with requests.post(f'http://{SERVICE_URL}/openai/v1/chat/completions', json={\n    \"model\": MODEL_NAME, \n    \"messages\":[\n        {\"role\":\"system\",\"content\":\"You are an assistant that speaks like Shakespeare.\"},\n        {\"role\":\"user\",\"content\":\"Write a poem about colors\"}\n    ], \n    \"max_tokens\":30,\n    \"stream\": False}) as r:\n    res = r.json()\nprint(res)\n</code></pre> <p>Expected output:</p> <pre><code> {\n   \"id\": \"cmpl-9aad539128294069bf1e406a5cba03d3\",\n   \"choices\": [\n     {\n       \"finish_reason\": \"length\",\n       \"index\": 0,\n       \"message\": {\n         \"content\": \"  O, fair and vibrant colors, how ye doth delight\\nIn the world around us, with thy hues so bright!\\n\",\n         \"tool_calls\": null,\n         \"role\": \"assistant\",\n         \"function_call\": null\n       },\n       \"logprobs\": null\n     }\n   ],\n   \"created\": 1718638005,\n   \"model\": \"mymodel\",\n   \"system_fingerprint\": null,\n   \"object\": \"chat.completion\",\n   \"usage\": {\n     \"completion_tokens\": 30,\n     \"prompt_tokens\": 37,\n     \"total_tokens\": 67\n   }\n }    \n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality. </p>"},{"location":"scenarios/mlllm/llm/#fine-tuned-llm-model","title":"Fine-tuned LLM model","text":"<p>when it comes to custom LLM model, it is possible to create HuggingFace-based fine tuned model, log it and then serve it from the model path.</p> <p>When using SDK, this may be accomplished as follows.</p> <p>First, import necessary libraries <pre><code>import digitalhub as dh\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Create a project to host the functions and executions</p> <pre><code>PROJECT = \"llm\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre> <p>Create the training procedure that logs model to the platform:</p> <pre><code>%%writefile \"src/train_model.py\"\n\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\nimport numpy as np\nimport evaluate\nimport os\n\nfrom digitalhub_runtime_python import handler\n\n@handler()\ndef train(project):\n    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n    metric = evaluate.load(\"accuracy\")\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n\n    dataset = load_dataset(\"yelp_review_full\")\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\")\n\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\n    training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=small_train_dataset,\n        eval_dataset=small_eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\n    save_model = \"model\"\n    if not os.path.exists(save_model):\n        os.makedirs(save_model)\n\n    save_dir = \"model\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    trainer.save_model(save_dir)\n    tokenizer.save_pretrained(save_dir)\n\n    project.log_model(\n            name=\"test_llm_model\", \n            kind=\"huggingface\", \n            base_model=\"google-bert/bert-base-cased\",\n            source=save_dir\n    )    \n</code></pre> <p>Register the function and execute it:</p> <pre><code>train_func = project.new_function(\n                         name=\"train_model\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         code_src=\"src/train_model.py\",\n                         handler=\"train\",\n                         requirements=[\"evaluate\", \"transformers[torch]\", \"torch\", \"torchvision\", \"accelerate\"]\n                        )\n\ntrain_run=train_func.run(action=\"job\",  local_execution=False, profile=\"template-a100\")                        \n</code></pre> <p>Create the serving function definition:</p> <pre><code>llm_function = project.new_function(\"llm_classification\", \n                                   kind=\"huggingfaceserve\",\n                                   model_name=\"mymodel\",\n                                   path=\"s3://datalake/llm/model/test_llm_model/f8026820-2471-4497-97f5-8e6d49baac5f/\"\n                                  )\n</code></pre> <p>Serve the model:</p> <pre><code>llm_run = llm_function.run(action=\"serve\", profile=\"template-a100\")\n</code></pre> <p>Please note the use of the <code>profile</code> parameter. As the LLM models require specific hardware (GPU in particular), it is necessary to specify the HW requirements as described in the  Configuring Kubernetes executions section. In particular, it is possible to rely on the predefined resource templates of the platform deployment.</p> <p>Once the service becomes available, it is possible to make the calls:</p> <pre><code>SERVICE_URL = llm_run.refresh().status.to_dict()[\"service\"][\"url\"]\nMODEL_NAME = \"mymodel\"\n\nwith requests.post(f'http://{SERVICE_URL}/v2/models/{MODEL_NAME}/infer', json={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [2],\n        \"datatype\": \"BYTES\",\n        \"data\": [\"Hello, my dog is cute\", \"I am feeling sad\"]\n        }\n    ]\n}) as r:\n    res = r.json()\nprint(res)\n</code></pre> <p>Here the classification LLM service API follows the Open Inference protocol API and the expected result should have the following form:</p> <pre><code>{\n    'model_name': 'mymodel', \n    'model_version': None, \n    'id': 'cab30aa5-c10f-4233-94e2-14e4bc8fbf6f', \n    'parameters': None, \n    'outputs': [\n        {'name': 'output-0', 'shape': [2], 'datatype': 'INT64', 'parameters': None, 'data': [4, 0]}\n        ]}\n</code></pre> <p>As in case of other services (ML model services or Serverless functions), it is possible to expose the service using the KRM API gateway functionality. </p>"},{"location":"scenarios/mlmlflow/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a MLFLow model is easy: <code>mlflowserve</code> runtime supports this functionality out of the box. It is sufficient to specify the path to the model artifact and optionally the name of the model to expose.</p> <p>It is important to note that the path should point to the folder, where the MLFlow <code>MLModel</code> artifact is placed. If the model  is created from MLFlow run artifact path, besides the <code>model</code> folder it may contain additional artifacts.</p> <p>Register it and deploy: <pre><code>func = project.new_function(name=\"serve_mlflowmodel\",\n                            kind=\"mlflowserve\",\n                            model_name=\"testmodel\",\n                            path=model.spec.path + 'model')\n\nserve_run = func.run(action=\"serve\")\n</code></pre></p> <p>You can now test the endpoint (using e.g., the subset of data): <pre><code>import requests\nfrom sklearn import datasets\n\nSERVICE_URL = serve_run.refresh().status.to_dict()[\"service\"][\"url\"]\nMODEL_NAME = \"testmodel\"\n\niris = datasets.load_iris()\ntest_input = iris.data[0:2].tolist()\n\nwith requests.post(f'http://{SERVICE_URL}/v2/models/{MODEL_NAME}/infer', json={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [2, 4],\n        \"datatype\": \"FP64\",\n        \"data\": test_input\n        }\n    ]\n}) as r:\n    res = r.json()\nprint(res)\n</code></pre></p> <p>Please note that the MLFLow model serving exposes also the Open API specification under <code>/v2/docs</code> path.</p>"},{"location":"scenarios/mlmlflow/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance. </p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlmlflow/intro/","title":"MLFLow ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a machine learning application based on model tracked with MLFlow framework using the functionalities of the platform.</p> <p>The resulting edited notebook, as well as a file for the function we will create, are available in the <code>documentation/examples/mlmlflow</code> path within the repository of this documentation.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p>"},{"location":"scenarios/mlmlflow/intro/#set-up","title":"Set-up","text":"<p>Install the necessary libraries: <pre><code>%pip install mlflow scikit-learn==1.5.0\n</code></pre></p> <p>Let's initialize our working environment. Import required libraries: <pre><code>import digitalhub as dh\nimport pandas as pd\nimport os\n</code></pre></p> <p>Create a project: <pre><code>PROJECT = \"demo-ml\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p>"},{"location":"scenarios/mlmlflow/intro/#generate-data","title":"Generate data","text":"<p>For the sake of simplicity, we use the predefined IRIS dataset.</p>"},{"location":"scenarios/mlmlflow/intro/#training-the-model","title":"Training the model","text":"<p>Let us define the training function. </p> <pre><code>%%writefile train-model.py\n\nfrom digitalhub_runtime_python import handler\n\nfrom digitalhub_ml.entities.utils import from_mlflow_run\nimport mlflow\n\nfrom sklearn import datasets, svm\nfrom sklearn.model_selection import GridSearchCV\n\n@handler()\ndef train(project):\n    mlflow.sklearn.autolog(log_datasets=True)\n\n    iris = datasets.load_iris()\n    parameters = {\"kernel\": (\"linear\", \"rbf\"), \"C\": [1, 10]}\n    svc = svm.SVC()\n    clf = GridSearchCV(svc, parameters)\n\n    clf.fit(iris.data, iris.target)\n    run_id = mlflow.last_active_run().info.run_id\n\n    # utility to map mlflow run artifacts to model metadata\n    model_params = from_mlflow_run(run_id)\n\n    project.log_model(\n        name=\"model-mlflow\",\n        kind=\"mlflow\",\n        **model_params\n)\n</code></pre> <p>The function creates an SVC model with the scikit-learn framework. Note that here we use the autologging functionality of MLFlow and then construct the necessary model metadata out of the tracked MLFlow model. Specifically, MLFlow creates a series of artifacts that describe the model and the corresponding model files, as well as additional files representing the model properties and metrics.</p> <p>We then log the model of <code>mlflow</code> kind using the extract metadata as kwargs.  </p> <p>Let us register it: <pre><code>train_fn = project.new_function(\n                         name=\"train\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"train-model.py\", \"handler\": \"train\"},\n                         requirements=[\"scikit-learn==1.5.0\", \"mlflow==2.15.1\"])\n</code></pre></p> <p>and run it locally: <pre><code>train_run = train_fn.run(action=\"job\", local_execution=True)\n</code></pre></p> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = project.get_model(\"model-mlflow\")\nmodel.spec.path\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/mlsklearn/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a scikit-learn model is easy: <code>sklearnserve</code> runtime supports this functionality out of the box. It is sufficient to specify the path to the model artifact and optionally the name of the model to expose.</p> <p>Register it and deploy: <pre><code>func = project.new_function(name=\"serve_sklearnmodel\",\n                            kind=\"sklearnserve\",\n                            model_name=\"testmodel\",\n                            path=model.spec.path)\n\nserve_run = func.run(action=\"serve\")\n</code></pre></p> <p>You can now test the endpoint (using e.g., X_test): <pre><code>import requests\n\nSERVICE_URL = serve_run.refresh().status.to_dict()[\"service\"][\"url\"]\nMODEL_NAME = \"testmodel\"\n\ntest_input = X_test.head(2).to_numpy().tolist()\n\nwith requests.post(f'http://{SERVICE_URL}/v2/models/{MODEL_NAME}/infer', json={\n    \"inputs\": [\n        {\n        \"name\": \"input-0\",\n        \"shape\": [2, 30],\n        \"datatype\": \"FP32\",\n        \"data\": test_input\n        }\n    ]\n}) as r:\n    res = r.json()\nprint(res)\n</code></pre></p> <p>Please note that the scikit-learn model serving exposes also the Open API specification under <code>/docs</code> path.</p>"},{"location":"scenarios/mlsklearn/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to the Kubernetes Resource Manager component (available from dashboard) and go to the API Gateways section. To expose a service it is necessary to define</p> <ul> <li>name of the gateway</li> <li>the service to expose</li> <li>the endpoint where to publish</li> <li>and the authentication method (right now only no authentication or basic authentication are available). in case of basic authentication it is necessary to specify  Username and Password.</li> </ul> <p>The platform by default support exposing the methods at the subdomains of <code>services.&lt;platform-domain&gt;</code>, where platform-domain is the domain of the platform instance. </p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you defined! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/mlsklearn/intro/","title":"Scikit-learn ML scenario introduction","text":"<p>This scenario provides a quick overview of developing and deploying a scikit-learn machine learning application using the functionalities of the platform.</p> <p>The resulting edited notebook, as well as a file for the function we will create, are available in the <code>documentation/examples/mlsklearn</code> path within the repository of this documentation.</p> <p>We will prepare data, train a generic model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p>"},{"location":"scenarios/mlsklearn/intro/#set-up","title":"Set-up","text":"<p>Let's initialize our working environment. Import required libraries: <pre><code>import digitalhub as dh\nimport pandas as pd\nimport os\n</code></pre></p> <p>Create a project: <pre><code>PROJECT = \"demo-ml\"\nproject = dh.get_or_create_project(PROJECT)\n</code></pre></p>"},{"location":"scenarios/mlsklearn/intro/#generate-data","title":"Generate data","text":"<p>Define the following function, which generates the dataset as required by the model: <pre><code>%%writefile data-prep.py\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nfrom digitalhub_runtime_python import handler\n\n@handler(outputs=[\"dataset\"])\ndef breast_cancer_generator():\n    \"\"\"\n    A function which generates the breast cancer dataset\n    \"\"\"\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(\n        data=breast_cancer.data, columns=breast_cancer.feature_names\n    )\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"target\"])\n    breast_cancer_dataset = pd.concat(\n        [breast_cancer_dataset, breast_cancer_labels], axis=1\n    )\n\n    return breast_cancer_dataset\n</code></pre></p> <p>Register it: <pre><code>data_gen_fn = project.new_function(\n                         name=\"data-prep\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"data-prep.py\", \"handler\": \"breast_cancer_generator\"})\n</code></pre></p> <p>Run it locally: <pre><code>gen_data_run = data_gen_fn.run(action=\"job\", outputs={\"dataset\": \"dataset\"}, local_execution=True)\n</code></pre></p> <p>You can view the state of the execution with <code>gen_data_run.status</code> or its output with <code>gen_data_run.outputs()</code>. You can see a few records from the output artifact: <pre><code>gen_data_run.outputs()[\"dataset\"].as_df().head()\n</code></pre></p> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/mlsklearn/training/","title":"Training the model","text":"<p>Let us define the training function. </p> <pre><code>%%writefile train-model.py\n\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nfrom digitalhub_runtime_python import handler\nfrom sklearn.svm import SVC \nfrom pickle import dump\nimport sklearn.metrics\nimport os\n\n@handler(outputs=[\"dataset\"])\ndef train(project, di):\n\n    df_cancer = di.as_df()\n    X = df_cancer.drop(['target'],axis=1)\n    y = df_cancer['target']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=5)\n    svc_model = SVC()\n    svc_model.fit(X_train, y_train)\n    y_predict = svc_model.predict(X_test)\n\n    if not os.path.exists(\"model\"):\n        os.makedirs(\"model\")\n\n    with open(\"model/cancer_classifier.pkl\", \"wb\") as f:\n        dump(svc_model, f, protocol=5)\n\n    metrics = {\n        \"f1_score\": sklearn.metrics.f1_score(y_test, y_predict),\n        \"accuracy\": sklearn.metrics.accuracy_score(y_test, y_predict),\n        \"precision\": sklearn.metrics.precision_score(y_test, y_predict),\n        \"recall\": sklearn.metrics.recall_score(y_test, y_predict),\n    }\n    project.log_model(\n            name=\"cancer_classifier\", \n            kind=\"sklearn\", \n            source=\"./model/\", \n            metrics=metrics\n    )\n</code></pre> <p>The function takes the analysis dataset as input, creates an SVC model with the scikit-learn framework and logs the model with its metrics.</p> <p>Let us register it: <pre><code>train_fn = project.new_function(\n                         name=\"train\",\n                         kind=\"python\",\n                         python_version=\"PYTHON3_9\",\n                         source={\"source\": \"train-model.py\", \"handler\": \"train\"},\n                         requirements=[\"scikit-learn==1.2.2\"])\n</code></pre></p> <p>and run it locally: <pre><code>train_run = train_fn.run(action=\"job\", inputs={\"di\": gen_data_run.outputs()[\"dataset\"].key}, local_execution=False)\n</code></pre></p> <p>As a result, a new model is registered in the Core and may be used by different inference operations:</p> <pre><code>model = project.get_model(\"cancer_classifier\")\nmodel.spec.path\n</code></pre> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"scenarios/postgrest/apigw/","title":"Expose service externally","text":"<p>Finally, we make the PostgREST service available externally. Access API Gateways on the left menu and click <code>CREATE</code>.</p> <p>Fill the fields as follows:</p> <ul> <li>Name: name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Service: select the one referring to the PostgREST service you created. Its name may be something like <code>postgrest-my-postgrest</code>. Port will automatically be filled.</li> <li>Host: full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>pgrest.services.example.com</code>.</li> <li>Path: Leave the default <code>/</code>.</li> <li>Authentication: <code>None</code>.</li> </ul> <p></p> <p>Save and the API gateway will be created. You can try a simple query like the following, even in your browser, to view its results (remember to change domain according to your case): <pre><code>https://pgrest.services.example.com/readings?limit=3\n</code></pre></p>"},{"location":"scenarios/postgrest/data/","title":"Insert data into the database","text":"<p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>Open a new notebook using the <code>Python 3 (ipykernel)</code> kernel.</p> <p>We will now insert some data into the database we created earlier. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p> <p>The notebook file is available in the <code>documentation/examples/postgrest</code> path within the repository of this documentation.</p> <p>Import required libraries: <pre><code>import os\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport requests\n</code></pre></p> <p>Connect to the database. You will need the value of POSTGRES_URL you got from the owner's secret in the first stage of the scenario. <pre><code>engine = create_engine('postgresql://owner-UrN9ct:88aX8tLFJ95qYU7@database-postgres-cluster/mydb')\n</code></pre></p> <p>Download a CSV file and parse it (may take a few minutes): <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n\nwith requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n\ndf = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>The following will create a table and insert the dataframe into it. If it fails, resources allocated to the Jupyter workspace may be insufficient. The table will be created automatically, or replaced if it already exists. <pre><code>df.to_sql(\"readings\", engine, if_exists=\"replace\")\n</code></pre></p> <p>Run a test select query to check data has been successfully inserted: <pre><code>select = \"SELECT * FROM readings LIMIT 3\"\nselect_df = pd.read_sql(select,con=engine)\nselect_df.head()\n</code></pre></p> <p>If everything went right, a few rows are returned. We will now create a PostgREST service to expose this data via a REST API.</p>"},{"location":"scenarios/postgrest/intro/","title":"PostgREST scenario introduction","text":"<p>In this scenario, we download some data into a Postgres database, then use PostgREST - a tool to make Postgres tables accessible via REST API - to expose this data and run a simple request to view the results.</p>"},{"location":"scenarios/postgrest/intro/#database-set-up","title":"Database set-up","text":"<p>Let's start by setting up the database. Access your KRM instance and Postgres DBs on the left menu, then click Create.</p> <ul> <li><code>Name</code>: This is just an identifier for Kubernetes. Type <code>my-db</code>.</li> <li><code>Database</code>: The actual name on the database. Type <code>mydb</code>.</li> <li>Toggle on <code>Drop on delete</code>, which conveniently deletes the database when you delete the custom resource.</li> </ul> <p></p> <p>Click Save. You should now see your database listed.</p>"},{"location":"scenarios/postgrest/intro/#add-users-to-database","title":"Add users to database","text":"<p>PostgREST needs a user to authenticate and a user to assume its role when the API is called. These can be separate users, but for our purposes, we will create just one to fill both roles. Click Show on your database's entry and then Add user on the bottom. Enter values as follows:</p> <ul> <li><code>Name</code>: An identifier for Kubernetes. Type <code>db-owner</code>.</li> <li><code>Role</code>: The actual name on the database. Type <code>owner</code>.</li> <li><code>Privileges</code>: Indicates what privileges the user will have. Pick <code>Owner</code>.</li> <li><code>Secret name</code>: The secret to authenticate the user. Type <code>owner</code>.</li> </ul> <p></p>"},{"location":"scenarios/postgrest/intro/#retrieve-postgres_url","title":"Retrieve POSTGRES_URL","text":"<p>Together with the user, a secret has also been created. Go to Secrets on the left menu; the list should contain a secret with a name referring the user you created. Find it and click Show.</p> <p>Write down the following information somewhere, as we will need it later: - Name of the secret - Value of <code>POSTGRES_URL</code> (click on Decode to obtain it) - Value of <code>ROLE</code></p> <p></p>"},{"location":"scenarios/postgrest/postgrest/","title":"Create PostgREST service","text":"<p>We will go back to KRM to create a PostgREST service and expose the database's table via API.</p>"},{"location":"scenarios/postgrest/postgrest/#inspect-users-secrets","title":"Inspect users' secrets","text":"<p>We need some parameters to configure PostgREST. Similarly to what you did in the first stage of the scenario, go to Secrets on the left and look for the two secrets, belonging to the owner and reader users you created.</p> <p>For the owner, you will need the Name of the secret. For the reader, you will need the content of the ROLE field.</p>"},{"location":"scenarios/postgrest/postgrest/#creating-the-postgrest-service","title":"Creating the PostgREST service","text":"<p>Click PostgREST Data Services on the left and then Create.</p> <p>Fill the first few fields as follows:</p> <ul> <li><code>Name</code>: Anything you'd like, it's once again an identifier for Kubernetes.</li> <li><code>Schema</code>: <code>public</code></li> <li>Toggle on <code>With existing DB user</code>.</li> <li><code>Existing DB user name</code>: Value of ROLE in the owner's secret.</li> </ul> <p>Under Connection, fill the fields as follows:</p> <ul> <li><code>Secret name</code>: Name of the owner's secret.</li> </ul> <p>When you hit Save, the PostgREST instance will be launched.</p> <p></p>"},{"location":"sdk/intro/","title":"Python SDK Reference","text":"<p>Python SDK provides the functionality of the Core API and allows for working with the platform entities directly from interactive workspaces or your prefered development environment.</p> <p>See here the SDK documentation (refer to the version corresponding to the version of the platform):</p> <ul> <li>SDK Reference</li> </ul>"},{"location":"tasks/data/","title":"Data and transformations","text":"<p>The platform supports data of different types to be stored and operated by the underlying storage subsystems.</p> <p>Specifically, the platform natively supports two types of storages:</p> <ul> <li>persistence object storage (datalake S3 Minio), which manages immutable data in the form of files.</li> <li>operational relational data storage (PostgreSQL database), which is used for efficient querying of mutable data. Postgres is rich with extensions, most notably for geo-spatial and time-series data.</li> </ul> <p>The data is represented in the platform as entities of different types, depending on its usage and format. More specifically, we distinguish:</p> <ul> <li>data items, which represent immutable data sets resulting from different transformation operations and are ready for use in differerent types of analysis. Data items are enriched with metadata (versions, lineage, stats, profiling, schema, ...) and unique keys and managed and persisted to the datalake directly by the platform in the form of Parquet files. It is possible to treat tabular data (items of <code>table</code> kind) as, for example, DataFrames, using conventional libraries.</li> <li>artifacts, which represent arbitrary files, not limited to tabular format, stored to the datalake with some extra metadata.</li> </ul> <p>Each data entity may be accessed and manipulated by the platform via UI or using the API, for example with SDK.</p>"},{"location":"tasks/data/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/data/#artifacts","title":"Artifacts","text":"<p>Artifacts can be managed as entities from the UI. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new artifact</li> <li><code>filter</code> artifacts by name and kind</li> <li><code>expand</code> an artifact to see its 5 latest versions</li> <li><code>show</code> the details of an artifact</li> <li><code>edit</code> an artifact</li> <li><code>delete</code> an artifact</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete artifacts using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the artifact</li> <li><code>Kind</code>: kind of the artifact</li> <li>(Spec) <code>Path</code>: remote path where the artifact is stored. If you instead upload the artifact at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later.</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description of the artifact</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the artifact</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path to the artifact, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view an artifact's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update","title":"Update","text":"<p>You can update an artifact by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete","title":"Delete","text":"<p>You can delete an artifact from either its detail page or the list of artifacts, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#data-items","title":"Data items","text":"<p>Data items can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new data item</li> <li><code>expand</code> a data item and see its 5 latest versions</li> <li><code>show</code> the details of a data item</li> <li><code>edit</code> a data item</li> <li><code>delete</code> a data item</li> <li><code>filter</code> data items by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete data items using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/data/#create_1","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name of the dataitem</li> <li><code>Kind</code>: kind of the dataitem</li> <li>(Spec) <code>Path</code>: remote path where the data item is stored. If you instead upload the data item at the bottom of the form, this will be the path to where it will be stored.</li> </ul> <p>Other fields are optional and may be updated later:</p> <ul> <li>(Metadata) <code>Description</code>: a human-readable description</li> <li>(Metadata) <code>Labels</code>: list of labels</li> <li>(Metadata) <code>Name</code>: name of the data item</li> <li>(Metadata) <code>Embedded</code>: flag for embedded metadata</li> <li>(Metadata) <code>Versioning</code>: version of the function</li> <li>(Metadata) <code>Openmetadata</code>: flag to publish metadata</li> <li>(Metadata) <code>Audit</code>: author of creation and modification</li> <li>(Spec) <code>Source path</code>: local path of the data item, used in case of upload into remote storage</li> </ul>"},{"location":"tasks/data/#kind","title":"Kind","text":"<p>There are 2 possible kinds for dataitems:</p> <ul> <li><code>Dataitem</code>: indicates it is a generic data item. There are no specific attributes in the creation page.</li> <li><code>table</code>: indicates that the data item points to a table. The optional parameter is the schema of the table in table_schema format.</li> </ul>"},{"location":"tasks/data/#read_1","title":"Read","text":"<p>Click <code>SHOW</code> to view a data item's details.</p> <p></p> <p>Based on the <code>kind</code>, there may be a <code>schema</code>, indicating that the dataitem point to a table.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/data/#update_1","title":"Update","text":"<p>You can update a data item by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/data/#delete_1","title":"Delete","text":"<p>You can delete a data item from either its detail page or the list of data items, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/data/#management-via-sdk","title":"Management via SDK","text":""},{"location":"tasks/data/#managing-artifacts-with-sdk","title":"Managing artifacts with SDK","text":"<p>Artifacts can be created and managed as entities with the SDK CRUD methods. Check the SDK Artifacts documentation for more information.</p>"},{"location":"tasks/data/#managing-dataitems-with-sdk","title":"Managing dataitems with SDK","text":"<p>Dataitems can be created and managed as entities with the SDK CRUD methods. Check the SDK Dataitem documentation for more information.</p>"},{"location":"tasks/functions/","title":"Functions and Runtimes","text":"<p>Functions are the logical description of something that the platform may execute and track for you. A function may represent code to run as a job, an ML model inference to be used as batch procedure or as a service, a data validation, etc.</p> <p>In the platform we perform actions over functions (also referred to as \"tasks\"), such as job execution, deploy, container image build. A single action execution is called run, and the platform keeps track of these runs, with metadata about function version, operation parameters, and runtime parameters for a single execution.</p> <p>They are associated with a given runtime, which implements the actual execution and determines which actions are available. Examples are DBT, Container, Python, etc. Runtimes  are highly specialized components which can translate the representation of a given execution, as expressed in the run, into an actual execution operation performed via libraries, code, external tools etc.</p> <p>Runtimes define the key point of extension of the platform: new runtimes may be added in order to implement the low-level logic of \"translating\" the high level operation definition into an executable run. For example, DBT runtime allows for defining the transformation as a task that, given the input table reference, produces a datastt appyling the function defined as SQL code. The runtime in this case is responsible for converting the specification and the references to a dedicated Kubernetes Job that runs DBT transformation and stores the corresponding dataset.</p> <p>The set of the supported runtimes is documented in Runtimes References section. Independently of the specific runtime implementation, the flow of actions with respect to the function definition and execution is the following:</p> <ul> <li>define a new function providing its name, runtime, definition (e.g., source code), and configuration (e.g., dependencies). The function definition is saved by the project. Each change to the function spec creates a new function version so that the executions of different function versions are independently tracked.</li> <li>execute a task over the function providing the configuration of the task (e.g., the K8S resources needed for the execution), the execution parameters and inputs (if any). This creates a new task specification and a new run instance tracked by the platform.</li> </ul> <p>The definition and execution of the functions may be performed either via UI or via Python SDK.</p>"},{"location":"tasks/functions/#management-via-ui","title":"Management via UI","text":""},{"location":"tasks/functions/#functions","title":"Functions","text":"<p>Functions can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new function</li> <li><code>expand</code> a function to see its 5 latest versions</li> <li><code>show</code> the details of a function</li> <li><code>edit</code> a function</li> <li><code>delete</code> a function</li> <li><code>filter</code> functions by name and kind</li> </ul> <p></p> <p>Here we analyze how to create, read, update and delete functions using the UI, similarly to what can be done through the SDK.</p>"},{"location":"tasks/functions/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the function</li> <li><code>Kind</code>: kind of function</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p><code>Spec</code> fields will change depending on the function's kind.</p>"},{"location":"tasks/functions/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a function's details.</p> <p></p> <p>Tabs next to <code>SUMMARY</code> will change depending on the function's <code>kind</code>. Some of them allow you to create runs, but we will see this in a later section.</p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/functions/#update","title":"Update","text":"<p>You can update a function by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/functions/#delete","title":"Delete","text":"<p>You can delete a function from either its detail page or the list of functions, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/functions/#runs","title":"Runs","text":""},{"location":"tasks/functions/#create_1","title":"Create","text":"<p>A run represents the execution of a task through a function. As such, the starting point to create a run is the function it is based on. Select one of the functions you created. You will notice multiple tabs at the top, next to <code>SUMMARY</code>. These tabs may differ depending on the function's <code>kind</code>.</p> <p></p> <p>Click <code>CREATE</code> to create a new run. You will start a 3-steps process to create a run.</p> <p>The first step will ask for parameters that depend on the function's <code>kind</code> and the task you are creating the run for, but will generally also ask if you wish to configure resources to allocate, environment variables, secrets, volumes and node modules.</p> <p>The second step will ask, if applicable, to specify inputs, outputs and parameters.</p> <p>The third step will simply present a recap.</p> <p></p>"},{"location":"tasks/functions/#view-and-manage","title":"View and manage","text":"<p>By going through a function's tabs, you can access the corresponding runs, but you may also access all runs from the Runs section in the left menu (also available as Jobs and runs in the dashboard).</p> <p>You can filter runs by name, kind and status.</p> <p></p> <p>Click on a run to view its details.</p> <p></p> <p>From here, click on <code>LOGS</code> to view its logs.</p> <p></p>"},{"location":"tasks/functions/#management-via-sdk","title":"Management via SDK","text":"<p>Functions can be created and managed as entities with the SDK CRUD methods. Check the SDK Functions documentation for more information.</p>"},{"location":"tasks/kubernetes-resources/","title":"Using Kubernetes Resources for Runs","text":""},{"location":"tasks/kubernetes-resources/#managing-kubernetes-resources-with-sdk","title":"Managing Kubernetes Resources with SDK","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on the standard Kubernetes functionality and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul> <p>In the platform, these requirements may be defined in two ways.</p> <p>First, it is possible to configure them explictly when defining the function run, either via Core UI or via SDK. Please note that it is possible to describe only some of these properties, leaving the rest blank without constraints. All the defaults are managed by the underlying Kubernetes deployment.</p> <p>Second, it is possible to rely on a set of preconfigured HW profiles defined by the platform deployment. The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p> <p>With SDK you can manage Kubernetes resources for your tasks. When you run a function, you can require some Kubernetes resources for the task. Resources and data are specified in the <code>function.run()</code> method.</p> <p>Please see the Kubernetes Resources section of the documentation for more information.</p>"},{"location":"tasks/models/","title":"ML Models","text":"<p>Support for MLOps is one of the key functionality of the platform. Creation, management, and serving ML models is supported by the platform via ML Model entities and the corresponding functionality for their registration and serving.</p> <p>ML Model entity represent the relevant information about the model - framework and algorithms used to create it, hyper parameters and metrics, necessary artifacts constituting the model, etc. The platform support a list of standard model kinds as well as generic models. Specifically, it is possible to define models of the following kinds</p> <ul> <li><code>sklearn</code> - ML models created with Scikit-learn framework and packaged as a single artifact.</li> <li><code>mlflow</code> - ML models created with any MLFlow-compatible framework (or <code>flavor</code> in MLFlow terminology) and logged following the MLFlow model format.</li> <li><code>huggingface</code> - LLM created using the HuggingFace framework and format, either standard one or fine-tuned.</li> <li><code>model</code> - generic ML Model with custom packaging and framework.</li> </ul> <p>For the specific ML Model formats the platform provides the support for serving those models as inference API in line with the V2 open inference protocol. These is achieved with the corresponding model serving runtimes.</p>"},{"location":"tasks/models/#management-via-ui","title":"Management via UI","text":"<p>Models can be created and managed as entities with the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new model</li> <li><code>expand</code> a model to see its 5 latest versions</li> <li><code>show</code> the details of a model</li> <li><code>edit</code> a model</li> <li><code>delete</code> a model</li> <li><code>filter</code> models by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete models using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/models/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the model</li> <li><code>Kind</code>: kind of the model</li> <li>(Spec) <code>Path</code>: remote path where the model is stored. If you instead upload the model at the bottom of the form, this will be the path to where it will be stored.</li> </ul>"},{"location":"tasks/models/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a model's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p>"},{"location":"tasks/models/#update","title":"Update","text":"<p>You can update a model by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/models/#delete","title":"Delete","text":"<p>You can delete a model from either its detail page or the list of models, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/models/#management-via-sdk","title":"Management via SDK","text":"<p>Models can be created and managed as entities with the SDK CRUD methods. Check the SDK Model documentation for more information.</p>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project represents a data and AI application and is a container for different entities (code, assets, configuration, ...) that form the application. It is the context in which you can run functions and manage models, data, and artifacts. Projects may be created and managed from the UI, but also by using DH Core's API, for example via Python SDK.</p>"},{"location":"tasks/projects/#management-via-ui","title":"Management via UI","text":"<p>In the following sections we document project management via the <code>Core Console</code> UI.</p> <p>Here we detail how to create, read, update and delete projects using the UI, similarly to SDK usage.</p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A project is created by clicking <code>CREATE A NEW PROJECT</code> in the console's home page.</p> <p></p> <p>A form asking for the project's details is then shown:</p> <p></p> <p>The following parameters are mandatory:</p> <ul> <li><code>name</code>: name of the project, also acts as identifier of the project</li> </ul> <p><code>Metadata</code> parameters are optional and may be changed later:</p> <ul> <li><code>name</code>: name of the project</li> <li><code>description</code>: a human-readable description of the project</li> <li><code>labels</code>: list of labels</li> </ul> <p><code>Save</code> and the project will appear in the home page.</p>"},{"location":"tasks/projects/#read","title":"Read","text":"<p>All projects present in the database are listed in the home page. Each tile shows:</p> <ul> <li>Identifier of the project</li> <li>Name of the project (hidden if same as identifier)</li> <li>Description</li> <li>Date of creation</li> <li>Date of last modification</li> </ul> <p></p> <p>Click on the tile to access the project's dashboard:</p> <p></p> <p>This dashboard shows a summary of the resources associated with the project and allows you to access the management of these resources.</p> <ul> <li><code>Jobs and runs</code>: list and status of performed runs</li> <li><code>Models</code>: number and list of latest models</li> <li><code>Functions and code</code>: number and list of latest functions</li> <li><code>Data items</code>: number and list of latest data items</li> <li><code>Artifacts</code>: number and list of latest artifacts</li> </ul> <p>You can return to the list of projects at any time by clicking Projects at the bottom of the left menu, or switch directly to a specific project by using the drop-down menu in the upper left of the interface.</p> <p></p>"},{"location":"tasks/projects/#update","title":"Update","text":"<p>To update a project's <code>Metadata</code>, first click <code>Configuration</code> in the left menu.</p> <p></p> <p>Click <code>Edit</code> in the top right and the edit form for <code>Metadata</code> properties will be shown. In the example below, a label was added.</p> <p></p> <p>When you're done updating the project, click Save.</p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>You can delete a project from the <code>Configuration</code> page, by clicking <code>Delete</code>. You will be asked to confirm by entering the project's identifier.</p> <p></p>"},{"location":"tasks/projects/#management-via-sdk","title":"Management via SDK","text":"<p>Projects can be created and managed as entities with the SDK. Check the SDK Project documentation for more information.</p>"},{"location":"tasks/resources/","title":"Resource Management with KRM","text":"<p>Different platform entities are associated with and represented as Kubernetes resources: they are deployed as services, user volumes and secrets, captured as Custom Resources, etc. Kubernetes Resource Manager (KRM) component allows for performing various operations over these resources depending on their kind.</p> <p></p> <p>KRM navigation menu provides access to different types of resources. This includes both standard resources (Services, Deployments, Persistent Volume Claims, Secrets) and custom resources based on Custom Resource Definitions currently installed on the platform. Some custom resources are managed with the customized UI (e.g., PostgreSQL instances, PostgREST Data services o Dremio Data service), while the others may be managed with the standard UI based on their JSON schema.</p>"},{"location":"tasks/resources/#management-of-standard-kubernetes-resources","title":"Management of Standard Kubernetes Resources","text":"<p>KRM allows for accessing and managing the standard K8S resources relevant for the DigitalHub platform: space (through Persistent Volume Claims), services and deployments, and secrets.</p>"},{"location":"tasks/resources/#listing-k8s-services","title":"Listing K8S Services","text":"<p>Accessing the <code>Services</code> menu of the KRM, it is possible to list the (subset of) services deployed on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each service KRM shows its name, type (e.g., Coder workspace type), exposed port type and value. In the service details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#listing-k8s-deployments","title":"Listing K8S Deployments","text":"<p>Accessing the <code>Deployments</code> menu of the KRM, it is possible to list the (subset of) deployments on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each deployment KRM shows its name and availability of instances. In the deployment details view it is possible to access other metadata, such as labels, creation timestamps. version, etc.</p>"},{"location":"tasks/resources/#managing-persistent-volume-claims","title":"Managing Persistent Volume Claims","text":"<p>In certain cases, the operations developed with the platform may require more substantial disk space, e.g., for training / producing significant amounts of data. In this case, it is possible to attach to the tasks the corresponding Persistent Volume Claim (PVC) references. To create a new PVC for the use of the pipeline or Job, KRM provides the corresponding interface.</p> <p>Accessing <code>Persistent Volume Claims</code> menu, it is possible to list and manage the PVCs of the platform.</p> <p></p> <p>For each PVC, you can see the status (Pending or Bound) of the PVC, the name of the volume (if specified), the storage class and the size in Gi. The details view provides further metadata regarding the PVC.</p> <p>It is also possible to delete the PVC and create new ones.</p> <p>Deleting PVC</p> <p>Please note that deleting a PVC bound to a Pod or a Job may affect negatively their execution.</p> <p>To create a new PVC, provide the following:</p> <ul> <li>name of the resource</li> <li>Disk space requeste</li> <li>Storage class name (select one of the available in your deployment)</li> <li>(Optional) name of the volume</li> <li>Access modes (standard K8S values)</li> <li>PVC mode (Filesystem or Block)</li> </ul> <p></p>"},{"location":"tasks/resources/#listing-k8s-secrets","title":"Listing K8S Secrets","text":"<p>Accessing the <code>Secrets</code> menu of the KRM, it is possible to list the (subset of) secrets on Kubernetes relevant to DigitalHub.</p> <p></p> <p>For each secret KRM shows its name, type, and number of elements. In the secret details view it is possible to access other metadata and also a list of secret elements. The values are not available directly; to retrieve the actual value of the secret element, use <code>Decode</code> button that will copy the decoded content of the secret into Clipboard.</p> <p></p>"},{"location":"tasks/resources/#managing-custom-resources","title":"Managing Custom Resources","text":"<p>KRM allows for the management of generic CRs as well as for the management of some predefined ones, such as PostgreSQL instances, PostgREST and Dremio Data services.</p>"},{"location":"tasks/resources/#managing-postgresql-instances-with-krm","title":"Managing PostgreSQL instances with KRM","text":"<p>Using PostgreSQL operator (https://github.com/movetokube/postgres-operator) it is possible to create new DB instances and the DB users to organize the data storage.</p> <p>Accessing <code>Postgres DBs</code> menu of the KRM, it is possible to list, create, and delete PostgreSQL databases.</p> <p></p> <p>To create a new Database, provide the following:</p> <ul> <li>name of the database to create</li> <li>whether to drop the DB on resource deletion</li> <li>Comma-separated list of PostgreSQL extensions to enable (e.g., timescale and/or postgis) as supported by the platform deployment (optional).</li> <li>Comma-separated list of schemas to create in DB (optional)</li> <li>Name of the master role for the DB access management (optional)</li> </ul> <p></p> <p>In the Database details view it is possible also to configure the DB users that can access and operate the Database (create, edit, view, delete). To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the user to be created</li> <li>access privileges (e.g., Owner, Read, or Write)</li> <li>name of the secret to be create to store the user credentials and DB access information. This results in creating a secret  <code>&lt;user-cr-name&gt;-&lt;secret-name&gt;</code> that can be accessed in the Secrets section of KRM.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-s3-resources-with-krm","title":"Managing S3 resources with KRM","text":"<p>If supported by the deployment, using Minio S3 operator (http://github.com/scc-digitalhub/minio-operator/) it is possible to create new S3 buckets, policies, and create/associate users to them.</p> <p>Accessing <code>S3 Buckets</code> menu of the KRM, it is possible to list, create, and delete S3 buckets, policies, and users.</p> <p></p> <p>To create a new Bucket, provide the following:</p> <ul> <li>name of the bucket to create</li> <li>optional quota for the bucket</li> </ul> <p></p> <p>In the S3 Policies tab view it is possible also to configure the S3 policies. To create a new policy, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>name of the policy to be created</li> <li>standard S3 policy spec in JSON format.</li> </ul> <p></p> <p>In the S3 Users tab view it is possible also to configure the S3 users. To create a new user, it is necessary to specify:</p> <ul> <li>name of the associated resource</li> <li>access key for the user to be created</li> <li>secret key for the user</li> <li>list of policies to associate to the user.</li> </ul> <p></p>"},{"location":"tasks/resources/#managing-postgrest-data-services-with-krm","title":"Managing PostgREST Data Services with KRM","text":"<p>You can deploy new PostgREST data services through KRM. A PostgREST service exposes a set of PostgreSQL tables through a REST API, allowing to query and even modify them.</p> <p>Access <code>PostgREST Data Services</code> on the left menu.</p> <p></p> <p>When creating a new PostgREST service, fields are as follows:</p> <ul> <li>Name of the resource</li> <li>Schema to expose</li> <li>Existing DB user (role) on behalf of which the service will operate OR the list of actions to enable and the list of tables which will be exposed and on which these actions may be performed. The user will be created automatically for this second option.</li> <li>Connection information.<ul> <li>If you choose not to provide an existing secret, Host, Database, User and Password are required.</li> <li>If you decide to use a secret, you must provide the secret's name. Two possible configurations are valid:<ul> <li>If the secret contains <code>POSTGRES_URL</code> (the full connection string, as <code>postgresql://user:password@host:port/database</code>), all other connection fields will be ignored.</li> <li>If the secret contains <code>USERNAME</code> and <code>PASSWORD</code>, Host and Database must be provided, or the resource will enter an error state.</li> </ul> </li> </ul> </li> </ul> <p>Port is optional and defaults to <code>5432</code> if not provided.</p> <p>Extra connection parameters may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable SSL: <code>sslmode=disable</code>.</p> <p>Connection information</p> <p>Please note that when no existing DB user is specified, the user specified in the Connection section must have sufficient privileges to manage roles. By default, the owner/writer/reader users created by the Postgres operator do not have this permission.</p> <p>Schema exposure</p> <p>PostgREST exposes all tables and views in the specified schema. In order to have better control over the exposed data, it is recommended to create a separate schema (e.g., <code>api</code>) and provide access to data through views or stored procedures. You can use SQLPad to do this.</p> <p></p> <p>Check out the official documentation for more information on PostgREST.</p>"},{"location":"tasks/resources/#managing-dremio-data-services-with-krm","title":"Managing Dremio Data Services with KRM","text":"<p>You can deploy new Dremio data services through KRM. A Dremio data service exposes Dremio data through a REST API.</p> <p>Access <code>Dremio Data Services</code> on the left menu.</p> <p></p> <p>To create a new service, provide the following:</p> <ul> <li>Name of the resource</li> <li>List of virtual datasets to expose</li> <li>Connection information.<ul> <li>Host is required.</li> <li>Port is optional and will default to <code>32010</code> if not provided.</li> <li>User and Password are required, unless you choose to use a secret, in which case the secret's name must be provided. The secret should contain <code>USER</code> and <code>PASSWORD</code>.</li> <li>Extra connection parameters are optional and may be provided in the format <code>param1=value1&amp;param2=value2</code>. For example, to disable certificate verification: <code>useEncryption=false&amp;disableCertificateVerification=true</code>.</li> </ul> </li> </ul> <p>If you instantiated Dremio through Coder, the value of Host is the value of Arrow Flight Endpoint, stripped of <code>:</code> and port. User is <code>admin</code> and Password is the value you entered when creating the workspace.</p> <p></p> <p>A Dremio REST service will be deployed, connected to the specified Dremio instance and exposing a simple REST API over the listed datasets.</p>"},{"location":"tasks/resources/#exposing-services-externally","title":"Exposing services externally","text":"<p>Various APIs and services (e.g., PostgREST or Dremio data services, Nuclio serverless functions) may be exposed externally, outside of the platform, on a public domain of the platform. Using KRM, the operation amounts to defining a new API gateway resource that will be transformed into the corresponding ingress routing specification.</p> <p></p> <p>To create a new API gateway, provide the following:</p> <ul> <li>Name of the gateway. This is merely an identifier for Kubernetes.</li> <li>Kubernetes service to be exposed (select it from the dropdown list and port will automatically be provided).</li> <li>Host defines the full domain name under which the service will be exposed. By default, it refers to the <code>services</code> subdomain. If your instance of the platform is found in the <code>example.com</code> domain, this field's value could be <code>myservice.services.example.com</code>.</li> <li>Relative path to expose the service on.</li> <li>Authentication information. Currently, services may be unprotected (<code>None</code>) or protected with <code>Basic</code> authentication, specifying username and password.</li> </ul>"},{"location":"tasks/resources/#defining-and-managing-crd-schemas","title":"Defining and Managing CRD Schemas","text":"<p>To have a valid representation of the CRs in the system, it is necessary to have a JSON specification schema for each CRDs. Normally, such schema is provided with the CRD definition and is used by KRM to manage the resources. However, in certain cases a CRD may have no structured schema definition attached. To allow for managing such resources, it is possible to provide a custom schema for the CRD.</p> <p>Creating a schema is fairly simple. Access the Settings section from the left menu and click Create.</p> <p>The CRD drop-down menu will list all Custom Resource Definitions available on the Kubernetes instance; when you pick one, the Version field will automatically be filled with the version of the currently active schema.</p> <p>Provide the Schema definition and save it in KRM for future CR management.</p>"},{"location":"tasks/run-resources/","title":"Configuring resource-critical executions","text":"<p>When it come to execution of AI tasks, either batch jobs or model serving, it is important to be able to allocate an appropriate set of resources, such as memory, GPU, node types, etc.</p> <p>For this purpose the platform relies on the standard Kubernetes functionality and resource definitions. More specifically, the run configuration may have a specific requirements for</p> <ul> <li>node selection</li> <li>volumes (Persistent Volume Claims and Config Maps)</li> <li>HW resources in terms of CPU and memory requests and limits, numbers of GPUs</li> <li>Kubernetes affinity definition and/or toleration properties</li> <li>Additional secrets and environment variables to be associated with the execution</li> </ul> <p>In the platform, these requirements may be defined in two ways.</p> <p>First, it is possible to configure them explictly when defining the function run, either via Core UI or via SDK. Please note that it is possible to describe only some of these properties, leaving the rest blank without constraints. All the defaults are managed by the underlying Kubernetes deployment.</p> <p>Second, it is possible to rely on a set of preconfigured HW profiles defined by the platform deployment. The mechanism of profiles is described in the administration section of the documentation and is managed by the platform admins. The profile allow for abstracting the platform users from the underlying complexity. Each profile corresponds to a specific resource configuration that defines a combination of requirements. For example, the profile may define a specific type of GPU, memory, and CPUs to be used. In this case it is sufficient to specify the corresponding profile name in the run execution configuration to allocate the corresponding resources.</p> <p>Please note that the requirements defined in the template have priority over those defined by the user and are not overwritten.</p>"},{"location":"tasks/secrets/","title":"Secret Management","text":"<p>Working with different operations may implu the usage of a sensitive values, such as external API credentials, storage credentials, etc.</p> <p>In order to avoid embedding the credentials in the code of functions, the platform supports an explicit management of credentials as secrets. This operation exploits the underlying secret management subsystem, such as Kubernetes Secret Manager.</p> <p>Besides the secrets managed natively by the platform to integrate e.g., default storage credentials, it is possible to define custom secrets at the level of a single project. The project secrets are managed as any other project-related entities, such as functions, dataitems, etc.</p> <p>At the level of the project the secrets are represented as key-value pairs. The management of secrets is delegated to a secret provider, and currently only Kubernetes Secret Manager is supported. Each project has its own Kubernetes secret, where all the key-value pairs are stored.</p> <p>To create a new secret value it is possible to use the Core UI console or directly via API, e.g., using the SDK.</p>"},{"location":"tasks/secrets/#creating-and-managing-secrets-via-ui","title":"Creating and Managing Secrets via UI","text":"<p>Core console can be used to manage project secrets. To create a new one, it is necessary to provide a secret key and a value to be stored.</p> <p></p> <p>The entries may be then deleted and updated, as well as their metadata.</p>"},{"location":"tasks/secrets/#managing-secrets-via-sdk","title":"Managing Secrets via SDK","text":"<p>Secrets can be created and managed as entities with the SDK CRUD methods. Check the SDK Secrets documentation for more information.</p>"},{"location":"tasks/workflows/","title":"Workflows","text":"<p>Workflows allow for organizing the single operations in a advanced management pipelines, to perform a series operation of data processing, ML model training and serving, etc. Workflows represent long-running procedures defined as Directed Acyclic Graphs (DAGs) where each node is a single unit of work performed by the platform (e.g., as a Kubernetes Job).</p> <p>As in case of functions, it is possible for the platform to have different workflow runtimes. Currently, the only workflow runtime implemented is the one based on Kubeflow Pipelines infrastructure. See KFP Runtime for further details about how the workflow is defined and executed with the Kubeflow Pipelines component of the platform.</p> <p>Similarly, to functions the workflows may be managed via console UI or via Python SDK.</p>"},{"location":"tasks/workflows/#management-via-ui","title":"Management via UI","text":"<p>Workflows can be created and managed as entities from the console. You can access them from the dashboard or the left menu. You can:</p> <ul> <li><code>create</code> a new workflow</li> <li><code>expand</code> a workflow to see its 5 latest versions</li> <li><code>show</code> the details of a workflow</li> <li><code>edit</code> a workflow</li> <li><code>delete</code> a workflow</li> <li><code>filter</code> workflows by name and kind</li> </ul> <p></p> <p>We will now see how to create, read, update and delete workflows using the UI, similarly to what is done with the SDK.</p>"},{"location":"tasks/workflows/#create","title":"Create","text":"<p>Click <code>CREATE</code> and a form will be shown:</p> <p></p> <p>Mandatory fields are:</p> <ul> <li><code>Name</code>: name and identifier of the workflow</li> <li><code>Kind</code>: kind of workflow</li> </ul> <p>Metadata fields are optional and may be updated later.</p> <ul> <li><code>Description</code>: a human-readable description</li> <li><code>Labels</code>: list of labels</li> <li><code>Name</code>: name of the function</li> <li><code>Embedded</code>: flag for embedded metadata</li> <li><code>Versioning</code>: version of the function</li> <li><code>Openmetadata</code>: flag to publish metadata</li> <li><code>Audit</code>: author of creation and modification</li> </ul> <p>In case of a <code>kfp</code> workflow, the source code and handler fields are required as well.</p>"},{"location":"tasks/workflows/#read","title":"Read","text":"<p>Click <code>SHOW</code> to view a workflow's details.</p> <p></p> <p>On the right side, all versions of the resource are listed, with the current one highlighted. By clicking a different version, values displayed will change accordingly.</p> <p>The <code>INSPECTOR</code> button will show a dialog containing the resource in JSON format.</p> <p></p> <p>The <code>EXPORT</code> button will download the resource's information as a yaml file.</p> <p>In case of <code>kfp</code> workflows, the executions of the workflow instances can be monitored with the corresponding DAG viewer.</p> <p></p>"},{"location":"tasks/workflows/#update","title":"Update","text":"<p>You can update a workflow by clicking <code>EDIT</code>. Greyed-out fields may not be updated.</p>"},{"location":"tasks/workflows/#delete","title":"Delete","text":"<p>You can delete a workflow from either its detail page or the list of workflows, by clicking <code>DELETE</code>.</p>"},{"location":"tasks/workflows/#management-via-sdk","title":"Management via SDK","text":"<p>Workflows can be created and managed as entities with the SDK CRUD methods. Check the SDK Workflows documentation for more information.</p>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p> <p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>Let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}